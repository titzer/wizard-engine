// Copyright 2021 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def RT: X86_64Runtime;
def IVAR_FRAME = IVarFrame.new();
// Deprecated.
class IVarFrame {
	def WASM_FUNC	= X86_64Regs.RSP.plus(X86_64InterpreterFrame.wasm_func.offset);
	def MEM0_BASE	= X86_64Regs.RSP.plus(X86_64InterpreterFrame.mem0_base.offset);
	def VFP		= X86_64Regs.RSP.plus(X86_64InterpreterFrame.vfp.offset);
	def VSP		= X86_64Regs.RSP.plus(X86_64InterpreterFrame.vsp.offset);
	def SIDETABLE	= X86_64Regs.RSP.plus(X86_64InterpreterFrame.sidetable.offset);
	def STP		= X86_64Regs.RSP.plus(X86_64InterpreterFrame.stp.offset);
	def CODE	= X86_64Regs.RSP.plus(X86_64InterpreterFrame.code.offset);
	def IP		= X86_64Regs.RSP.plus(X86_64InterpreterFrame.ip.offset);
	def EIP		= X86_64Regs.RSP.plus(X86_64InterpreterFrame.eip.offset);
	def FUNC_DECL	= X86_64Regs.RSP.plus(X86_64InterpreterFrame.func_decl.offset);
	def INSTANCE	= X86_64Regs.RSP.plus(X86_64InterpreterFrame.instance.offset);
	def CURPC	= X86_64Regs.RSP.plus(X86_64InterpreterFrame.curpc.offset);
	def ACCESSOR	= X86_64Regs.RSP.plus(X86_64InterpreterFrame.accessor.offset);
}

var recordCurIpForTraps = !FeatureDisable.stacktraces;

// Implements a Wasm interpreter by running handwritten x86-64 interpreter loop.
component X86_64Interpreter {
	var dispatchTable: Pointer;
	var interpreterCode: X86_64InterpreterCode;

	def onProbeEnable() {
		if (interpreterCode != null) dispatchTable = interpreterCode.start + interpreterCode.header.probedDispatchTableOffset;
	}
	def onProbeDisable() {
		if (interpreterCode != null) dispatchTable = interpreterCode.start + interpreterCode.header.fastDispatchTableOffset;
	}
	def inCode(p: Pointer) -> bool {
		return p >= interpreterCode.start && p < interpreterCode.end;
	}
	def computePCFromFrame(sp: Pointer) -> int {
		if (Debug.interpreter) dumpFrame(sp);
		if (!FeatureDisable.stacktraces) return (sp + X86_64InterpreterFrame.curpc.offset).load<int>();
		var ip   = (sp + X86_64InterpreterFrame.ip.offset).load<Pointer>();
		var code = Pointer.atContents((sp + X86_64InterpreterFrame.code.offset).load<Array<byte>>());
		return int.!(ip - code - 1);
	}
}
def dumpFrame(sp: Pointer) {
	Trace.OUT.put1("WASM_FUNC = %x\n", (sp + X86_64InterpreterFrame.wasm_func.offset).load<u64>());
	Trace.OUT.put1("MEM0_BASE = %x\n", (sp + X86_64InterpreterFrame.mem0_base.offset).load<u64>());
	Trace.OUT.put1("VFP       = %x\n", (sp + X86_64InterpreterFrame.vfp.offset).load<u64>());
	Trace.OUT.put1("VSP       = %x\n", (sp + X86_64InterpreterFrame.vsp.offset).load<u64>());
	Trace.OUT.put1("SIDETABLE = %x\n", (sp + X86_64InterpreterFrame.sidetable.offset).load<u64>());
	Trace.OUT.put1("STP       = %x\n", (sp + X86_64InterpreterFrame.stp.offset).load<u64>());
	Trace.OUT.put1("CODE      = %x\n", (sp + X86_64InterpreterFrame.code.offset).load<u64>());
	Trace.OUT.put1("IP        = %x\n", (sp + X86_64InterpreterFrame.ip.offset).load<u64>());
	Trace.OUT.put1("EIP       = %x\n", (sp + X86_64InterpreterFrame.eip.offset).load<u64>());
	Trace.OUT.put1("FUNC_DECL = %x\n", (sp + X86_64InterpreterFrame.func_decl.offset).load<u64>());
	Trace.OUT.put1("INSTANCE  = %x\n", (sp + X86_64InterpreterFrame.instance.offset).load<u64>());
	Trace.OUT.put1("CURPC     = %x\n", (sp + X86_64InterpreterFrame.curpc.offset).load<u64>());
	Trace.OUT.put1("ACCESSOR  = %x\n", (sp + X86_64InterpreterFrame.accessor.offset).load<u64>());
	Trace.OUT.ln();
}

// Signal-handling for traps
def ucontext_rip_offset = 168;
def ucontext_rsp_offset = 160;
def SIGFPE  = 8;
def SIGBUS  = 10;
def SIGSEGV = 11;

// Implements the RiUserCode interface in order to add generated machine code to the V3 runtime.
// Also stores several important offsets needed in handling signals.
class X86_64InterpreterCode extends RiUserCode {
	def frameSize = X86_64InterpreterFrame.size;
	def header: Ref<X86_64PreGenHeader>;
	var intV3Entry: (WasmFunction, Pointer) -> Throwable;

	new(start: Pointer, end: Pointer, header) super(start, end) { }

	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: Range<byte> -> void) {
		if (Debug.stack) X86_64Stacks.traceIpAndSp(ip, sp, out);
		var msg = "\tin [fast-int] ";
		out(msg);
		var instance = (sp + X86_64InterpreterFrame.instance.offset).load<Instance>();
		var func = (sp + X86_64InterpreterFrame.func_decl.offset).load<FuncDecl>();
		// TODO: lazy parse of names section may allocate; must avoid this in OOM situation
		var buf = X86_64Runtime.globalFrameDescriptionBuf;
		if (func == null) buf.puts("(func=null)");
		else if (instance == null) func.render(null, buf);
		else func.render(instance.module.names, buf);
		buf.ln().send(out);
		buf.reset();
	}

	// Called from V3 runtime for a frame where {ip} is in interpreter code.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp += X86_64InterpreterFrame.size;	 // assume frame is allocated
		ip = sp.load<Pointer>(); // return address on stack
		return (ip + -1, sp + Pointer.SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}

	// Called from V3 runtime when the garbage collector needs to scan an interpreter stack frame.
	def scanFrame(ip: Pointer, sp: Pointer) {
		// Handle code and interior pointers
		var code_loc = (sp + X86_64InterpreterFrame.code.offset);
		var code = code_loc.load<Pointer>();
		var ip_delta = (sp + X86_64InterpreterFrame.ip.offset).load<Pointer>() - code;
		var eip_delta = (sp + X86_64InterpreterFrame.eip.offset).load<Pointer>() - code;
		RiGc.scanRoot(code_loc);
		var new_code = code_loc.load<Pointer>();
		if (new_code != code) {
			(sp + X86_64InterpreterFrame.ip.offset).store<Pointer>(new_code + ip_delta);
			(sp + X86_64InterpreterFrame.eip.offset).store<Pointer>(new_code + eip_delta);
		}

		// Handle sidetable and interior pointer
		var sidetable_loc = (sp + X86_64InterpreterFrame.sidetable.offset);
		var sidetable = sidetable_loc.load<Pointer>();
		var stp_delta = (sp + X86_64InterpreterFrame.stp.offset).load<Pointer>() - sidetable;
		RiGc.scanRoot(sidetable_loc);
		var new_sidetable = sidetable_loc.load<Pointer>();
		if (new_sidetable != sidetable) {
			(sp + X86_64InterpreterFrame.stp.offset).store<Pointer>(new_sidetable + stp_delta);
		}

		// Handle other roots in the frame
		RiGc.scanRoot(sp + X86_64InterpreterFrame.wasm_func.offset);
		RiGc.scanRoot(sp + X86_64InterpreterFrame.func_decl.offset);
		RiGc.scanRoot(sp + X86_64InterpreterFrame.instance.offset);
		RiGc.scanRoot(sp + X86_64InterpreterFrame.accessor.offset);
	}

	// Called from V3 runtime to handle an OS-level signal that occurred while {ip} was in interpreter code.
	def handleSignal(signum: int, siginfo: Pointer, ucontext: Pointer, ip: Pointer, sp: Pointer) -> bool {
		var pip = ucontext + ucontext_rip_offset;
		var ip = pip.load<Pointer>();
		if (Debug.interpreter) {
			Trace.OUT.put2("  !signal %d in interpreter @ 0x%x", signum, ip - Pointer.NULL).ln();
		}
		match (signum) {
			SIGFPE => {
				// presume divide/modulus by zero
				pip.store<Pointer>(start + header.divZeroHandlerOffset);
				return true;
			}
			SIGBUS, SIGSEGV => {
				var addr = RiOs.getAccessAddress(siginfo, ucontext);
				if (RedZones.isInRedZone(addr)) {
					def RETADDR_SIZE = Pointer.SIZE;
					var handler_ip = X86_64Runtime.curStack.handleOverflow(ip, sp + X86_64InterpreterFrame.size + RETADDR_SIZE);
					pip.store<Pointer>(handler_ip);
					return true;
				}
				pip.store<Pointer>(start + header.oobMemoryHandlerOffset);
				return true;
			}
		}
		return true;
	}
	def setV3Entry() {
		var I: X86_64Interpreter;
		intV3Entry = CiRuntime.forgeClosure<
			X86_64Interpreter,				// closure type
			(/*wf: */ WasmFunction, /*sp: */ Pointer), Throwable>( // param and return types
				start + header.intV3EntryOffset, I);
	}
}

def fatal(msg: string) {
	System.error("X86_64InterpreterError", msg);
}

//------------------------------------------------------------------------------------------------
//-- Begin Interpreter Generator
//------------------------------------------------------------------------------------------------

// Describes the register and frame information for the fast interpreter.
class IntExecEnv {
	// Frame information.
	var frameSize: int;
	var accessor_slot: MasmAddr;
	var code_slot: MasmAddr;
	var curpc_slot: MasmAddr;
	var eip_slot: MasmAddr;
	var func_decl_slot: MasmAddr;
	var instance_slot: MasmAddr;
	var ip_slot: MasmAddr;
	var mem0_base_slot: MasmAddr;
	var pc_slot: MasmAddr;
	var sidetable_slot: MasmAddr;
	var stp_slot: MasmAddr;
	var vfp_slot: MasmAddr;
	var vsp_slot: MasmAddr;
	var wasm_func_slot: MasmAddr;

	// Register information.
	var sp: Reg;
	var func_arg: Reg;
	var vsp: Reg;
	var vfp: Reg;
	var stp: Reg;
	var ip: Reg;
	var func_decl: Reg;
	var eip: Reg;
	var mem0_base: Reg;
	var instance: Reg;
	var runtime_arg0: Reg;
	var runtime_arg1: Reg;
	var runtime_arg2: Reg;
	var runtime_ret0: Reg;
	var runtime_ret1: Reg;
	var ret_throw: Reg;
	var dispatch: Reg;
	var curpc: Reg;
	var scratch: Reg;
	var xmm0: Reg;
	var xmm1: Reg;
	var xmm2: Reg;
	var xmm3: Reg;
	var tmp0: Reg;
	var tmp1: Reg;
	var tmp2: Reg;
	var tmp3: Reg;
	var tmp4: Reg;
}

// Internal register configuration for variables live in the interpreter execution context.
def R: X86_64Regs, GPRs = X86_64Regs.GPRs, C: X86_64Conds;
// Helper for various slot addresses.
type SlotAddrs(tag: X86_64Addr, value: X86_64Addr, upper: X86_64Addr) #unboxed { }
class VspHelper(vsp: X86_64Gpr, valuerep: Tagging, depth: int) {
	private def slots = Array<SlotAddrs>.new(depth + 1);
	new() {
		for (i < slots.length) {
			var offset = -1 * i * valuerep.slot_size;
			slots[i] = SlotAddrs(
				vsp.plus(offset),
				vsp.plus(offset + valuerep.tag_size),
				vsp.plus(offset + valuerep.tag_size + 4));
		}
	}
	def [i: int] -> SlotAddrs {
		return slots[0 - i]; // so caller can supply -1
	}
}
// Type table used in decoding local declarations.
def TYPE_HAS_IMM: byte = 0x80;
def TYPE_IS_LEB: byte = 0x40;
def LEB_UPPER_BIT: byte = 0x80;
def G = X86_64MasmRegs.toGpr;
def X = X86_64MasmRegs.toXmmr;

// Generates {X86_64InterpreterCode} for X86-64.
class X86_64InterpreterGen(ic: X86_64InterpreterCode, w: DataWriter) {
	def masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	def asm = masm.asm;
	def xenv: IntExecEnv = X86_64MasmRegs.INT_EXEC_ENV;

	def offsets = V3Offsets.new();
	def valuerep = Target.tagging;
	def initialSize = w.data.length;

	var oolULeb32Sites = Vector<OutOfLineLEB>.new();
	var firstDispatchOffset: int;
	var dispatchJmpOffset: int = -1;
	var callEntryOffset: int;
	var handlerEndOffset: int;
	var callReentryLabel = X86_64Label.new();
	var tailCallReentryLabel = X86_64Label.new();
	var hostTailCallStubLabel = masm.newLabel(-1);
	var hostCallStubLabel = masm.newLabel(-1);
	var resumeStub = X86_64Label.new();
	var suspendStub = X86_64Label.new();
	var spcEntryLabel = X86_64Label.new();
	var controlFallThruLabel = X86_64Label.new();
	var controlTransferLabel = X86_64Label.new();
	var controlSkipSidetableAndDispatchLabel = X86_64Label.new();
	var probedDispatchTableRef: IcCodeRef;
	var typeTagTableOffset: int;

	def r_v3_wasm_func	= Target.V3_PARAM_GPRS[1];
	def r_v3_vsp		= Target.V3_PARAM_GPRS[2];

	def r_sp		= G(xenv.sp);
	def r_func_arg		= G(xenv.func_arg);
	def r_vsp		= G(xenv.vsp);
	def r_vfp		= G(xenv.vfp);
	def r_stp		= G(xenv.stp);
	def r_ip		= G(xenv.ip);
	def r_func_decl		= G(xenv.func_decl);
	def r_eip		= G(xenv.eip);
	def r_mem0_base		= G(xenv.mem0_base);
	def r_instance		= G(xenv.instance);
	def r_runtime_arg0	= G(xenv.runtime_arg0);
	def r_runtime_arg1	= G(xenv.runtime_arg1);
	def r_runtime_arg2	= G(xenv.runtime_arg2);
	def r_runtime_ret0	= G(xenv.runtime_ret0);
	def r_runtime_ret1	= G(xenv.runtime_ret1);
	def r_dispatch		= G(xenv.dispatch);
	def r_ret_throw		= G(xenv.ret_throw);
	def r_scratch		= G(xenv.scratch);
	def r_curpc		= G(xenv.curpc);

	def r_tmp0		= G(xenv.tmp0);
	def r_tmp1		= G(xenv.tmp1);
	def r_tmp2		= G(xenv.tmp2);
	def r_tmp3		= G(xenv.tmp3);
	def r_tmp4		= G(xenv.tmp4);

	def r_xmm0		= X(xenv.xmm0);
	def r_xmm1		= X(xenv.xmm1);
	def r_xmm2		= X(xenv.xmm2);
	def r_xmm3		= X(xenv.xmm3);

	def ip_ptr		= r_ip.plus(0);

	def m_wasm_func 	= IVAR_FRAME.WASM_FUNC;
	def m_mem0_base 	= IVAR_FRAME.MEM0_BASE;
	def m_vfp 		= IVAR_FRAME.VFP;
	def m_vsp 		= IVAR_FRAME.VSP;
	def m_sidetable 	= IVAR_FRAME.SIDETABLE;
	def m_stp 		= IVAR_FRAME.STP;
	def m_code 		= IVAR_FRAME.CODE;
	def m_ip 		= IVAR_FRAME.IP;
	def m_eip 		= IVAR_FRAME.EIP;
	def m_func_decl 	= IVAR_FRAME.FUNC_DECL;
	def m_instance 		= IVAR_FRAME.INSTANCE;
	def m_curpc 		= IVAR_FRAME.CURPC;
	def m_accessor 		= IVAR_FRAME.ACCESSOR;

	def k_frame_size	= X86_64InterpreterFrame.size;

	def vsph = VspHelper.new(r_vsp, valuerep, 3);

	def dispatchTables = Array<(byte, IcCodeRef, IcCodeRef)>.new(Opcodes.code_pages.length + 1);

	def ivar_MEM0_BASE	= (r_mem0_base, m_mem0_base);
	def ivar_VFP		= (r_vfp, m_vfp);
	def ivar_VSP		= (r_vsp, m_vsp);
	def ivar_STP		= (r_stp, m_stp);
	def ivar_IP		= (r_ip, m_ip);
	def ivar_EIP		= (r_eip, m_eip);
	def ivar_FUNC_DECL	= (r_func_decl, m_func_decl);
	def ivar_INSTANCE	= (r_instance, m_instance);
	def ivar_CURPC		= (r_curpc, m_curpc);

	def mutable_ivars = [
		ivar_VSP,
		ivar_STP,
		ivar_IP
	];
	def all_ivars = [
		ivar_MEM0_BASE,
		ivar_VFP,
		ivar_VSP,
		ivar_STP,
		ivar_IP,
		ivar_EIP,
		ivar_FUNC_DECL,
		ivar_INSTANCE,
		ivar_CURPC
	];

	new() {
		w.refill = reportOom;
		var p = Patcher.new(w);
		asm.patcher = asm.d.patcher = p;
	}

	def gen(range: MemoryRange) {
		if (FastIntTuning.dispatchEntrySize == 4 && (range.start - Pointer.NULL) > int.max) {
			fatal(Strings.format1("global buffer start address of 0x%x out of 31-bit range", (range.start - Pointer.NULL)));
		}
		// Record code start
		ic.header.codeStart = w.atEnd().pos; // XXX: don't make dispatch tables executable

		// Reserve the type tag table.
		genTypeTagTable();
		// Reserve space for the dispatch tables.
		reserveDispatchTables();
		// Begin code generation
		genHostCallStub();
		genStackSwitchStubs();
		genInterpreterEntry();
		genOpcodeHandlers();
		handlerEndOffset = w.atEnd().pos;
		// Generate out-of-line code
		genOutOfLineLEBs();
		genTraps();

		if (Debug.interpreter) {
			var s = range.start - Pointer.NULL;
			Trace.OUT
				.put3("Finished asm interpreter @ (0x%x ... 0x%x), used %d bytes\n",
					s, (range.end - Pointer.NULL), w.pos)
				.put1("\tcall entry     = 0x%x\n", s + callEntryOffset);
			for (t in dispatchTables) Trace.OUT.put2("\tdispatch %x    = 0x%x\n", byte.view(t.0), s + t.1.offset);
			Trace.OUT
				.put1("\tfirst dispatch = 0x%x\n", s + firstDispatchOffset)
				.put1("\thandlers end   = 0x%x\n", s + handlerEndOffset)
				.put1("break *0x%x\n", s + dispatchJmpOffset)
				.ln();
		}
		if (w.pos > initialSize) fatal(Strings.format2("need %d bytes for interpreter code, only allocated %d", w.pos, initialSize));
	}
	def genTypeTagTable() {
		if (FeatureDisable.complexBlockTypes) return;
		// Reserve space for the type tag table and fill it out
		typeTagTableOffset = w.pos;
		w.skipN(256);
		for (t in BpTypeCode) {
			var offset = typeTagTableOffset + t.code;
			w.at(offset + LEB_UPPER_BIT).putb(TYPE_IS_LEB);
		}
		for (t in [BpTypeCode.REF_NULL, BpTypeCode.REF]) {
			var offset = typeTagTableOffset + t.code;
			w.at(offset).putb(TYPE_HAS_IMM);
			w.at(offset + LEB_UPPER_BIT).putb(TYPE_HAS_IMM | TYPE_IS_LEB);
		}
		w.atEnd();
	}
	def reserveDispatchTables() {
		{ // table #0
			w.align(FastIntTuning.dispatchEntrySize);
			var ref = IcCodeRef.new(-1);
			ref.offset = ic.header.fastDispatchTableOffset = w.pos;
			w.skipN(256 * FastIntTuning.dispatchEntrySize);
			dispatchTables[0] = (0, ref, null);
		}
		if (!FeatureDisable.globalProbes) {
			w.align(FastIntTuning.dispatchEntrySize);
			probedDispatchTableRef = IcCodeRef.new(-1);
			probedDispatchTableRef.offset = ic.header.probedDispatchTableOffset = w.pos;
			w.skipN(256 * FastIntTuning.dispatchEntrySize);
		}
		for (i < Opcodes.code_pages.length) {
			var page = Opcodes.code_pages[i];
			var ref = IcCodeRef.new(-1);
			w.align(FastIntTuning.dispatchEntrySize);
			ref.offset = w.pos;
			w.skipN(256 * FastIntTuning.dispatchEntrySize);
			var ref2: IcCodeRef;

			if (!page.oneByte) {
				ref2 = IcCodeRef.new(-1);
				ref2.offset = w.pos;
				w.skipN(256 * FastIntTuning.dispatchEntrySize);
			}

			dispatchTables[i + 1] = (page.prefix, ref, ref2);
		}
	}
	def genHostCallStub() {
		var offsets = masm.getOffsets();
		var m_curStack = MasmAddr(Reg(0), int.!(offsets.X86_64Runtime_curStack - Pointer.NULL));
		var r_stack = xenv.tmp0;
		var r_func = xenv.func_arg;

		// Tail-calls deallocate their interpreter frame first.
		masm.bindLabel(hostTailCallStubLabel);
		asm.q.add_r_i(r_sp, k_frame_size);

		// Fall through to host call
		masm.bindLabel(hostCallStubLabel);
		ic.header.hostCallStubOffset = w.pos;
		// mov %stack, [cur_stack]    ; reload stack object from (thread-local) curStack
		masm.emit_mov_r_m(ValueKind.REF, r_stack, m_curStack);
		// mov [%stack.rsp], %rsp     ; save rsp in case of possible unwind
		masm.emit_mov_m_r(ValueKind.REF, MasmAddr(r_stack, offsets.X86_64Stack_rsp), xenv.sp);
		// mov [%stack.vsp], %vsp
		masm.emit_mov_m_r(ValueKind.REF, MasmAddr(r_stack, offsets.X86_64Stack_vsp), xenv.vsp);
		// move function into correct register
		masm.emit_mov_r_r(ValueKind.REF, xenv.runtime_arg0, r_func);
		// call runtime
		masm.emit_call_runtime_callHost(xenv.func_arg);

		// mov %stack, [cur_stack]    ; reload stack object from curStack
		masm.emit_mov_r_m(ValueKind.REF, r_stack, m_curStack);
		// mov %vsp, [%stack.vsp]     ; reload VSP from stack object
		masm.emit_mov_r_m(ValueKind.REF, xenv.vsp, MasmAddr(r_stack, masm.offsets.X86_64Stack_vsp));
		// mov %func, %runtime_ret1   ; move WasmFunction return into r_func
		masm.emit_mov_r_r(ValueKind.REF, r_func, xenv.runtime_ret1);
		// Check if WasmFunction != null, which means host tail-called Wasm
		var call_wasm = masm.newLabel(-1);
		masm.emit_br_r(r_func, MasmBrCond.REF_NONNULL, call_wasm); // XXX: near jump
		// Otherwise return (possibly null) throwable
		masm.emit_ret();

		// call into interpreter reentry or target code
		masm.bindLabel(call_wasm);
		if (FeatureDisable.multiTier) {
			// jump to interpreter directly as if tail-calling from SPC code
			// XXX: could save a jump above
			asm.jmp_rel_far(spcEntryLabel);
		} else {
			// Load entrypoint from r_func.decl.target_code
			var do_jump = masm.newLabel(-1);
			masm.emit_mov_r_m(ValueKind.REF, xenv.tmp2, MasmAddr(r_func, masm.offsets.WasmFunction_decl));
			masm.emit_mov_r_m(ValueKind.REF, xenv.tmp2, MasmAddr(xenv.tmp2, masm.offsets.FuncDecl_target_code));
			// Jump to interpreter if entrypoint not set
			asm.q.cmp_r_i(G(xenv.tmp2), 0);
			asm.jc_rel_far(C.Z, spcEntryLabel);
			masm.emit_jump_r(xenv.tmp2);
		}
		ic.header.hostCallStubEnd = w.pos;
	}
	def genStackSwitchStubs() {
		var m_curStack = X86_64Addr.new(null, null, 1, int.view(offsets.X86_64Runtime_curStack - Pointer.NULL));

		ic.header.resumeStubOffset = w.pos;
		asm.bind(resumeStub); {
			var r_cont = r_tmp0, r_stack = r_tmp1;
			// pop cont ref from stack
			decrementVsp();
			asm.movq_r_m(r_cont, vsph[0].value);
			// mov %stack, [%curStack]
			asm.movq_r_m(r_stack, m_curStack);

			var r_bottom = r_tmp3;
			asm.movq_r_m(r_bottom, r_cont.plus(offsets.Continuation_bottom));
			// mov [%r_bottom.parent_rsp_ptr], rsp
			asm.movq_r_m(r_scratch, r_bottom.plus(offsets.X86_64Stack_parent_rsp_ptr));
			asm.movq_m_r(r_scratch.plus(0), r_sp);

			var r_new = r_tmp0;
			asm.movq_r_m(r_new, r_cont.plus(offsets.Continuation_top));

			// mov [%curStack, %new]
			asm.movq_m_r(m_curStack, r_new);

			// save before switching to new stack pointer
			// mov [%stack.vsp], %vsp
			asm.movq_m_r(r_stack.plus(offsets.X86_64Stack_vsp), r_vsp);
			// mov [%stack.rsp], %rsp
			asm.movq_m_r(r_stack.plus(offsets.X86_64Stack_rsp), r_sp);

			// mov %rsp, [%newStack.rsp]
			asm.movq_r_m(r_sp, r_new.plus(offsets.X86_64Stack_rsp));
			// mov %vsp, [%newStack.vsp]
			asm.movq_r_m(r_vsp, r_new.plus(offsets.X86_64Stack_vsp));

			// mov %ret_throw, [%newStack.throw_on_resume]
			asm.movq_r_m(r_ret_throw, r_new.plus(offsets.X86_64Stack_throw_on_resume));
			// mov [%newStack.throw_on_resume], 0
			asm.movq_m_i(r_new.plus(offsets.X86_64Stack_throw_on_resume), 0);

			// pop return address and jump to it
			masm.emit_pop_r(ValueKind.REF, xenv.scratch);
			// jump *%tmp
			masm.emit_jump_r(xenv.scratch);
		}
		ic.header.resumeStubEnd = w.pos;

		ic.header.suspendStubOffset = w.pos;
		asm.bind(suspendStub); {
			asm.movq_r_m(r_scratch, m_curStack);
			asm.movq_r_m(r_sp, r_scratch.plus(offsets.X86_64Stack_rsp));
			asm.movq_r_m(r_vsp, r_scratch.plus(offsets.X86_64Stack_vsp));

			// load regs from handler stack
			restoreCallerIVars();
			restoreDispatchTableReg();
			endHandler();
		}
		ic.header.suspendStubEnd = w.pos;
	}
	def genInterpreterEntry() {
		var shared_entry = X86_64Label.new();
		var tmp = r_scratch;
		{ // Entrypoint for calls coming from V3
			ic.header.intV3EntryOffset = w.pos;

			// Allocate and initialize interpreter stack frame from incoming V3 args.
			asm.q.sub_r_i(r_sp, k_frame_size);

			// Spill VSP (value stack pointer)
			asm.movq_m_r(m_vsp, r_v3_vsp);
			// load dispatch table into register
			if (!FeatureDisable.globalProbes) masm.emit_load_dispatch_table_reg(xenv.dispatch);
			// move WasmFunction into tmp
			asm.movq_r_r(tmp, r_v3_wasm_func);
			restoreReg(r_vsp);
			asm.jmp_rel_near(shared_entry);
		}

		{ // Entrypoint for calls coming from SPC code
			ic.header.intSpcEntryOffset = w.pos;
			asm.bind(spcEntryLabel);

			// Allocate and initialize interpreter stack frame from incoming SPC args.
			asm.q.sub_r_i(r_sp, k_frame_size);

			// Spill VSP (value stack pointer)
			asm.movq_m_r(m_vsp, r_vsp);
			// load dispatch table into register
			if (!FeatureDisable.globalProbes) masm.emit_load_dispatch_table_reg(xenv.dispatch);
			// move WasmFunction into tmp
			asm.movq_r_r(tmp, r_func_arg);
			asm.jmp_rel_near(shared_entry);
		}

		{ // Re-entry for calls within the interpreter itself
			asm.bind(callReentryLabel);
			ic.header.intIntEntryOffset = w.pos;
			// Allocate actual stack frame
			asm.q.sub_r_i(r_sp, k_frame_size);
			// Spill the (valid) stack pointer
			saveIVar(r_vsp);
			// WasmFunction is in r1 for interpreter reentry
			asm.movq_r_r(tmp, r_func_arg);
		}

		asm.bind(shared_entry);
		genInvalidateFrameAccessor();
		// Load wf.instance, wf.decl and spill
		asm.movq_m_r(m_wasm_func, tmp);
		asm.movq_r_m(r_instance, tmp.plus(offsets.WasmFunction_instance));
		saveIVar(r_instance);
		asm.movq_r_m(r_func_decl, tmp.plus(offsets.WasmFunction_decl));
		saveIVar(r_func_decl);

		// Compute VFP = VSP - func.sig.params.length * SLOT_SIZE
		asm.movq_r_m(tmp, r_func_decl.plus(offsets.FuncDecl_sig));
		asm.movq_r_m(tmp, tmp.plus(offsets.SigDecl_params));
		asm.movd_r_m(tmp, tmp.plus(offsets.Array_length));
		asm.q.shl_r_i(tmp, valuerep.slot_size_log);
		asm.movq_r_r(r_vfp, r_vsp);
		asm.q.sub_r_r(r_vfp, tmp);
		saveIVar(r_vfp);

		asm.bind(tailCallReentryLabel);
		// Load &func.cur_bytecode[0] into IP
		asm.movq_r_m(tmp, r_func_decl.plus(offsets.FuncDecl_cur_bytecode));
		asm.movq_m_r(m_code, tmp); // save CODE
		asm.lea(r_ip, tmp.plus(offsets.Array_contents));
		saveIVar(r_ip);
		// Load IP + code.length into EIP
		asm.movd_r_m(r_eip, tmp.plus(offsets.Array_length));
		asm.q.add_r_r(r_eip, r_ip);
		saveIVar(r_eip);
		// Load &func.sidetable[0] into STP
		asm.movq_r_m(r_stp, r_func_decl.plus(offsets.FuncDecl_sidetable));
		asm.movq_m_r(m_sidetable, r_stp); // save SIDETABLE
		asm.q.add_r_i(r_stp, offsets.Array_contents);
		saveIVar(r_stp);

		// Load instance.memories[0].start into MEM0_BASE
		var mem0 = r_mem0_base;
		asm.movq_r_m(mem0, r_instance.plus(offsets.Instance_memories));
		var no_mem = X86_64Label.new();
		asm.movd_r_m(r_tmp0, mem0.plus(offsets.Array_length)); // XXX: always have a memories[0].start to avoid a branch?
		asm.d.cmp_r_i(r_tmp0, 0);
		asm.jc_rel_near(C.Z, no_mem);
		asm.movq_r_m(mem0, mem0.plus(offsets.Array_contents));
		asm.movq_r_m(mem0, mem0.plus(offsets.X86_64Memory_start));
		asm.bind(no_mem);
		saveIVar(r_mem0_base);

		callEntryOffset = w.pos;
		// Decode locals and initialize them. (XXX: special-case 0 locals)
		var countGpr = r_tmp0;
		genReadUleb32(countGpr);
		var start = X86_64Label.new(), done = X86_64Label.new();
		// gen: if (count != 0) do
		asm.d.cmp_r_i(countGpr, 0);
		asm.jc_rel_near(C.Z, done);
		asm.bind(start);
		// gen: var num = read_uleb32()
		var numGpr = r_tmp1;
		genReadUleb32(numGpr);
		// gen: var type = read_type();
		var type_done = X86_64Label.new();
		var typeGpr = r_tmp2;
		asm.d.movbzx_r_m(typeGpr, ip_ptr);	// load first byte
		asm.q.inc_r(r_ip);			// increment pointer
		// check for extended LEB, abstract type, or type with immediate
		var typeTableAddr = int.!((ic.start + typeTagTableOffset) - Pointer.NULL);
		asm.test_m_i(typeGpr.plus(typeTableAddr), TYPE_HAS_IMM | TYPE_IS_LEB);
		var complex_type_decode = X86_64Label.new();
		asm.jc_rel_far(C.NZ, complex_type_decode);
		asm.bind(type_done);

		// gen: if(num != 0) do
		var start2 = X86_64Label.new(), done2 = X86_64Label.new();
		asm.d.cmp_r_i(numGpr, 0);
		asm.jc_rel_near(C.Z, done2);
		asm.bind(start2);
		genTagPushR(typeGpr);			// *(sp) = type
		asm.movq_m_i(vsph[0].value, 0);		// *(sp + 8) = 0
		incrementVsp();	// sp += 1 slot
		// gen: while (--num != 0)
		asm.d.dec_r(numGpr);
		asm.jc_rel_near(C.NZ, start2);

		// gen: while (--count != 0)
		asm.d.dec_r(countGpr);
		asm.jc_rel_near(C.NZ, start);
		asm.bind(done);

		// Check for function entry probe.
		var func_entry_probe_label: X86_64Label;
		if (!FeatureDisable.entryProbes) {
			func_entry_probe_label = asm.newLabel();
			asm.test_m_i(r_func_decl.plus(offsets.FuncDecl_entry_probed), 1);
			asm.jc_rel_far(C.NZ, func_entry_probe_label);
		}

		// execute first instruction
		genDispatchOrJumpToDispatch();

		// handle extended LEB, types with immediates, and abstract types
		var type_not_leb = X86_64Label.new();
		asm.bind(complex_type_decode);
		asm.d.test_r_i(typeGpr, LEB_UPPER_BIT);		// check for LEB first
		asm.jc_rel_near(C.Z, type_not_leb);
		genSkipLeb0(r_tmp3);
		asm.bind(type_not_leb);
		// check for types that have an immediate
		asm.d.test_m_i(typeGpr.plus(typeTableAddr), TYPE_HAS_IMM);
		asm.jc_rel_near(C.Z, type_done);
		genReadUleb32(r_tmp3);			// decode offset
		asm.jmp_rel_near(type_done);

		// Emit code for function entry probe. XXX: move further out-of-line
		if (func_entry_probe_label != null) {
			asm.bind(func_entry_probe_label);
			computePcFromCurIp();
			if (!FeatureDisable.stacktraces) asm.movq_r_i(r_curpc, 0);
			saveCallerIVars();
			asm.movq_r_m(r_tmp0, m_wasm_func); // XXX: compute func and pc directly in the right regs
			callRuntime(refRuntimeCall(RT.runtime_PROBE_instr), [r_tmp0, r_curpc], true);
			// fall through
			restoreCallerIVars();
			//TODO: restoreDispatchTableReg();
			genJumpToDispatch();
		}
		ic.header.deoptReentryOffset = w.atEnd().pos;
		restoreCallerIVars();
		restoreReg(r_vsp);
		restoreDispatchTableReg();
		genJumpToDispatch();
	}

	// Generate all the opcode handlers.
	def genOpcodeHandlers() {
		// Generate the default handler and initialize dispatch tables
		var pos = w.atEnd().pos;
		computeCurIpForTrap(-1);
		asm.jmp_rel_far(newTrapLabel(TrapReason.INVALID_OPCODE));
		for (t in dispatchTables) {
			var ref = t.1;
			for (i < 256) writeDispatchEntry(ref, i, pos);
		}

		// Generate the secondary dispatch tables and point main table at them
		var ref0 = dispatchTables[0].1;
		for (t in dispatchTables) {
			if (t.0 == 0) continue; // main dispatch table
			var pos = w.atEnd().pos;
			computeCurIpForTrap(-1);
			genDispatch0(ip_ptr, t.1, true);
			writeDispatchEntry(ref0, t.0, pos);
		}

		// Generate extended LEB landing pads for secondary dispatch tables
		for (i = 1; i < dispatchTables.length; i++) {
			var t = dispatchTables[i];
			var pos = w.pos;
			if (t.2 != null) {
				// TODO: some code in this page are in the upper 128; read extended LEB
				genDispatch0(null, t.2, false);
			} else {
				// all codes in this page are in the lower 128; just skip extended LEB
				genSkipLeb();
				asm.d.and_r_i(r_tmp0, 0x7F);
				genDispatch0(null, t.1, false);
			}
			for (i = 128; i < 256; i++) {
				writeDispatchEntry(t.1, i, pos);
			}
		}

		genConsts();
		genControlFlow();
		genLocals();
		genCallsAndRet();
		genLoadsAndStores();
		genAtomicWaitAndNotify();
		genAtomicLoadsAndStores();
        	genAtomicRMWOperations();
		genCompares();
		genI32Arith();
		genI64Arith();
		genExtensions();
		genF32Arith();
		genF64Arith();
		genFloatCmps();
		genGcInstrs();
		genFloatMinAndMax();
		genFloatTruncs();
		genFloatConversions();
		genRuntimeCallOps();
		genSimdInstrs();
		genMisc();
		if (!FeatureDisable.globalProbes) genGlobalProbeSupport();
	}
	def writeDispatchEntry(ref: IcCodeRef, opcode: int, offset: int) {
		match (FastIntTuning.dispatchEntrySize) {
			2 => w.at(ref.offset + 2 * opcode).put_b16(offset - ref.offset);
			4 => w.at(ref.offset + 4 * opcode).put_b32(int.!((ic.start + offset) - Pointer.NULL));
			8 => w.at(ref.offset + 8 * opcode).put_b64((ic.start + offset) - Pointer.NULL);
		}
		w.atEnd();
	}
	def genConsts() {
		bindHandler(Opcode.I32_CONST); {
			genReadSleb32_inline(r_tmp1);
			genTagPush(BpTypeCode.I32.code);
			asm.movq_m_r(vsph[0].value, r_tmp1);
			incrementVsp();
			endHandler();
		}
		bindHandler(Opcode.I64_CONST); {
			genReadSleb64_inline(r_tmp1);
			genTagPush(BpTypeCode.I64.code);
			asm.movq_m_r(vsph[0].value, r_tmp1);
			incrementVsp();
			endHandler();
		}
		bindHandler(Opcode.F32_CONST); {
			asm.movd_r_m(r_tmp0, ip_ptr);
			asm.add_r_i(r_ip, 4);
			genTagPush(BpTypeCode.F32.code);
			asm.movq_m_r(vsph[0].value, r_tmp0);
			incrementVsp();
			endHandler();
		}
		bindHandler(Opcode.F64_CONST); {
			asm.movq_r_m(r_tmp0, ip_ptr);
			asm.add_r_i(r_ip, 8);
			genTagPush(BpTypeCode.F64.code);
			asm.movq_m_r(vsph[0].value, r_tmp0);
			incrementVsp();
			endHandler();
		}
	}
	def genControlFlow() {
		// NOP: just goes directly back to the dispatch loop
		patchDispatchTable(Opcode.NOP, firstDispatchOffset);

		// UNREACHABLE: abrupt return
		bindHandler(Opcode.UNREACHABLE);
		computeCurIpForTrap(-1);
		asm.jmp_rel_far(newTrapLabel(TrapReason.UNREACHABLE));

		// LOOP: gen tier-up trigger and skip block type
		bindHandler(Opcode.LOOP);
		if (!FeatureDisable.multiTier && !FeatureDisable.tierUpOsr) {
			// generate a tier-up trigger and then skip the block type
			genLoopTierUpTrigger();
			genSkipBlockType();
			endHandler();
		}
		// BLOCK: skip block type, maybe also check for repeated blocks
		var repeated_block = X86_64Label.new();
		bindHandler(Opcode.BLOCK);
		bindHandler(Opcode.TRY);
		asm.bind(repeated_block);
		genSkipBlockType();
		if (FastIntTuning.skipRepeatedBlocks && FastIntTuning.threadedDispatch) { // TODO: fix for global probes
			var opcode = r_tmp0;
			asm.movbzx_r_m(opcode, ip_ptr);
			asm.inc_r(r_ip);
			asm.cmp_r_i(opcode, Opcode.BLOCK.code);
			asm.jc_rel_near(C.Z, repeated_block);
			genDispatch0(null, null, false); // inlined rest of dispatch
		} else {
			endHandler();
		}

		var ctl_xfer_nostack = X86_64Label.new();

		// IF: check condition and either fall thru to next bytecode or ctl xfer (without stack copying)
		bindHandler(Opcode.IF);
		decrementVsp();
		asm.d.cmp_m_i(vsph[0].value, 0);
		asm.jc_rel_far(C.Z, ctl_xfer_nostack); // XXX: can be near if no complex block types
		genSkipBlockType();
		asm.bind(controlSkipSidetableAndDispatchLabel);
		genSkipSidetableEntry();
		endHandler();

		// BR_IF: check condition and either fall thru to next bytecode or ctl xfer (with stack copying)
		bindHandler(Opcode.BR_IF);
		decrementVsp();
		asm.d.cmp_m_i(vsph[0].value, 0);
		asm.jc_rel_far(C.Z, controlFallThruLabel); // XXX: move shared fallthrough closer?
		// fallthru to BR

		// BR: unconditional ctl xfer with stack copying
		bindHandlerNoAlign(Opcode.BR);
		asm.bind(controlTransferLabel);
		var popcount = r_tmp0;
		var valcount = r_tmp1;
		// if popcount > 0
		asm.movd_r_m(popcount, r_stp.plus(Sidetable_BrEntry.popcount.offset));
		asm.d.cmp_r_i(popcount, 0);
		asm.jc_rel_near(C.Z, ctl_xfer_nostack);
		// load valcount
		asm.movd_r_m(valcount, r_stp.plus(Sidetable_BrEntry.valcount.offset));
		// popcount = popcount * SLOT_SIZE
		asm.d.shl_r_i(popcount, valuerep.slot_size_log);
		// vsp -= valcount + popcount (XXX: save an instruction here?)
		asm.q.sub_r_r(r_vsp, popcount);
		asm.movd_r_r(r_scratch, valcount);
		asm.d.shl_r_i(r_scratch, valuerep.slot_size_log);
		asm.q.sub_r_r(r_vsp, r_scratch);
		// do { [vsp] = [vsp + popcount]; vsp++; valcount--; } while (valcount != 0)
		var loop = X86_64Label.new();
		asm.bind(loop);
		genCopySlot(r_vsp.plus(0), r_vsp.plusR(popcount, 1, 0));
		incrementVsp();
		asm.d.dec_r(valcount);
		asm.jc_rel_near(C.G, loop);

		// ELSE, (legal) CATCH, (legacy) CATCH_ALL: unconditional ctl xfer without stack copying
		bindHandlerNoAlign(Opcode.CATCH);
		bindHandlerNoAlign(Opcode.CATCH_ALL);
		bindHandlerNoAlign(Opcode.ELSE);
		asm.bind(ctl_xfer_nostack);
		if (FastIntTuning.fourByteSidetable) { // load and sign-extend a 4-byte pc delta
			asm.movd_r_m(r_tmp0, r_stp.plus(Sidetable_BrEntry.pc_delta.offset));
			asm.q.shl_r_i(r_tmp0, 32);
			asm.q.sar_r_i(r_tmp0, 32);
		} else {
			asm.movwsx_r_m(r_tmp0, r_stp.plus(Sidetable_BrEntry.pc_delta.offset));
		}
		asm.q.lea(r_ip, r_ip.plusR(r_tmp0, 1, -1)); // adjust ip
		if (FastIntTuning.fourByteSidetable) { // load and sign-extend a 4-byte STP delta
			asm.movwsx_r_m(r_tmp1, r_stp.plus(Sidetable_BrEntry.stp_delta.offset));
			asm.q.shl_r_i(r_tmp1, 32);
			asm.q.sar_r_i(r_tmp1, 32);
		} else {
			asm.movwsx_r_m(r_tmp1, r_stp.plus(Sidetable_BrEntry.stp_delta.offset));
		}
		asm.q.lea(r_stp, r_stp.plusR(r_tmp1, 4, 0)); // adjust stp XXX: preshift?
		endHandler();

		// BR_TABLE: adjust STP based on input value and then ctl xfer with stack copying
		bindHandler(Opcode.BR_TABLE);
		var max = r_tmp0, key = r_tmp1;
		asm.movd_r_m(max, r_stp.plus(Sidetable_BrEntry.pc_delta.offset));
		decrementVsp();
		asm.movd_r_m(key, vsph[0].value);
		asm.d.cmp_r_r(key, max);
		var ok = X86_64Label.new();
		asm.jc_rel_near(C.NC, ok);
		asm.d.inc_r(key);
		asm.movd_r_r(max, key);
		asm.bind(ok);
		asm.q.add_r_r(r_ip, max);
		asm.shl_r_i(max, u6.!(Ints.log(u32.!(Sidetable_BrEntry.size))));
		asm.q.add_r_r(r_stp, max);
		asm.jmp_rel_near(controlTransferLabel);

		// BR_ON_NULL: check condition and either fall thru to next bytecode or ctl xfer (with stack copying)
		bindHandler(Opcode.BR_ON_NULL);
		asm.q.cmp_m_i(vsph[-1].value, 0);
		asm.jc_rel_near(C.NZ, controlFallThruLabel);
		decrementVsp();
		asm.jmp_rel_near(controlTransferLabel);

		// BR_ON_NON_NULL: check condition and either fall thru to next bytecode or ctl xfer (with stack copying)
		bindHandler(Opcode.BR_ON_NON_NULL);
		asm.q.cmp_m_i(vsph[-1].value, 0);
		asm.jc_rel_near(C.NZ, controlTransferLabel);
		decrementVsp();
		// shared code for not-taken banches
		asm.bind(controlFallThruLabel);
		genSkipLeb();
		genSkipSidetableEntry();
		endHandler();

		bindHandler(Opcode.SELECT); {
			var label = X86_64Label.new();
			asm.d.cmp_m_i(vsph[-1].value, 0);
			asm.jc_rel_near(C.NZ, label);
			// false case; copy false value down
			if (valuerep.value_size == 16) {
				asm.movdqu_s_m(r_xmm0, vsph[-2].value);
				asm.movdqu_m_s(vsph[-3].value, r_xmm0);
			} else {
				asm.movq_r_m(r_tmp0, vsph[-2].value);
				asm.movq_m_r(vsph[-3].value, r_tmp0);
			}
			// true case, nothing to do
			asm.bind(label);
			adjustVsp(-2);
			endHandler();
		}
		bindHandler(Opcode.SELECT_T); {
			genReadUleb32(r_tmp0); // load # values
			var skip = X86_64Label.new();
			asm.movd_r_r(r_tmp1, r_tmp0);
			asm.bind(skip);  // skip value types
			genSkipLeb();
			asm.dec_r(r_tmp1);
			asm.jc_rel_near(C.NZ, skip);

			asm.d.shl_r_i(r_tmp0, valuerep.slot_size_log);
			asm.movd_r_m(r_tmp1, vsph[-1].value);
			asm.sub_r_r(r_vsp, r_tmp0);
			decrementVsp(); // XXX: combine with above using lea
			asm.d.cmp_r_i(r_tmp1, 0);
			var label = X86_64Label.new();
			asm.jc_rel_near(C.NZ, label);
			// false case; copy false values down
			asm.movq_r_r(r_tmp1, r_vsp);
			asm.q.sub_r_r(r_tmp1, r_tmp0);
			var copy = X86_64Label.new();
			asm.bind(copy);
			if (valuerep.value_size == 16) {
				asm.movdqu_s_m(r_xmm0, r_vsp.plusR(r_tmp0, 1, - valuerep.value_size));
				asm.movdqu_m_s(r_tmp1.plusR(r_tmp0, 1, - valuerep.value_size), r_xmm0);
			} else {
				asm.movq_r_m(r_tmp2, r_vsp.plusR(r_tmp0, 1, - Pointer.SIZE));
				asm.movq_m_r(r_tmp1.plusR(r_tmp0, 1, - Pointer.SIZE), r_tmp2);
			}
			asm.d.sub_r_i(r_tmp0, valuerep.slot_size);
			asm.jc_rel_near(C.NZ, copy);
			// true case, nothing to do
			asm.bind(label);
			endHandler();
		}
		bindHandler(Opcode.TRY_TABLE); { // XXX: speed up with sidetable entry to skip immediates?
			genSkipBlockType();
			var countGpr = r_tmp0, kindGpr = r_tmp1, tagGpr = r_tmp2;
			genReadUleb32(countGpr);
			asm.movd_r_r(r_tmp1, countGpr);
			asm.imul_r_i(r_tmp1, Sidetable_CatchEntry.size);
			asm.q.add_r_r(r_stp, r_tmp1);   // adjust sidetable pointer
			var start = X86_64Label.new(), done = X86_64Label.new(), catch_all = X86_64Label.new();
			asm.bind(start);
			asm.d.dec_r(countGpr);		// while (--count >= 0) {
			asm.jc_rel_near(C.S, done);
			asm.movb_r_m(kindGpr, ip_ptr);	//  kind = *rip++;
			asm.inc_r(r_ip);
			genSkipLeb();			//  skip LEB (tag or label)
			asm.test_r_i(kindGpr, 0x02);	//  if (kind & 0x2 == 1), is catch_all
			asm.jc_rel_near(C.NZ, catch_all);
			genSkipLeb();			//  skip label
			asm.bind(catch_all);
			asm.jmp_rel_near(start);
			asm.bind(done);
			endHandler();
		}
		bindHandler(Opcode.THROW); {
			computeCurIpForTrap(-1);
			computePcFromCurIp();
			genReadUleb32(r_tmp0);
			saveCallerIVars();
			callRuntime(refRuntimeCall(RT.runtime_THROW), [r_instance, r_tmp0], true);
			restoreCallerIVars();
			restoreDispatchTableReg();
			genDispatchOrJumpToDispatch();
		}
		bindHandler(Opcode.THROW_REF); {
			computeCurIpForTrap(-1);
			computePcFromCurIp();
			asm.movd_r_m(r_tmp0, vsph[-1].value);
			decrementVsp();
			saveCallerIVars();
			callRuntime(refRuntimeCall(RT.runtime_THROW_REF), [r_tmp0], true);
			restoreCallerIVars();
			restoreDispatchTableReg();
			genDispatchOrJumpToDispatch();
		}
	}
	def genLoopTierUpTrigger() {
		// In multi-tier mode with OSR, the loop bytecode decrements {FuncDecl.tierup_trigger}.
		var done = X86_64Label.new(), tierup = X86_64Label.new();
		asm.d.sub_m_i(r_func_decl.plus(offsets.FuncDecl_tierup_trigger), FastIntTuning.loopTierUpDecrement);
		asm.jc_rel_near(C.C, tierup);
		asm.bind(done);
		genSkipBlockType();
		endHandler();
		// TierUp triggered; call runtime
		asm.bind(tierup);
		computePc(-1);
		saveCallerIVars();
		asm.movq_r_m(r_tmp1, m_wasm_func);
		callRuntime(refRuntimeCall(RT.runtime_TIERUP), [r_tmp1, r_curpc, r_tmp4], true); // XXX: register movement
		restoreCallerIVars();
		asm.jmp_rel_near(done);
	}
	def genLocals() {
		bindHandler(Opcode.DROP);
		decrementVsp();
		endHandler();

		bindHandler(Opcode.LOCAL_GET);
		genReadUleb32(r_tmp0);
		asm.d.shl_r_i(r_tmp0, valuerep.slot_size_log);
		genCopySlot(r_vsp.indirect(), r_vfp.plusR(r_tmp0, 1, 0));
		incrementVsp();
		endHandler();

		bindHandler(Opcode.LOCAL_SET);
		genReadUleb32(r_tmp0);
		asm.d.shl_r_i(r_tmp0, valuerep.slot_size_log);
		decrementVsp();
		if (valuerep.value_size == 16) {
			asm.movdqu_s_m(r_xmm0, vsph[0].value);
			asm.movdqu_m_s(r_vfp.plusR(r_tmp0, 1, valuerep.tag_size), r_xmm0);
		} else {
			asm.movq_r_m(r_tmp1, vsph[0].value);
			asm.movq_m_r(r_vfp.plusR(r_tmp0, 1, valuerep.tag_size), r_tmp1);
		}
		endHandler();

		bindHandler(Opcode.LOCAL_TEE);
		genReadUleb32(r_tmp0);
		asm.d.shl_r_i(r_tmp0, valuerep.slot_size_log);
		if (valuerep.value_size == 16) {
			asm.movdqu_s_m(r_xmm0, vsph[-1].value);
			asm.movdqu_m_s(r_vfp.plusR(r_tmp0, 1, valuerep.tag_size), r_xmm0);
		} else {
			asm.movq_r_m(r_tmp1, vsph[-1].value);
			asm.movq_m_r(r_vfp.plusR(r_tmp0, 1, valuerep.tag_size), r_tmp1);
		}
		endHandler();
	}
	def genCallsAndRet() {
		bindHandler(Opcode.END);
		var returnLabel = X86_64Label.new();
		asm.q.cmp_r_r(r_ip, r_eip);
		if (FastIntTuning.threadedDispatch) {
			// jump over inlined dispatch
			asm.jc_rel_near(C.GE, returnLabel);
			genDispatch();
		} else {
			// jump to first dispatch
			asm.jc_rel(C.L, firstDispatchOffset - w.pos); // jump to dispatch (loop)
			// fall through to return bytecode
		}

		var callFunction = X86_64Label.new();
		var func_arg = r_func_arg;
		bindHandlerNoAlign(Opcode.RETURN); {
			asm.bind(returnLabel);
			// Copy return values from stack to overwrite locals
			var cnt = r_tmp0;
			asm.movq_r_m(cnt, r_func_decl.plus(offsets.FuncDecl_sig));
			asm.movq_r_m(cnt, cnt.plus(offsets.SigDecl_results));
			asm.movd_r_m(cnt, cnt.plus(offsets.Array_length));
			genCopyStackValsToVfp(cnt, r_tmp1);
			// Deallocate interpreter frame and return to calling code.
			asm.movd_r_i(Target.V3_RET_GPRS[0], 0);
			genPopFrameAndRet();

			bindHandler(Opcode.CALL);
			computeCurIpForTrap(-1);
			genReadUleb32(r_tmp1);

			asm.movq_r_m(r_tmp0, r_instance.plus(offsets.Instance_functions));
			asm.movq_r_m(func_arg, r_tmp0.plusR(r_tmp1, offsets.REF_SIZE, offsets.Array_contents));

			// call_indirect jumps here
			asm.bind(callFunction);
			computePcFromCurIp();
			saveCallerIVars();
			var call_host = masm.newLabel(-1);
			masm.emit_br_r(xenv.func_arg, MasmBrCond.IS_NOT_WASM_FUNC, call_host); // XXX: near jump

			// WasmFunction: call into interpreter reentry or target code
			if (!FeatureDisable.multiTier) {
				var tmp = r_tmp2;
				asm.movq_r_m(tmp, func_arg.plus(offsets.WasmFunction_decl));
				asm.icall_m(tmp.plus(offsets.FuncDecl_target_code));
			} else {
				asm.call_rel_far(callReentryLabel);
			}
			restoreCallerIVars();
			if (!FeatureDisable.multiTier) restoreDispatchTableReg();
			genDispatchOrJumpToDispatch();

			// HostFunction: call into runtime
			asm.bind(call_host.label);
			asm.call_rel_far(hostCallStubLabel.label);
			restoreCallerIVars();
			restoreDispatchTableReg();
			genJumpToDispatch();
		}

		var trap_func_invalid = newTrapLabel(TrapReason.FUNC_INVALID);
		var check_sig_mismatch = X86_64Label.new();
		bindHandler(Opcode.CALL_INDIRECT); {
			computeCurIpForTrap(-1);
			var sig_index = r_tmp1, table_index = r_tmp2, func_index = r_tmp0;
			genReadUleb32(sig_index);
			genReadUleb32(table_index);

			decrementVsp();
			asm.movd_r_m(func_index, vsph[0].value);

			var tmp = r_tmp3;
			// load instance.sig_ids[sig_index] into sig_index
			asm.movq_r_m(tmp, r_instance.plus(offsets.Instance_sig_ids));
			asm.movd_r_m(sig_index, tmp.plusR(sig_index, offsets.INT_SIZE, offsets.Array_contents));
			// Bounds-check table.ids[func_index]
			asm.movq_r_m(tmp, r_instance.plus(offsets.Instance_tables));
			var table = table_index;
			asm.movq_r_m(table, tmp.plusR(table_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(tmp, table.plus(offsets.Table_ids));
			asm.d.cmp_r_m(func_index, tmp.plus(offsets.Array_length));
			asm.jc_rel_far(C.NC, trap_func_invalid);
			// Check table.ids[func_index] == sig_index
			asm.d.cmp_r_m(sig_index, tmp.plusR(func_index, offsets.INT_SIZE, offsets.Array_contents));
			asm.jc_rel_near(C.NZ, check_sig_mismatch);
			// Load table.funcs[func_index] into {func_arg} and jump to calling sequence
			asm.movq_r_m(tmp, table.plus(offsets.Table_funcs));
			asm.movq_r_m(func_arg, tmp.plusR(func_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.jmp_rel_near(callFunction);  // XXX: duplicate call sequence here?
			// Signature check failed. Mismatch or invalid function?
			asm.bind(check_sig_mismatch);
			asm.d.cmp_m_i(tmp.plusR(func_index, offsets.INT_SIZE, offsets.Array_contents), 0);
			asm.jc_rel_far(C.S, trap_func_invalid); // < 0 implies invalid function, not function sig mismatch
			asm.jmp_rel_far(newTrapLabel(TrapReason.FUNC_SIG_MISMATCH));
		}

		bindHandler(Opcode.CALL_REF); {
			computeCurIpForTrap(-1);
			genSkipLeb(); // skip signature index
			decrementVsp();
			asm.movq_r_m(func_arg, vsph[0].value);
			asm.q.cmp_r_i(func_arg, 0);
			asm.jc_rel_near(X86_64Conds.NZ, callFunction);
			asm.jmp_rel_far(newTrapLabel(TrapReason.NULL_DEREF));
		}

		var tailCallFunction = X86_64Label.new();
		bindHandler(Opcode.RETURN_CALL); {
			genReadUleb32(r_tmp1);

			asm.movq_r_m(r_tmp0, r_instance.plus(offsets.Instance_functions));
			asm.movq_r_m(func_arg, r_tmp0.plusR(r_tmp1, offsets.REF_SIZE, offsets.Array_contents));

			// return_call_indirect and return_call_ref jump here
			asm.bind(tailCallFunction);
			asm.movq_m_r(m_wasm_func, func_arg);
			genInvalidateFrameAccessor();
			// Overwrite current locals with outgoing arguments
			var cnt = r_tmp0;
			asm.movq_r_m(cnt, func_arg.plus(offsets.Function_sig));
			asm.movq_r_m(cnt, cnt.plus(offsets.SigDecl_params));
			asm.movd_r_m(cnt, cnt.plus(offsets.Array_length));
			genCopyStackValsToVfp(cnt, r_tmp2);

			// Check if the target function is a WasmFunction or HostFunction
			masm.emit_br_r(xenv.func_arg, MasmBrCond.IS_NOT_WASM_FUNC, hostTailCallStubLabel);

			// WasmFunction: jump into interpreter reentry
			asm.q.lea(r_vsp, r_vfp.plusR(cnt, 1, 0)); // set VSP properly
			asm.movq_r_m(r_instance, func_arg.plus(offsets.WasmFunction_instance));
			saveIVar(r_instance);
			asm.movq_r_m(r_func_decl, func_arg.plus(offsets.WasmFunction_decl));
			saveIVar(r_func_decl);
			asm.jmp_rel_far(tailCallReentryLabel);
		}

		bindHandler(Opcode.RETURN_CALL_INDIRECT); {
			computeCurIpForTrap(-1);
			var sig_index = r_tmp1, table_index = r_tmp2, func_index = r_tmp0;
			genReadUleb32(sig_index);
			genReadUleb32(table_index);

			decrementVsp();
			asm.movd_r_m(func_index, vsph[0].value);

			var tmp = r_tmp3;
			// load instance.sig_ids[sig_index] into sig_index
			asm.movq_r_m(tmp, r_instance.plus(offsets.Instance_sig_ids));
			asm.movd_r_m(sig_index, tmp.plusR(sig_index, offsets.INT_SIZE, offsets.Array_contents));
			// Bounds-check table.ids[func_index]
			asm.movq_r_m(tmp, r_instance.plus(offsets.Instance_tables));
			var table = table_index;
			asm.movq_r_m(table, tmp.plusR(table_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(tmp, table.plus(offsets.Table_ids));
			asm.d.cmp_r_m(func_index, tmp.plus(offsets.Array_length));
			asm.jc_rel_far(C.NC, trap_func_invalid);
			// Check table.ids[func_index] == sig_index
			asm.d.cmp_r_m(sig_index, tmp.plusR(func_index, offsets.INT_SIZE, offsets.Array_contents));
			asm.jc_rel_near(C.NZ, check_sig_mismatch);
			// Load table.funcs[func_index] into r1 and jump to calling sequence
			asm.movq_r_m(tmp, table.plus(offsets.Table_funcs));
			asm.movq_r_m(func_arg, tmp.plusR(func_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.jmp_rel_near(tailCallFunction); // XXX: duplicate tail call sequence here?
		}

		bindHandler(Opcode.RETURN_CALL_REF); {
			computeCurIpForTrap(-1);
			genSkipLeb(); // skip signature index
			decrementVsp();
			asm.movq_r_m(func_arg, vsph[0].value);
			asm.q.cmp_r_i(func_arg, 0);
			asm.jc_rel_near(X86_64Conds.NZ, tailCallFunction);
			asm.jmp_rel_far(newTrapLabel(TrapReason.NULL_DEREF));
		}
	}
	def genCopyStackValsToVfp(cnt: X86_64Gpr, i: X86_64Gpr) {
		var done = X86_64Label.new();
		// Copy argument(s) from VSP to VFP.
		asm.cmp_r_i(cnt, 0);
		asm.jc_rel_near(C.Z, done);
		asm.movd_r_i(i, 0);
		asm.d.shl_r_i(cnt, valuerep.slot_size_log);
		asm.q.sub_r_r(r_vsp, cnt);
		var loop = X86_64Label.new();
		// while (i < cnt)
		asm.bind(loop);
		genCopySlot(r_vfp.plusR(i, 1, 0), r_vsp.plusR(i, 1, 0));
		asm.q.add_r_i(i, valuerep.slot_size);
		asm.q.cmp_r_r(i, cnt);
		asm.jc_rel_near(C.L, loop);
		asm.bind(done);
		// set VSP properly
		asm.q.lea(r_vsp, r_vfp.plusR(cnt, 1, 0));
	}
	def genLoadsAndStores() {
		genLoad(Opcode.I32_LOAD, BpTypeCode.I32.code, asm.movd_r_m);
		genLoad(Opcode.I64_LOAD, BpTypeCode.I64.code, asm.movq_r_m);
		genLoad(Opcode.F32_LOAD, BpTypeCode.F32.code, asm.movd_r_m);
		genLoad(Opcode.F64_LOAD, BpTypeCode.F64.code, asm.movq_r_m);
		genLoad(Opcode.I32_LOAD8_S, BpTypeCode.I32.code, asm.movbsx_r_m);
		genLoad(Opcode.I32_LOAD8_U, BpTypeCode.I32.code, asm.movbzx_r_m);
		genLoad(Opcode.I32_LOAD16_S, BpTypeCode.I32.code, asm.movwsx_r_m);
		genLoad(Opcode.I32_LOAD16_U, BpTypeCode.I32.code, asm.movwzx_r_m);
		genLoad(Opcode.I64_LOAD8_S, BpTypeCode.I64.code, asm.movbsx_r_m);
		genLoad(Opcode.I64_LOAD8_U, BpTypeCode.I64.code, asm.movbzx_r_m);
		genLoad(Opcode.I64_LOAD16_S, BpTypeCode.I64.code, asm.movwsx_r_m);
		genLoad(Opcode.I64_LOAD16_U, BpTypeCode.I64.code, asm.movwzx_r_m);
		genLoad(Opcode.I64_LOAD16_U, BpTypeCode.I64.code, asm.movwzx_r_m);
		genLoad(Opcode.I64_LOAD32_S, BpTypeCode.I64.code, masm.emit_movq_32s_r_m);
		genLoad(Opcode.I64_LOAD32_U, BpTypeCode.I64.code, asm.movd_r_m);

		bindHandler(Opcode.I32_STORE);
		bindHandler(Opcode.F32_STORE);
		bindHandler(Opcode.I64_STORE32);
		genStore(asm.movd_m_r);

		bindHandler(Opcode.I64_STORE);
		bindHandler(Opcode.F64_STORE);
		genStore(asm.movq_m_r);

		bindHandler(Opcode.I32_STORE8);
		bindHandler(Opcode.I64_STORE8);
		genStore(asm.movb_m_r);

		bindHandler(Opcode.I32_STORE16);
		bindHandler(Opcode.I64_STORE16);
		genStore(asm.movw_m_r);
	}
	def genAtomicWaitAndNotify() {
		for (t in [
			(Opcode.MEMORY_ATOMIC_NOTIFY, refRuntimeCall(RT.runtime_MEMORY_ATOMIC_NOTIFY)),
			(Opcode.MEMORY_ATOMIC_WAIT32, refRuntimeCall(RT.runtime_MEMORY_ATOMIC_WAIT32)),
			(Opcode.MEMORY_ATOMIC_WAIT64, refRuntimeCall(RT.runtime_MEMORY_ATOMIC_WAIT64))
		]) {
			bindHandler(t.0);
			asm.q.inc_r(r_ip);		// TODO: handle multi-memory, memory64
			genReadUleb32(r_tmp1);		// decode offset
			saveCallerIVars();
			asm.movd_r_i(r_tmp0, 0);
			callRuntime(t.1, [r_instance, r_tmp0, r_tmp1], true);
			restoreCallerIVars();
			endHandler();
		}
		// TODO: ATOMIC_FENCE
	}
	def genAtomicLoadsAndStores() {
		// Load Seq_Cst -> MOV (from memory)
		genLoad(Opcode.I32_ATOMIC_LOAD, BpTypeCode.I32.code, asm.movd_r_m);
		genLoad(Opcode.I64_ATOMIC_LOAD, BpTypeCode.I64.code, asm.movq_r_m);
		genLoad(Opcode.I32_ATOMIC_LOAD8_U, BpTypeCode.I32.code, asm.movbzx_r_m);
		genLoad(Opcode.I32_ATOMIC_LOAD16_U, BpTypeCode.I32.code, asm.movwzx_r_m);
		genLoad(Opcode.I64_ATOMIC_LOAD8_U, BpTypeCode.I64.code, asm.movbzx_r_m);
		genLoad(Opcode.I64_ATOMIC_LOAD16_U, BpTypeCode.I64.code, asm.movwzx_r_m);
		genLoad(Opcode.I64_ATOMIC_LOAD16_U, BpTypeCode.I64.code, asm.movwzx_r_m);
		genLoad(Opcode.I64_ATOMIC_LOAD32_U, BpTypeCode.I64.code, asm.movd_r_m);
		// Store Seq_Cst -> used: (LOCK) XCHG // alternative: MOV (into memory),MFENCE
		// The parenthesised (LOCK) reflects the fact that the XCHG instruction on x86 (including x86-64) has an implicit LOCK prefix
		bindHandler(Opcode.I32_ATOMIC_STORE);
		bindHandler(Opcode.I64_ATOMIC_STORE32);
		genStore(asm.xchgd_m_r);

		bindHandler(Opcode.I64_ATOMIC_STORE);
		genStore(asm.xchgq_m_r);

		bindHandler(Opcode.I32_ATOMIC_STORE8);
		bindHandler(Opcode.I64_ATOMIC_STORE8);
		genStore(asm.xchgb_m_r);

		bindHandler(Opcode.I32_ATOMIC_STORE16);
		bindHandler(Opcode.I64_ATOMIC_STORE16);
		genStore(asm.xchgw_m_r);
	}
	def emitI64_Load32s(dst: X86_64Gpr, addr: X86_64Addr) {
		asm.movd_r_m(dst, addr);
		asm.q.shl_r_i(dst, 32);
		asm.q.sar_r_i(dst, 32);
	}
	def genCompares() {
		// 32-bit integer compares
		for (t in [
			(Opcode.I32_EQ, C.Z),
			(Opcode.I32_NE, C.NZ),
			(Opcode.I32_LT_S, C.L),
			(Opcode.I32_LT_U, C.C),
			(Opcode.I32_GT_S, C.G),
			(Opcode.I32_GT_U, C.A),
			(Opcode.I32_LE_S, C.LE),
			(Opcode.I32_LE_U, C.NA),
			(Opcode.I32_GE_S, C.GE),
			(Opcode.I32_GE_U, C.NC)
		]) {
			bindHandler(t.0);
			asm.movd_r_m(r_tmp0, vsph[-1].value);
			asm.d.cmp_m_r(vsph[-2].value, r_tmp0);
			asm.set_r(t.1, r_tmp0);
			asm.movbzx_r_r(r_tmp0, r_tmp0);
			asm.movd_m_r(vsph[-2].value, r_tmp0);
			decrementVsp();
			endHandler();
		}
		// 64-bit integer compares
		bindHandler(Opcode.REF_EQ); // share handler with I64_EQ
		for (t in [
			(Opcode.I64_EQ, C.Z),
			(Opcode.I64_NE, C.NZ),
			(Opcode.I64_LT_S, C.L),
			(Opcode.I64_LT_U, C.C),
			(Opcode.I64_GT_S, C.G),
			(Opcode.I64_GT_U, C.A),
			(Opcode.I64_LE_S, C.LE),
			(Opcode.I64_LE_U, C.NA),
			(Opcode.I64_GE_S, C.GE),
			(Opcode.I64_GE_U, C.NC)
		]) {
			bindHandler(t.0);
			asm.movq_r_m(r_tmp0, vsph[-1].value);
			asm.q.cmp_m_r(vsph[-2].value, r_tmp0);
			asm.set_r(t.1, r_tmp0);
			asm.movbzx_r_r(r_tmp0, r_tmp0);
			asm.movq_m_r(vsph[-2].value, r_tmp0);
			if (valuerep.tagged) asm.movq_m_i(vsph[-2].tag, BpTypeCode.I32.code);
			decrementVsp();
			endHandler();
		}

	}
	def genI32Arith() {
		bindHandler(Opcode.I32_EQZ); {
			asm.d.test_m_i(vsph[-1].value, -1);
			asm.set_r(C.Z, r_tmp0);
			asm.movbzx_r_r(r_tmp0, r_tmp0);
			asm.movd_m_r(vsph[-1].value, r_tmp0);
			endHandler();
		}
		bindHandler(Opcode.I32_CLZ); {
			asm.movd_r_i(r_tmp1, -1);
			asm.d.bsr_r_m(r_tmp0, vsph[-1].value);
			asm.d.cmov_r(C.Z, r_tmp0, r_tmp1);
			asm.movd_r_i(r_tmp1, 31);
			asm.d.sub_r_r(r_tmp1, r_tmp0);
			asm.movd_m_r(vsph[-1].value, r_tmp1);
			endHandler();
		}
		bindHandler(Opcode.I32_CTZ); {
			asm.d.bsf_r_m(r_tmp0, vsph[-1].value);
			asm.movd_r_i(r_tmp1, 32);
			asm.d.cmov_r(C.Z, r_tmp0, r_tmp1);
			asm.movd_m_r(vsph[-1].value, r_tmp0);
			endHandler();
		}
		bindHandler(Opcode.I32_POPCNT); {
			asm.d.popcnt_r_m(r_tmp0, vsph[-1].value);
			asm.movd_m_r(vsph[-1].value, r_tmp0);
			endHandler();
		}
		bindHandler(Opcode.I32_MUL); {
			asm.movd_r_m(r_tmp0, vsph[-1].value);
			asm.d.imul_r_m(r_tmp0, vsph[-2].value);
			asm.movd_m_r(vsph[-2].value, r_tmp0);
			decrementVsp();
			endHandler();
		}
		for (t in [
			(Opcode.I32_DIV_S, masm.emit_i32_div_s, R.RAX),
			(Opcode.I32_DIV_U, masm.emit_i32_div_u, R.RAX),
			(Opcode.I32_REM_S, masm.emit_i32_rem_s, R.RDX),
			(Opcode.I32_REM_U, masm.emit_i32_rem_u, R.RDX)
		]) {
			bindHandler(t.0);
			computeCurIpForTrap(-1);
			asm.movd_r_m(r_tmp0, vsph[-1].value);
			spillReg(R.RAX);
			spillReg(R.RDX);
			asm.movd_r_m(R.RAX, vsph[-2].value);
			t.1(r_tmp0);
			asm.movd_m_r(vsph[-2].value, t.2);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			decrementVsp();
			endHandler();
		}
		for (t in [
			(Opcode.I32_ADD, asm.d.add_m_r),
			(Opcode.I32_SUB, asm.d.sub_m_r),
			(Opcode.I32_AND, asm.d.and_m_r),
			(Opcode.I32_OR, asm.d.or_m_r),
			(Opcode.I32_XOR, asm.d.xor_m_r)
		]) {
			bindHandler(t.0);
			asm.movd_r_m(r_tmp0, vsph[-1].value);
			t.1(vsph[-2].value, r_tmp0);
			decrementVsp();
			endHandler();
		}
		for (t in [
			(Opcode.I32_SHL, asm.d.shl_m_cl),
			(Opcode.I32_SHR_S, asm.d.sar_m_cl),
			(Opcode.I32_SHR_U, asm.d.shr_m_cl),
			(Opcode.I32_ROTL, asm.d.rol_m_cl),
			(Opcode.I32_ROTR, asm.d.ror_m_cl)
		]) {
			bindHandler(t.0);
			asm.movd_r_m(R.RCX, vsph[-1].value);
			t.1(vsph[-2].value);
			decrementVsp();
			endHandler();
		}
	}
	def genI64Arith() {
		bindHandler(Opcode.I64_EQZ); {
			asm.q.test_m_i(vsph[-1].value, -1);
			asm.set_r(C.Z, r_tmp0);
			asm.movbzx_r_r(r_tmp0, r_tmp0);
			asm.movd_m_r(vsph[-1].value, r_tmp0);
			if (valuerep.tagged) asm.movd_m_i(vsph[-1].tag, BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I64_CLZ); {
			asm.movq_r_i(r_tmp1, -1);
			asm.q.bsr_r_m(r_tmp0, vsph[-1].value);
			asm.q.cmov_r(C.Z, r_tmp0, r_tmp1);
			asm.movd_r_i(r_tmp1, 63);
			asm.q.sub_r_r(r_tmp1, r_tmp0);
			asm.movq_m_r(vsph[-1].value, r_tmp1);
			endHandler();
		}
		bindHandler(Opcode.I64_CTZ); {
			asm.q.bsf_r_m(r_tmp0, vsph[-1].value);
			asm.movd_r_i(r_tmp1, 64);
			asm.q.cmov_r(C.Z, r_tmp0, r_tmp1);
			asm.movq_m_r(vsph[-1].value, r_tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_POPCNT); {
			asm.q.popcnt_r_m(r_tmp0, vsph[-1].value);
			asm.movq_m_r(vsph[-1].value, r_tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_MUL); {
			asm.movq_r_m(r_tmp0, vsph[-1].value);
			asm.q.imul_r_m(r_tmp0, vsph[-2].value);
			asm.movq_m_r(vsph[-2].value, r_tmp0);
			decrementVsp();
			endHandler();
		}
		for (t in [
			(Opcode.I64_DIV_S, masm.emit_i64_div_s, R.RAX),
			(Opcode.I64_DIV_U, masm.emit_i64_div_u, R.RAX),
			(Opcode.I64_REM_S, masm.emit_i64_rem_s, R.RDX),
			(Opcode.I64_REM_U, masm.emit_i64_rem_u, R.RDX)
		]) {
			bindHandler(t.0);
			computeCurIpForTrap(-1);
			asm.movq_r_m(r_tmp0, vsph[-1].value);
			spillReg(R.RAX);
			spillReg(R.RDX);
			asm.movq_r_m(R.RAX, vsph[-2].value);
			t.1(r_tmp0);
			asm.movq_m_r(vsph[-2].value, t.2);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			decrementVsp();
			endHandler();
		}
		for (t in [
			(Opcode.I64_ADD, asm.q.add_m_r),
			(Opcode.I64_SUB, asm.q.sub_m_r),
			(Opcode.I64_AND, asm.q.and_m_r),
			(Opcode.I64_OR, asm.q.or_m_r),
			(Opcode.I64_XOR, asm.q.xor_m_r)
		]) {
			bindHandler(t.0);
			asm.movq_r_m(r_tmp0, vsph[-1].value);
			t.1(vsph[-2].value, r_tmp0);
			decrementVsp();
			endHandler();
		}
		for (t in [
			(Opcode.I64_SHL, asm.q.shl_m_cl),
			(Opcode.I64_SHR_S, asm.q.sar_m_cl),
			(Opcode.I64_SHR_U, asm.q.shr_m_cl),
			(Opcode.I64_ROTL, asm.q.rol_m_cl),
			(Opcode.I64_ROTR, asm.q.ror_m_cl)
		]) {
			bindHandler(t.0);
			asm.movq_r_m(R.RCX, vsph[-1].value);
			t.1(vsph[-2].value);
			decrementVsp();
			endHandler();
		}
	}
	def genAtomicRMWOperations() {
		// Atomic RMW Add
		bindHandler(Opcode.I32_ATOMIC_RMW_ADD);
		bindHandler(Opcode.I64_ATOMIC_RMW32_ADD_U);
		genAtomicAdd(asm.d.xadd_m_r);
		bindHandler(Opcode.I64_ATOMIC_RMW_ADD);
		genAtomicAdd(asm.q.xadd_m_r);
		bindHandler(Opcode.I32_ATOMIC_RMW8_ADD_U);
		bindHandler(Opcode.I64_ATOMIC_RMW8_ADD_U);
		genAtomicAdd(asm.xaddb_m_r);
		bindHandler(Opcode.I32_ATOMIC_RMW16_ADD_U);
		bindHandler(Opcode.I64_ATOMIC_RMW16_ADD_U);
		genAtomicAdd(asm.xaddw_m_r);
		// Atomic RMW Sub
		bindHandler(Opcode.I32_ATOMIC_RMW_SUB);
		bindHandler(Opcode.I64_ATOMIC_RMW32_SUB_U);
		genAtomicSub(asm.d.xadd_m_r, asm.d.neg_r);
		bindHandler(Opcode.I64_ATOMIC_RMW_SUB);
		genAtomicSub(asm.q.xadd_m_r, asm.q.neg_r);
		bindHandler(Opcode.I32_ATOMIC_RMW8_SUB_U);
		bindHandler(Opcode.I64_ATOMIC_RMW8_SUB_U);
		genAtomicSub(asm.xaddb_m_r, asm.negb_r);
		bindHandler(Opcode.I32_ATOMIC_RMW16_SUB_U);
		bindHandler(Opcode.I64_ATOMIC_RMW16_SUB_U);
		genAtomicSub(asm.xaddw_m_r, asm.negw_r);
		// Atomic RMW And
		bindHandler(Opcode.I32_ATOMIC_RMW_AND);
		bindHandler(Opcode.I64_ATOMIC_RMW32_AND_U);
		genAtomicBinop(asm.andd_m_r, asm.xchgd_m_r);
		bindHandler(Opcode.I64_ATOMIC_RMW_AND);
		genAtomicBinop(asm.andq_m_r, asm.xchgq_m_r);
		bindHandler(Opcode.I32_ATOMIC_RMW8_AND_U);
		bindHandler(Opcode.I64_ATOMIC_RMW8_AND_U);
		genAtomicBinop(asm.andb_m_r, asm.xchgb_m_r);
		bindHandler(Opcode.I32_ATOMIC_RMW16_AND_U);
		bindHandler(Opcode.I64_ATOMIC_RMW16_AND_U);
		genAtomicBinop(asm.andw_m_r, asm.xchgw_m_r);
		// Atomic RMW Or
		bindHandler(Opcode.I32_ATOMIC_RMW_OR);
		bindHandler(Opcode.I64_ATOMIC_RMW32_OR_U);
		genAtomicBinop(asm.ord_m_r, asm.xchgd_m_r);
		bindHandler(Opcode.I64_ATOMIC_RMW_OR);
		genAtomicBinop(asm.orq_m_r, asm.xchgq_m_r);
		bindHandler(Opcode.I32_ATOMIC_RMW8_OR_U);
		bindHandler(Opcode.I64_ATOMIC_RMW8_OR_U);
		genAtomicBinop(asm.orb_m_r, asm.xchgb_m_r);
		bindHandler(Opcode.I32_ATOMIC_RMW16_OR_U);
		bindHandler(Opcode.I64_ATOMIC_RMW16_OR_U);
		genAtomicBinop(asm.orw_m_r, asm.xchgw_m_r);
		// Atomic RMW Xor
		bindHandler(Opcode.I32_ATOMIC_RMW_XOR);
		bindHandler(Opcode.I64_ATOMIC_RMW32_XOR_U);
		genAtomicBinop(asm.xord_m_r, asm.xchgd_m_r);
		bindHandler(Opcode.I64_ATOMIC_RMW_XOR);
		genAtomicBinop(asm.xorq_m_r, asm.xchgq_m_r);
		bindHandler(Opcode.I32_ATOMIC_RMW8_XOR_U);
		bindHandler(Opcode.I64_ATOMIC_RMW8_XOR_U);
		genAtomicBinop(asm.xorb_m_r, asm.xchgb_m_r);
		bindHandler(Opcode.I32_ATOMIC_RMW16_XOR_U);
		bindHandler(Opcode.I64_ATOMIC_RMW16_XOR_U);
		genAtomicBinop(asm.xorw_m_r, asm.xchgw_m_r);
		// Atomic RMW xchg
		bindHandler(Opcode.I32_ATOMIC_RMW_XCHG);
		bindHandler(Opcode.I64_ATOMIC_RMW32_XCHG_U);
		genAtomicExchange(asm.xchgd_m_r);
		bindHandler(Opcode.I64_ATOMIC_RMW_XCHG);
		genAtomicExchange(asm.xchgq_m_r);
		bindHandler(Opcode.I32_ATOMIC_RMW8_XCHG_U);
		bindHandler(Opcode.I64_ATOMIC_RMW8_XCHG_U);
		genAtomicExchange(asm.xchgb_m_r);
		bindHandler(Opcode.I32_ATOMIC_RMW16_XCHG_U);
		bindHandler(Opcode.I64_ATOMIC_RMW16_XCHG_U);
		genAtomicExchange(asm.xchgw_m_r);
		// Atomic RMW cmpxchg
		bindHandler(Opcode.I32_ATOMIC_RMW_CMPXCHG);
		bindHandler(Opcode.I64_ATOMIC_RMW32_CMPXCHG_U);
		genAtomicCompareAndExchange(asm.d.cmpxchg_m_r);
		bindHandler(Opcode.I64_ATOMIC_RMW_CMPXCHG);
		genAtomicCompareAndExchange(asm.q.cmpxchg_m_r);
		bindHandler(Opcode.I32_ATOMIC_RMW8_CMPXCHG_U);
		bindHandler(Opcode.I64_ATOMIC_RMW8_CMPXCHG_U);
		genAtomicCompareAndExchange(asm.cmpxchgb_m_r);
		bindHandler(Opcode.I32_ATOMIC_RMW16_CMPXCHG_U);
		bindHandler(Opcode.I64_ATOMIC_RMW16_CMPXCHG_U);
		genAtomicCompareAndExchange(asm.cmpxchgw_m_r);
    }
    def genAtomicOp<T>(op: (X86_64Addr, X86_64Gpr) -> T,
                       	neg: (X86_64Gpr) -> T,
                		exchange: (X86_64Addr, X86_64Gpr) -> T,
						isCmpAndExchange: bool) {
		computeCurIpForTrap(-1);
		var finish = asm.newLabel(), has_index: X86_64Label;
		if (!FeatureDisable.multiMemory) { // dynamically check for memory index
			has_index = asm.newLabel();
			asm.q.inc_r(r_ip);				// skip flags byte
			asm.test_m_i(r_ip.plus(-1), BpConstants.MEMARG_INDEX_FLAG); // XXX: test byte
			asm.jc_rel_near(C.NZ, has_index);
		} else {
			asm.q.inc_r(r_ip);				// skip flags byte
		}
		genReadUleb32(r_tmp0);			// decode offset
		if (isCmpAndExchange) {
			asm.movq_r_m(r_tmp1, vsph[-3].value);	 // read index
			asm.q.add_r_r(r_tmp0, r_tmp1);	      // add index + offset
			asm.movq_r_m(r_tmp1, vsph[-2].value);	  // new value for cmpxchg
			spillReg(R.RAX);
			asm.movq_r_m(R.RAX, vsph[-1].value);
			asm.lock();
			op(r_mem0_base.plusR(r_tmp0, 1, 0), r_tmp1);
			// asm.movq_r_m(r_tmp1, r_mem0_base.plusR(r_tmp0, 1, 0)); // This will return the return of the operation
			restoreReg(R.RAX);  // Restore the original RAX if it was used elsewhere
			decrementVsp();
		} else {
			asm.movq_r_m(r_tmp1, vsph[-2].value);	// read index
			asm.q.add_r_r(r_tmp0, r_tmp1);		// add index + offset
			asm.movq_r_m(r_tmp1, vsph[-1].value);	// read value
			if (neg != null) {
					neg(r_tmp1);
			} else if (exchange != null) {
					exchange(r_mem0_base.plusR(r_tmp0, 1, 0), r_tmp1);
			}
			asm.lock();
			op(r_mem0_base.plusR(r_tmp0, 1, 0), r_tmp1);
			// asm.movq_r_m(r_tmp1, r_mem0_base.plusR(r_tmp0, 1, 0)); // This will return the return of the operation
			asm.xchgq_m_r(vsph[-1].value, r_tmp1);
		}
		asm.bind(finish);
		endHandler();
		if (has_index != null) {
			asm.bind(has_index);
			genReadUleb32(r_tmp0);			// decode memory index
			var memN = r_tmp3;
			asm.movq_r_m(memN, r_instance.plus(offsets.Instance_memories));
			asm.movq_r_m(memN, memN.plusR(r_tmp0, 8, offsets.Array_contents));
			asm.movq_r_m(memN, memN.plus(offsets.X86_64Memory_start));
			genReadUleb32(r_tmp0);			// decode offset

			if (isCmpAndExchange) {
				asm.movq_r_m(r_tmp1, vsph[-3].value); 	// read index
				asm.q.add_r_r(r_tmp0, r_tmp1);		// add index + offset
				asm.movq_r_m(r_tmp1, vsph[-2].value);	// new value for cmpxchg
				spillReg(R.RAX);
				asm.movq_r_m(R.RAX, vsph[-1].value);
				asm.lock();
				op(r_mem0_base.plusR(r_tmp0, 1, 0), r_tmp1);
				// asm.movq_r_m(r_tmp1, r_mem0_base.plusR(r_tmp0, 1, 0)); // This will return the return of the operation
				restoreReg(R.RAX);  // Restore the original RAX if it was used elsewhere
				decrementVsp();
			} else {
				asm.movq_r_m(r_tmp1, vsph[-2].value);	// read index
				asm.q.add_r_r(r_tmp0, r_tmp1);		// add index + offset
				asm.movq_r_m(r_tmp1, vsph[-1].value);	// read value
				if (neg != null) {
						neg(r_tmp1);
				} else if (exchange != null) {
						exchange(r_mem0_base.plusR(r_tmp0, 1, 0), r_tmp1);
				}
				asm.lock();
				op(r_mem0_base.plusR(r_tmp0, 1, 0), r_tmp1);
				// asm.movq_r_m(r_tmp1, r_mem0_base.plusR(r_tmp0, 1, 0)); // This will return the return of the operation
				asm.xchgq_m_r(vsph[-1].value, r_tmp1);
			}
			asm.jmp_rel_near(finish);
		}
	}
	def genAtomicAdd<T>(add: (X86_64Addr, X86_64Gpr) -> T) {
		genAtomicOp(add, null, null, false);
	}
	def genAtomicSub<T>(add: (X86_64Addr, X86_64Gpr) -> T, neg: (X86_64Gpr) -> T) {
		genAtomicOp(add, neg, null, false);
	}
	def genAtomicBinop<T>(binop: (X86_64Addr, X86_64Gpr) -> T, exchange: (X86_64Addr, X86_64Gpr) -> T) {
		genAtomicOp(binop, null, exchange, false);
	}
	def genAtomicExchange<T>(exchange: (X86_64Addr, X86_64Gpr) -> T) {
		genAtomicOp(exchange, null, null, false);
	}
	def genAtomicCompareAndExchange<T>(cmpxchg: (X86_64Addr, X86_64Gpr) -> T) {
		genAtomicOp(cmpxchg, null, null, true);
	}
	def genExtensions() {
		bindHandler(Opcode.I32_WRAP_I64); {
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I32_REINTERPRET_F32); {
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I64_REINTERPRET_F64); {
			genTagUpdate(BpTypeCode.I64.code);
			endHandler();
		}
		bindHandler(Opcode.F32_REINTERPRET_I32); {
			genTagUpdate(BpTypeCode.F32.code);
			endHandler();
		}
		bindHandler(Opcode.F64_REINTERPRET_I64); {
			genTagUpdate(BpTypeCode.F64.code);
			endHandler();
		}
		bindHandler(Opcode.I32_EXTEND8_S); {
			asm.d.movbsx_r_m(r_tmp0, vsph[-1].value);
			asm.movd_m_r(vsph[-1].value, r_tmp0);
			endHandler();
		}
		bindHandler(Opcode.I32_EXTEND16_S); {
			asm.d.movwsx_r_m(r_tmp0, vsph[-1].value);
			asm.movd_m_r(vsph[-1].value, r_tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND8_S); {
			asm.q.movbsx_r_m(r_tmp0, vsph[-1].value);
			asm.movq_m_r(vsph[-1].value, r_tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND16_S); {
			asm.q.movwsx_r_m(r_tmp0, vsph[-1].value);
			asm.movq_m_r(vsph[-1].value, r_tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND_I32_S);
		bindHandler(Opcode.I64_EXTEND32_S); {
			genTagUpdate(BpTypeCode.I64.code);
			asm.movd_r_m(r_tmp0, vsph[-1].value);
			asm.q.shl_r_i(r_tmp0, 32);
			asm.q.sar_r_i(r_tmp0, 32);
			asm.movq_m_r(vsph[-1].value, r_tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND_I32_U); {
			genTagUpdate(BpTypeCode.I64.code);
			asm.movd_m_i(vsph[-1].value.plus(4), 0); // zero upper portion
			endHandler();
		}
	}
	def genF32Arith() {
		bindHandler(Opcode.F32_ABS); {
			asm.d.and_m_i(vsph[-1].value, 0x7FFFFFFF); // explicit update of upper word
			endHandler();
		}
		bindHandler(Opcode.F32_NEG); {
			asm.d.xor_m_i(vsph[-1].value, 0x80000000); // explicit update of upper word
			endHandler();
		}
		bindHandler(Opcode.F32_ADD); {
			asm.movss_s_m(r_xmm0, vsph[-2].value);
			asm.addss_s_m(r_xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-2].value, r_xmm0);
			decrementVsp();
			endHandler();
		}
		bindHandler(Opcode.F32_SUB); {
			asm.movss_s_m(r_xmm0, vsph[-2].value);
			asm.subss_s_m(r_xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-2].value, r_xmm0);
			decrementVsp();
			endHandler();
		}
		bindHandler(Opcode.F32_MUL); {
			asm.movss_s_m(r_xmm0, vsph[-2].value);
			asm.mulss_s_m(r_xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-2].value, r_xmm0);
			decrementVsp();
			endHandler();
		}
		bindHandler(Opcode.F32_DIV); {
			asm.movss_s_m(r_xmm0, vsph[-2].value);
			asm.divss_s_m(r_xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-2].value, r_xmm0);
			decrementVsp();
			endHandler();
		}
		bindHandler(Opcode.F32_SQRT); {
			asm.sqrtss_s_m(r_xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_COPYSIGN); {
			asm.movd_r_m(r_tmp0, vsph[-2].value); // XXX: tradeoff between memory operands and extra regs?
			asm.d.and_r_i(r_tmp0, 0x7FFFFFFF);
			asm.movd_r_m(r_tmp1, vsph[-1].value);
			asm.d.and_r_i(r_tmp1, 0x80000000);
			asm.d.or_r_r(r_tmp0, r_tmp1);
			asm.movd_m_r(vsph[-2].value, r_tmp0);
			decrementVsp();
			endHandler();
		}
		for (t in [
			(Opcode.F32_CEIL, X86_64Rounding.TO_POS_INF),
			(Opcode.F32_FLOOR, X86_64Rounding.TO_NEG_INF),
			(Opcode.F32_TRUNC, X86_64Rounding.TO_ZERO),
			(Opcode.F32_NEAREST, X86_64Rounding.TO_NEAREST)
		]) {
			bindHandler(t.0);
			asm.roundss_s_m(r_xmm0, vsph[-1].value, t.1);
			asm.movss_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
	}
	def genF64Arith() {
		bindHandler(Opcode.F64_ABS); {
			asm.d.and_m_i(vsph[-1].upper, 0x7FFFFFFF);
			endHandler();
		}
		bindHandler(Opcode.F64_NEG); {
			asm.d.xor_m_i(vsph[-1].upper, 0x80000000);
			endHandler();
		}
		bindHandler(Opcode.F64_ADD); {
			asm.movsd_s_m(r_xmm0, vsph[-2].value);
			asm.addsd_s_m(r_xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-2].value, r_xmm0);
			decrementVsp();
			endHandler();
		}
		bindHandler(Opcode.F64_SUB); {
			asm.movsd_s_m(r_xmm0, vsph[-2].value);
			asm.subsd_s_m(r_xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-2].value, r_xmm0);
			decrementVsp();
			endHandler();
		}
		bindHandler(Opcode.F64_MUL); {
				asm.movsd_s_m(r_xmm0, vsph[-2].value);
				asm.mulsd_s_m(r_xmm0, vsph[-1].value);
				asm.movsd_m_s(vsph[-2].value, r_xmm0);
				decrementVsp();
			endHandler();
		}
		bindHandler(Opcode.F64_DIV); {
			asm.movsd_s_m(r_xmm0, vsph[-2].value);
			asm.divsd_s_m(r_xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-2].value, r_xmm0);
			decrementVsp();
			endHandler();
		}
		bindHandler(Opcode.F64_SQRT); {
			asm.sqrtsd_s_m(r_xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_COPYSIGN); {
			asm.movd_r_m(r_tmp0, vsph[-2].upper); // XXX: tradeoff between memory operands and extra regs?
			asm.d.and_r_i(r_tmp0, 0x7FFFFFFF);
			asm.movd_r_m(r_tmp1, vsph[-1].upper);
			asm.d.and_r_i(r_tmp1, 0x80000000);
			asm.d.or_r_r(r_tmp0, r_tmp1);
			asm.movd_m_r(vsph[-2].upper, r_tmp0);
			decrementVsp();
			endHandler();
		}
		for (t in [
			(Opcode.F64_CEIL, X86_64Rounding.TO_POS_INF),
			(Opcode.F64_FLOOR, X86_64Rounding.TO_NEG_INF),
			(Opcode.F64_TRUNC, X86_64Rounding.TO_ZERO),
			(Opcode.F64_NEAREST, X86_64Rounding.TO_NEAREST)
		]) {
			bindHandler(t.0);
			asm.roundsd_s_m(r_xmm0, vsph[-1].value, t.1);
			asm.movsd_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
	}
	def genFloatCmps() {
		var ret_zero = X86_64Label.new(), ret_one = X86_64Label.new();
		for (t in [
			(Opcode.F32_EQ, C.NZ),
			(Opcode.F32_NE, C.Z),
			(Opcode.F32_LT, C.NC),
			(Opcode.F32_GT, C.NA),
			(Opcode.F32_LE, C.A),
			(Opcode.F32_GE, C.C)]) {
			bindHandler(t.0);
			asm.movss_s_m(r_xmm0, vsph[-2].value);
			asm.ucomiss_s_m(r_xmm0, vsph[-1].value);
			asm.jc_rel_near(C.P, if(t.0 == Opcode.F32_NE, ret_one, ret_zero));
			asm.jc_rel_near(t.1, ret_zero);
			asm.jmp_rel_near(ret_one);
		}

		asm.bind(ret_zero);
		decrementVsp();
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsph[-1].value, 0);
		endHandler();

		asm.bind(ret_one);
		decrementVsp();
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsph[-1].value, 1);
		endHandler();

		// XXX: too far of a near jump to share these between f32 and f64
		ret_zero = X86_64Label.new();
		ret_one = X86_64Label.new();
		for (t in [
			(Opcode.F64_EQ, C.NZ),
			(Opcode.F64_NE, C.Z),
			(Opcode.F64_LT, C.NC),
			(Opcode.F64_GT, C.NA),
			(Opcode.F64_LE, C.A),
			(Opcode.F64_GE, C.C)]) {
			bindHandler(t.0);
			asm.movsd_s_m(r_xmm0, vsph[-2].value);
			asm.ucomisd_s_m(r_xmm0, vsph[-1].value);
			asm.jc_rel_near(C.P, if(t.0 == Opcode.F64_NE, ret_one, ret_zero));
			asm.jc_rel_near(t.1, ret_zero);
			asm.jmp_rel_near(ret_one);
		}

		asm.bind(ret_zero);
		decrementVsp();
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsph[-1].value, 0);
		genDispatchOrJumpToDispatch();

		asm.bind(ret_one);
		decrementVsp();
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsph[-1].value, 1);
		genDispatchOrJumpToDispatch();
	}
	def genGcInstrs() {
		bindHandler(Opcode.REF_I31); {
			asm.movd_r_m(r_tmp0, vsph[-1].value);
			asm.d.shl_r_i(r_tmp0, 1);
			asm.d.or_r_i(r_tmp0, 1);
			asm.movq_m_r(vsph[-1].value, r_tmp0);
			genTagUpdate(BpTypeCode.I31REF.code);
			endHandler();
		}
		bindHandler(Opcode.I31_GET_S); {
			asm.movd_r_m(r_tmp0, vsph[-1].value);
			asm.d.cmp_r_i(r_tmp0, 0);
			asm.jc_rel_far(X86_64Conds.Z, newTrapLabel(TrapReason.NULL_DEREF));
			asm.d.sar_r_i(r_tmp0, 1);
			asm.movq_m_r(vsph[-1].value, r_tmp0);
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I31_GET_U); {
			asm.movd_r_m(r_tmp0, vsph[-1].value);
			asm.d.cmp_r_i(r_tmp0, 0);
			asm.jc_rel_far(X86_64Conds.Z, newTrapLabel(TrapReason.NULL_DEREF));
			asm.d.shr_r_i(r_tmp0, 1);
			asm.movd_m_r(vsph[-1].value, r_tmp0);
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.ARRAY_LEN); {
			asm.movq_r_m(r_tmp0, vsph[-1].value);
			asm.q.cmp_r_i(r_tmp0, 0);
			asm.jc_rel_far(X86_64Conds.Z, newTrapLabel(TrapReason.NULL_DEREF));
			asm.movq_r_m(r_tmp0, r_tmp0.plus(offsets.HeapArray_vals));
			asm.movd_r_m(r_tmp0, r_tmp0.plus(offsets.Array_length));
			asm.movd_m_r(vsph[-1].value, r_tmp0);
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.REF_TEST); {
			var nullable_reg = r_tmp3;
			var shared = X86_64Label.new();
			asm.movd_r_i(nullable_reg, 0);
			asm.bind(shared); // shared code between ref.test and ref.test_null
			genReadSleb32_inline(r_tmp1);
			saveCallerIVars();
			callRuntime(refRuntimeCall(RT.runtime_doCast), [r_instance, nullable_reg, r_tmp1], false);
			asm.movbzx_r_r(r_tmp0, Target.V3_RET_GPRS[0]); // XXX: restore just VSP and update first?
			restoreCallerIVars();
			asm.movd_m_r(vsph[-1].value, r_tmp0);
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
			// ref.test_null jumps back to ref.test
			bindHandler(Opcode.REF_TEST_NULL);
			asm.movd_r_i(nullable_reg, 2);
			asm.jmp_rel_near(shared);
		}
		bindHandler(Opcode.REF_CAST); {
			var nullable_reg = r_tmp3;
			var shared = X86_64Label.new();
			asm.movd_r_i(nullable_reg, 0);
			asm.bind(shared); // shared code between ref.cast and ref.cast_null
			genReadSleb32_inline(r_tmp1);
			saveCallerIVars();
			callRuntime(refRuntimeCall(RT.runtime_doCast), [r_instance, nullable_reg, r_tmp1], false);
			restoreCurPcFromFrame();
			asm.cmpb_r_i(Target.V3_RET_GPRS[0], 0);
			asm.jc_rel_far(C.Z, newTrapLabel(TrapReason.FAILED_CAST));
			restoreCallerIVars();
			endHandler();
			// ref.cast_null jumps back to ref.test
			bindHandler(Opcode.REF_CAST_NULL);
			asm.movd_r_i(nullable_reg, 2);
			asm.jmp_rel_near(shared);
		}
		for (t in [
			(true, Opcode.BR_ON_CAST),
			(false, Opcode.BR_ON_CAST_FAIL)]) {
			bindHandler(t.1);
			var nullable_reg = r_tmp3;
			var shared = X86_64Label.new();
			asm.bind(shared); 			// shared code between br_on_cast and br_on_cast_null
			asm.movbzx_r_m(nullable_reg, r_ip.indirect());
			asm.q.add_r_i(r_ip, 1);
			genSkipLeb();            	// skip target label
			genSkipLeb();            	// skip src heaptype
			genReadSleb32_inline(r_tmp1);	// read dest heaptype
			saveCallerIVars();
			callRuntime(refRuntimeCall(RT.runtime_doCast), [r_instance, nullable_reg, r_tmp1], false);
			restoreCurPcFromFrame();
			asm.cmpb_r_i(Target.V3_RET_GPRS[0], 0);
			restoreCallerIVars();
			var cond = if(t.0, C.Z, C.NZ); // TODO: br_on_cast/fail only differ on condition
			asm.jc_rel_far(cond, controlSkipSidetableAndDispatchLabel); // XXX: relies on no changes to flags
			restoreIpFromCurIp(-1); // TODO: sidetable entries are relative to current IP; we are at nextIP
			asm.jmp_rel_far(controlTransferLabel);
		}
		bindHandler(Opcode.REF_AS_NON_NULL); {
			computeCurIpForTrap(-1);
			asm.q.cmp_m_i(vsph[-1].value, 0);
			asm.jc_rel_far(C.Z, newTrapLabel(TrapReason.NULL_DEREF));
			endHandler();
		}
		// internalize/externalize are nops.
		patchDispatchTable(Opcode.ANY_CONVERT_EXTERN, firstDispatchOffset);
		patchDispatchTable(Opcode.EXTERN_CONVERT_ANY, firstDispatchOffset);
	}
	def genMisc() {
		var m_curStack = X86_64Addr.new(null, null, 1, int.view(offsets.X86_64Runtime_curStack - Pointer.NULL));
		bindHandler(Opcode.MEMORY_SIZE); {
			genReadUleb32(r_tmp1);
			asm.movq_r_m(r_tmp0, r_instance.plus(offsets.Instance_memories));
			asm.movq_r_m(r_tmp0, r_tmp0.plusR(r_tmp1, offsets.REF_SIZE, offsets.Array_contents));
			asm.movd_r_m(r_tmp1, r_tmp0.plus(offsets.X86_64Memory_num_pages));
			asm.movb_r_m(r_tmp0, r_tmp0.plus(offsets.X86_64Memory_index_tag));
			genTagPushR(r_tmp0);
			asm.movq_m_r(vsph[0].value, r_tmp1);
			incrementVsp();
			endHandler();
		}
		bindHandler(Opcode.REF_NULL); {
			genSkipLeb();
			genTagPush(BpTypeCode.REF_NULL.code);
			asm.movq_m_i(vsph[0].value, 0);
			incrementVsp();
			endHandler();
		}
		bindHandler(Opcode.REF_IS_NULL); {
			asm.d.test_m_i(vsph[-1].value, -1);
			asm.set_r(C.Z, r_tmp0);
			asm.movbzx_r_r(r_tmp0, r_tmp0);
			if (valuerep.tagged) asm.movd_m_i(vsph[-1].tag, i7.view(BpTypeCode.I32.code));
			asm.movd_m_r(vsph[-1].value, r_tmp0);
			endHandler();
		}
		bindHandler(Opcode.REF_FUNC); {
			genReadUleb32(r_tmp1);
			asm.movq_r_m(r_tmp0, r_instance.plus(offsets.Instance_functions));
			asm.movq_r_m(r_tmp0, r_tmp0.plusR(r_tmp1, offsets.REF_SIZE, offsets.Array_contents));
			genTagPush(BpTypeCode.FUNCREF.code);
			asm.movq_m_r(vsph[0].value, r_tmp0);
			incrementVsp();
			endHandler();
		}
		bindHandler(Opcode.DATA_DROP); {
			genReadUleb32(r_tmp1);
			asm.movq_r_m(r_tmp0, r_instance.plus(offsets.Instance_dropped_data));
			asm.movb_m_i(r_tmp0.plusR(r_tmp1, 1, offsets.Array_contents), 1);
			endHandler();
		}
		bindHandler(Opcode.ELEM_DROP); {
			genReadUleb32(r_tmp1);
			asm.movq_r_m(r_tmp0, r_instance.plus(offsets.Instance_dropped_elems));
			asm.movb_m_i(r_tmp0.plusR(r_tmp1, 1, offsets.Array_contents), 1);
			endHandler();
		}
		bindHandler(Opcode.TABLE_SIZE); {
			genReadUleb32(r_tmp1);
			asm.movq_r_m(r_tmp0, r_instance.plus(offsets.Instance_tables));
			asm.movq_r_m(r_tmp0, r_tmp0.plusR(r_tmp1, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(r_tmp0, r_tmp0.plus(offsets.Table_elems));
			asm.movq_r_m(r_tmp0, r_tmp0.plus(offsets.Array_length));
			genTagPush(BpTypeCode.I32.code);
			asm.movq_m_r(vsph[0].value, r_tmp0);
			incrementVsp();
			endHandler();
		}
		// ext:typed-continuation
		bindHandler(Opcode.CONT_NEW); {
			computeCurIpForTrap(-1);
			computePcFromCurIp();
			genReadUleb32(r_tmp0);
			saveCallerIVars();
			callRuntime(refRuntimeCall(RT.runtime_CONT_NEW), [r_instance, r_tmp0], true);
			restoreCallerIVars();
			endHandler();
		}
		bindHandler(Opcode.CONT_BIND); {
			computeCurIpForTrap(-1);
			computePcFromCurIp();
			genReadUleb32(r_tmp0);
			genReadUleb32(r_tmp1);
			saveCallerIVars();
			callRuntime(refRuntimeCall(RT.runtime_CONT_BIND), [r_instance, r_tmp0, r_tmp1], true);
			restoreCallerIVars();
			endHandler();
		}
		bindHandler(Opcode.RESUME); {
			var cont_id = r_tmp0, ip_store = r_tmp1;
			computeCurIpForTrap(-1);
			computePcFromCurIp();
			// read info and rewind %ip (need %ip at start of opcode during call)
			asm.movq_r_r(ip_store, r_ip);
			genReadUleb32(cont_id); // read signature index
			asm.movq_r_r(r_ip, ip_store);

			saveCallerIVars();
			callRuntime(refRuntimeCall(RT.runtime_process_resume), [r_instance, cont_id], true);
			asm.call_rel_far(resumeStub);

			/* === CHILD STACK RUNNING === */

			genOnResumeFinish(false);
			endHandler();
		}
		bindHandler(Opcode.RESUME_THROW); {
			var cont_id = r_tmp0, ip_store = r_tmp1, tag_index = r_tmp3;
			computeCurIpForTrap(-1);
			computePcFromCurIp();
			// read info and rewind %ip (need %ip at start of opcode during call)
			asm.movq_r_r(ip_store, r_ip);
			genReadUleb32(cont_id); // read signature index
			genReadUleb32(tag_index); // read tag index
			asm.movq_r_r(r_ip, ip_store);

			saveCallerIVars();
			callRuntime(refRuntimeCall(RT.runtime_process_resume_throw), [r_instance, cont_id, tag_index], true);
			asm.call_rel_far(resumeStub);

			/* === CHILD STACK RUNNING === */

			genOnResumeFinish(true);
			endHandler();
		}
		bindHandler(Opcode.SUSPEND); {
			var tag_index = r_tmp0;
			computeCurIpForTrap(-1);
			computePcFromCurIp();
			genReadUleb32(tag_index); // read tag index

			saveCallerIVars();
			callRuntime(refRuntimeCall(RT.runtime_handle_suspend), [r_instance, tag_index], true);
			asm.call_rel_far(suspendStub);

			/* === PARENT STACK RUNNING === */

			restoreCallerIVars();
			restoreDispatchTableReg();
			endHandler();
    }
		writeDispatchEntry(dispatchTables[0].1, InternalOpcode.PROBE.code, w.atEnd().pos); {
			computeCurIpFromIp(-1);
			computePcFromCurIp();
			saveCallerIVars();
			asm.movq_r_m(r_tmp0, m_wasm_func); // XXX: compute func and pc directly in the right regs
			callRuntime(refRuntimeCall(RT.runtime_PROBE_instr), [r_tmp0, r_curpc], true);
			restoreCallerIVars();
			// Compute a pointer to the original code at this pc offset
			var pc = r_tmp1; // = IP - CODE
			asm.movq_r_r(pc, r_ip);
			asm.sub_r_m(pc, m_code);
			var origIp = r_tmp0; // FUNC_DECL.orig_bytecode + pc - 1
			asm.movq_r_m(origIp, r_func_decl.plus(offsets.FuncDecl_orig_bytecode));
			asm.add_r_r(origIp, pc);
			asm.sub_r_i(origIp, 1);
			genDispatch0(origIp.indirect(), dispatchTables[0].1, false);
		}
		// specialized handler for whamm probes
		if (FastIntTuning.enableWhammProbeTrampoline) {
			writeDispatchEntry(dispatchTables[0].1, InternalOpcode.WHAMM_PROBE.code, w.atEnd().pos); {
				// call runtime to get the WhammProbe object
				computeCurIpFromIp(-1);
				computePcFromCurIp();
				saveCallerIVars();
				asm.movq_r_m(r_tmp0, m_wasm_func);
				callRuntime(refRuntimeCall(RT.runtime_GET_LOCAL_PROBE), [r_tmp0, r_curpc], false);

				// jump to the trampoline
				asm.movq_r_m(r_vsp, m_vsp);
				masm.emit_mov_r_m(ValueKind.REF, xenv.func_arg, MasmAddr(xenv.runtime_ret0, masm.offsets.WhammProbe_func));
				masm.emit_mov_r_m(ValueKind.REF, xenv.runtime_ret0, MasmAddr(xenv.runtime_ret0, masm.offsets.WhammProbe_trampoline));
				masm.emit_jump_r(xenv.runtime_ret0);

				// reentry point from the trampoline
				ic.header.whammReentryOffset = w.atEnd().pos;
				restoreCallerIVars();
				// Compute a pointer to the original code at this pc offset
				var pc = r_tmp1; // = IP - CODE
				asm.movq_r_r(pc, r_ip);
				asm.sub_r_m(pc, m_code);
				var origIp = r_tmp0; // FUNC_DECL.orig_bytecode + pc - 1
				asm.movq_r_m(origIp, r_func_decl.plus(offsets.FuncDecl_orig_bytecode));

				asm.add_r_r(origIp, pc);
				asm.sub_r_i(origIp, 1);
				genDispatch0(origIp.indirect(), dispatchTables[0].1, false);
			}
		}
	}
	def genGlobalProbeSupport() {
		var offset = w.atEnd().pos;
		// The probed dispatch table has all entries point here.
		for (i < 256) {
			writeDispatchEntry(probedDispatchTableRef, i, offset);
		}
		genGlobalProbeCall();
		asm.sub_r_i(r_ip, 1);
		genDispatch0(r_ip.indirect(), dispatchTables[0].1, true);

		// LOOP, BLOCK: don't tier up or skip blocks; call global probe and skip block type
		offset = w.atEnd().pos;
		for (op in [Opcode.LOOP, Opcode.BLOCK]) {
			writeDispatchEntry(probedDispatchTableRef, op.code, offset);
		}
		genGlobalProbeCall();
		genSkipBlockType();
		endHandler();
	}
	def genGlobalProbeCall() {
		computeCurIpForTrap(-1);
		computePcFromCurIp();
		saveCallerIVars();
		asm.movq_r_m(r_tmp0, m_wasm_func); // XXX: compute func and pc directly in the right regs
		callRuntime(refRuntimeCall(RT.runtime_PROBE_loop), [r_tmp0, r_curpc], true);
		restoreCallerIVars();
	}
	def genFloatMinAndMax() {
		var ret_b = X86_64Label.new(), ret_a = X86_64Label.new(), is_nan32 = X86_64Label.new(), is_nan64 = X86_64Label.new();
		bindHandler(Opcode.F32_MIN);
		asm.movss_s_m(r_xmm0, vsph[-2].value);
		asm.movss_s_m(r_xmm1, vsph[-1].value);
		asm.ucomiss_s_s(r_xmm0, r_xmm1);
		asm.jc_rel_far(C.P, is_nan32);
		asm.jc_rel_near(C.C, ret_a);
		asm.jc_rel_near(C.A, ret_b);
		asm.d.cmp_m_i(vsph[-1].value, 0);
		asm.jc_rel_near(C.S, ret_b); // handle min(-0, 0) == -0
		asm.jmp_rel_near(ret_a);

		bindHandler(Opcode.F32_MAX);
		asm.movss_s_m(r_xmm0, vsph[-2].value);
		asm.movss_s_m(r_xmm1, vsph[-1].value);
		asm.ucomiss_s_s(r_xmm0, r_xmm1);
		asm.jc_rel_far(C.P, is_nan32);
		asm.jc_rel_near(C.C, ret_b);
		asm.jc_rel_near(C.A, ret_a);
		asm.d.cmp_m_i(vsph[-1].value, 0);
		asm.jc_rel_near(C.NS, ret_b); // handle max(-0, 0) == 0
		asm.jmp_rel_near(ret_a);

		bindHandler(Opcode.F64_MIN);
		asm.movsd_s_m(r_xmm0, vsph[-2].value);
		asm.movsd_s_m(r_xmm1, vsph[-1].value);
		asm.ucomisd_s_s(r_xmm0, r_xmm1);
		asm.jc_rel_near(C.P, is_nan64);
		asm.jc_rel_near(C.C, ret_a);
		asm.jc_rel_near(C.A, ret_b);
		asm.d.cmp_m_i(vsph[-1].upper, 0);
		asm.jc_rel_near(C.S, ret_b); // handle min(-0, 0) == -0
		// fall through to ret_a
		asm.bind(ret_a);
		decrementVsp();
		endHandler();

		bindHandler(Opcode.F64_MAX);
		asm.movsd_s_m(r_xmm0, vsph[-2].value);
		asm.movsd_s_m(r_xmm1, vsph[-1].value);
		asm.ucomisd_s_s(r_xmm0, r_xmm1);
		asm.jc_rel_near(C.P, is_nan64);
		asm.jc_rel_near(C.C, ret_b);
		asm.jc_rel_near(C.A, ret_a);
		asm.d.cmp_m_i(vsph[-1].upper, 0);
		asm.jc_rel_near(C.S, ret_a); // handle max(-0, 0) == 0
		// fall through to ret_b
		asm.bind(ret_b);
		asm.movsd_m_s(vsph[-2].value, r_xmm1);
		decrementVsp();
		endHandler();

		asm.bind(is_nan32);
		asm.movd_m_i(vsph[-2].value, int.view(Floats.f_nan));
		asm.jmp_rel_near(ret_a);

		asm.bind(is_nan64);
		asm.movd_m_i(vsph[-2].upper, int.view(Floats.d_nan >> 32));
		asm.movd_m_i(vsph[-2].value, 0);
		asm.jmp_rel_near(ret_a);
	}
	def genFloatTruncs() {
		for (opcode in [
			Opcode.I32_TRUNC_F32_S,
			Opcode.I32_TRUNC_F32_U,
			Opcode.I32_TRUNC_F64_S,
			Opcode.I32_TRUNC_F64_U,
			Opcode.I64_TRUNC_F32_S,
			Opcode.I64_TRUNC_F32_U,
			Opcode.I64_TRUNC_F64_S,
			Opcode.I64_TRUNC_F64_U,
			Opcode.I32_TRUNC_SAT_F32_S,
			Opcode.I32_TRUNC_SAT_F32_U,
			Opcode.I32_TRUNC_SAT_F64_S,
			Opcode.I32_TRUNC_SAT_F64_U,
			Opcode.I64_TRUNC_SAT_F32_S,
			Opcode.I64_TRUNC_SAT_F32_U,
			Opcode.I64_TRUNC_SAT_F64_S,
			Opcode.I64_TRUNC_SAT_F64_U]) {
			bindHandler(opcode);
			// XXX: don't load current IP for saturating conversions
			computeCurIpForTrap(-1);
			// load value from stack
			if (opcode.sig.params[0] == ValueType.F32) asm.movss_s_m(r_xmm0, vsph[-1].value);
			else asm.movsd_s_m(r_xmm0, vsph[-1].value);
			// emit conversion
			masm.emit_i_trunc_f(opcode, r_tmp0, r_xmm0, r_xmm1);
			// store and update tag
			if (opcode.sig.results[0] == ValueType.I32) {
				asm.movd_m_r(vsph[-1].value, r_tmp0);
				genTagUpdate(BpTypeCode.I32.code);
			} else {
				asm.movq_m_r(vsph[-1].value, r_tmp0);
				genTagUpdate(BpTypeCode.I64.code);
			}
			endHandler();
		}
	}
	def genFloatConversions() {
		bindHandler(Opcode.F32_CONVERT_I32_S); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movd_r_m(r_tmp0, vsph[-1].value);
			asm.q.shl_r_i(r_tmp0, 32);
			asm.q.sar_r_i(r_tmp0, 32); // sign-extend
			asm.cvtsi2ss_s_r(r_xmm0, r_tmp0);
			asm.movss_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_CONVERT_I32_U); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movd_r_m(r_tmp0, vsph[-1].value);
			asm.cvtsi2ss_s_r(r_xmm0, r_tmp0);
			asm.movss_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_CONVERT_I64_S); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movq_r_m(r_tmp0, vsph[-1].value);
			asm.cvtsi2ss_s_r(r_xmm0, r_tmp0);
			asm.movss_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_CONVERT_I64_U); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movq_r_m(r_tmp0, vsph[-1].value);
			masm.emit_f32_convert_i64_u(r_xmm0, r_tmp0, r_xmm1, r_scratch);
			asm.movss_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_DEMOTE_F64); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.cvtsd2ss_s_m(r_xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I32_S); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movd_r_m(r_tmp0, vsph[-1].value);
			asm.q.shl_r_i(r_tmp0, 32);
			asm.q.sar_r_i(r_tmp0, 32); // sign-extend
			asm.cvtsi2sd_s_r(r_xmm0, r_tmp0);
			asm.movsd_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I32_U); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movd_r_m(r_tmp0, vsph[-1].value);
			asm.cvtsi2sd_s_r(r_xmm0, r_tmp0);
			asm.movsd_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I64_S); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movq_r_m(r_tmp0, vsph[-1].value);
			asm.cvtsi2sd_s_r(r_xmm0, r_tmp0);
			asm.movsd_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I64_U); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movq_r_m(r_tmp0, vsph[-1].value);
			masm.emit_f64_convert_i64_u(r_xmm0, r_tmp0, r_xmm1, r_scratch);
			asm.movsd_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_PROMOTE_F32); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.cvtss2sd_s_m(r_xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
	}
	def genRuntimeCallOps() {
		// generate code for runtime calls with 1 LEB that cannot trap.
		var call_irt = asm.newLabel();
		for (t in [
			(Opcode.GLOBAL_GET, refRuntimeCall(RT.runtime_GLOBAL_GET)),
			(Opcode.GLOBAL_SET, refRuntimeCall(RT.runtime_GLOBAL_SET)),
			(Opcode.MEMORY_GROW, refRuntimeCall(RT.runtime_MEMORY_GROW)),
			(Opcode.TABLE_GROW, refRuntimeCall(RT.runtime_TABLE_GROW)),
			(Opcode.STRUCT_NEW, refRuntimeCall(RT.runtime_STRUCT_NEW)),
			(Opcode.STRUCT_NEW_DEFAULT, refRuntimeCall(RT.runtime_STRUCT_NEW_DEFAULT))
		]) {
			bindHandler(t.0);
			genReadUleb32(r_tmp0);
			saveCallerIVars();
			callRuntime(t.1, [r_instance, r_tmp0], false);
			restoreCallerIVars();
			endHandler();
		}
		asm.bind(call_irt);

		// generate code for runtime calls with 1 LEB that can trap.
		call_irt = asm.newLabel();
		for (t in [
			(Opcode.TABLE_GET, refRuntimeCall(RT.runtime_TABLE_GET)),
			(Opcode.TABLE_SET, refRuntimeCall(RT.runtime_TABLE_SET)),
			(Opcode.MEMORY_FILL, refRuntimeCall(RT.runtime_MEMORY_FILL)),
			(Opcode.TABLE_FILL, refRuntimeCall(RT.runtime_TABLE_FILL)),
			(Opcode.ARRAY_NEW, refRuntimeCall(RT.runtime_ARRAY_NEW)),
			(Opcode.ARRAY_NEW_DEFAULT, refRuntimeCall(RT.runtime_ARRAY_NEW_DEFAULT)),
			(Opcode.ARRAY_GET, refRuntimeCall(RT.runtime_ARRAY_GET)),
			(Opcode.ARRAY_GET_S, refRuntimeCall(RT.runtime_ARRAY_GET_S)),
			(Opcode.ARRAY_GET_U, refRuntimeCall(RT.runtime_ARRAY_GET_U)),
			(Opcode.ARRAY_SET, refRuntimeCall(RT.runtime_ARRAY_SET)),
			(Opcode.ARRAY_FILL, refRuntimeCall(RT.runtime_ARRAY_FILL))
		]) {
			bindHandler(t.0);
			if (t.0.prefix == 0) computeCurIpFromIp(-1);
			computePcFromCurIp();
			genReadUleb32(r_tmp0);
			saveCallerIVars();
			callRuntime(t.1, [r_instance, r_tmp0], true);
			restoreCallerIVars();
			endHandler();
		}
		asm.bind(call_irt);

		// generate code for runtime calls with 2 LEBS that can trap.
		call_irt = asm.newLabel();
		for (t in [
			(Opcode.TABLE_INIT, refRuntimeCall(RT.runtime_TABLE_INIT)),
			(Opcode.MEMORY_INIT, refRuntimeCall(RT.runtime_MEMORY_INIT)),
			(Opcode.MEMORY_COPY, refRuntimeCall(RT.runtime_MEMORY_COPY)),
			(Opcode.TABLE_COPY, refRuntimeCall(RT.runtime_TABLE_COPY)),
			(Opcode.STRUCT_GET, refRuntimeCall(RT.runtime_STRUCT_GET)),
			(Opcode.STRUCT_GET_S, refRuntimeCall(RT.runtime_STRUCT_GET_S)),
			(Opcode.STRUCT_GET_U, refRuntimeCall(RT.runtime_STRUCT_GET_U)),
			(Opcode.STRUCT_SET, refRuntimeCall(RT.runtime_STRUCT_SET)),
			(Opcode.ARRAY_NEW_FIXED, refRuntimeCall(RT.runtime_ARRAY_NEW_FIXED)),
			(Opcode.ARRAY_NEW_DATA, refRuntimeCall(RT.runtime_ARRAY_NEW_DATA)),
			(Opcode.ARRAY_NEW_ELEM, refRuntimeCall(RT.runtime_ARRAY_NEW_ELEM)),
			(Opcode.ARRAY_COPY, refRuntimeCall(RT.runtime_ARRAY_COPY)),
			(Opcode.ARRAY_INIT_DATA, refRuntimeCall(RT.runtime_ARRAY_INIT_DATA)),
			(Opcode.ARRAY_INIT_ELEM, refRuntimeCall(RT.runtime_ARRAY_INIT_ELEM))
		]) {
			bindHandler(t.0);
			computePcFromCurIp();
			genReadUleb32(r_tmp0);
			genReadUleb32(r_tmp1);
			saveCallerIVars();
			callRuntime(t.1, [r_instance, r_tmp0, r_tmp1], true);
			restoreCallerIVars();
			endHandler();
		}
		asm.bind(call_irt);
	}

	// helper method to move values from vsp to registers
	// vsp[-2] -> s0, vsp[-1] -> s1
	def load_v128_s_s(s0: X86_64Xmmr, s1: X86_64Xmmr) {
		asm.movdqu_s_m(s0, vsph[-2].value);
		asm.movdqu_s_m(s1, vsph[-1].value);
	}
	def load_v128_s_r(s: X86_64Xmmr, r: X86_64Gpr) {
		asm.movdqu_s_m(s, vsph[-2].value);
		asm.movd_r_m(r, vsph[-1].value);
	}
	// vsp[-2] -> xmm0, vsp[-1] -> xmm1
	def load_v128_xmm0_xmm1(){
		load_v128_s_s(r_xmm0, r_xmm1);
	}
	// vsp[-2] -> xmm1, vsp[-1] -> xmm0
	def load_v128_xmm1_xmm0(){
		load_v128_s_s(r_xmm1, r_xmm0);
	}
	def load_v128_xmm0_tmp0(){
		load_v128_s_r(r_xmm0, r_tmp0);
	}
	def load_imm8(r: X86_64Gpr) {
		asm.movb_r_m(r, ip_ptr);
		asm.q.add_r_i(r_ip, 1);
	}
	def decode_memarg(src: X86_64Addr, scratch1: X86_64Gpr, scratch2: X86_64Gpr) -> X86_64Addr{
		asm.q.inc_r(r_ip);			// skip flags byte
		genReadUleb32(scratch1);			// decode offset
		asm.movd_r_m(scratch2, src);	// read index
		asm.q.add_r_r(scratch1, scratch2);		// add index + offset
		return r_mem0_base.plusR(scratch1, 1, 0);
	}
	def load_memarg8(dst: X86_64Gpr, src: X86_64Addr, scratch: X86_64Gpr) {
		def addr = decode_memarg(src, dst, scratch);
		asm.q.movb_r_m(dst, addr);
	}
	def load_memarg16(dst: X86_64Gpr, src: X86_64Addr, scratch: X86_64Gpr) {
		def addr = decode_memarg(src, dst, scratch);
		asm.q.movw_r_m(dst, addr);
	}
	def load_memarg32(dst: X86_64Gpr, src: X86_64Addr, scratch: X86_64Gpr) {
		def addr = decode_memarg(src, dst, scratch);
		asm.q.movd_r_m(dst, addr);
	}
	def load_memarg64(dst: X86_64Gpr, src: X86_64Addr, scratch: X86_64Gpr) {
		def addr = decode_memarg(src, dst, scratch);
		asm.q.movq_r_m(dst, addr);
	}
	def load_memarg128(dst: X86_64Xmmr, src: X86_64Addr, scratch1: X86_64Gpr, scratch2: X86_64Gpr) {
		def addr = decode_memarg(src, scratch1, scratch2);
		asm.movdqu_s_m(dst, addr);
	}
	def store_memarg128(src: X86_64Addr, data: X86_64Xmmr, scratch1: X86_64Gpr, scratch2: X86_64Gpr) {
		def addr = decode_memarg(src, scratch1, scratch2);
		asm.movdqu_m_s(addr, data);
	}
	def genExtractLane<T>(opcode: Opcode, tag: byte, size: byte,
		asm_mov_r_m: (X86_64Gpr, X86_64Addr) -> T,
		asm_mov_m_r: (X86_64Addr, X86_64Gpr) -> T,
		signExt: bool) {
		bindHandler(opcode);
		def dst : X86_64Addr = vsph[-1].value;
		def idx : X86_64Gpr = r_tmp0;
		def src : X86_64Gpr = r_tmp1;
		// load imm (one byte)
		load_imm8(idx);
		// load the address of the value to be extracted
		asm.q.lea(src, dst);
		// extract the lane from the value stack
		asm_mov_r_m(src, X86_64Addr.new(src, idx, size, 0));
		// sign-extend if necessary
		if (signExt == true) {
			if (size == 1) asm.movbsx_r_r(src, src);
			else if (size == 2) asm.movwsx_r_r(src, src);
		}
		if (valuerep.tagged) genTagUpdate(tag);
		asm_mov_m_r(dst, src);
		endHandler();
	}
	def genReplaceLane<T>(opcode: Opcode, size: byte,
		asm_mov_r_m: (X86_64Gpr, X86_64Addr) -> T,
		asm_mov_m_r: (X86_64Addr, X86_64Gpr) -> T) {
		bindHandler(opcode);
		def dst : X86_64Addr = vsph[-2].value;
		def src : X86_64Addr = vsph[-1].value;
		def idx : X86_64Gpr = r_tmp0;
		def val : X86_64Gpr = r_tmp1;
		def addr : X86_64Gpr = r_tmp2;
		load_imm8(idx); // load imm
		asm_mov_r_m(val, src); // load value
		asm.q.lea(addr, dst); // load address
		asm_mov_m_r(X86_64Addr.new(addr, idx, size, 0), val); // store (replace) value
		decrementVsp();
		endHandler();
	}
	def genLoadLane<T>(opcode: Opcode, size: byte,
		load_memarg: (X86_64Gpr, X86_64Addr, X86_64Gpr) -> void,
		asm_mov_m_r: (X86_64Addr, X86_64Gpr) -> T) {
		bindHandler(opcode);
		def dst : X86_64Addr = vsph[-1].value;
		def idx : X86_64Gpr = r_tmp0;
		def val : X86_64Gpr = r_tmp1;
		def addr : X86_64Gpr = r_tmp2;
		load_memarg(val, vsph[-2].value, addr); // should load memarg first per the bytecode order
		load_imm8(idx); // then load imm
		asm.q.lea(addr, dst); // load address
		asm_mov_m_r(X86_64Addr.new(addr, idx, size, 0), val); // store (replace) value
		if (valuerep.tagged) genTagUpdate(BpTypeCode.V128.code);
		endHandler();
	}
	def genV128LoadZero<T>(opcode: Opcode,
		load_memarg: (X86_64Gpr, X86_64Addr, X86_64Gpr) -> void,
		asm_insert_s_r_i: (X86_64Xmmr, X86_64Gpr, u8) -> T) {
		bindHandler(opcode);
		load_memarg(r_tmp0, vsph[-1].value, r_tmp1); // load memarg to tmp0
		masm.emit_v128_zero(r_xmm0); // zero out xmm0
		asm_insert_s_r_i(r_xmm0, r_tmp0, 0); // insert value to lowest bits
		asm.movdqu_m_s(vsph[-1].value, r_xmm0);
		if (valuerep.tagged) genTagUpdate(BpTypeCode.V128.code);
		endHandler();
	}
	def genV128LoadExtend<T>(opcode: Opcode,
		asm_extend_s_m: (X86_64Xmmr, X86_64Addr) -> T) {
		bindHandler(opcode);
		def dst = r_xmm0;
		def tmp1 = r_tmp0;
		def tmp2 = r_tmp1;
		def src = decode_memarg(vsph[-1].value, tmp1, tmp2);
		asm_extend_s_m(dst, src);
		if (valuerep.tagged) genTagUpdate(BpTypeCode.V128.code);
		asm.movdqu_m_s(vsph[-1].value, dst);
		endHandler();
	}
	def genStoreLane<T>(opcode: Opcode, size: byte,
		asm_mov_r_m: (X86_64Gpr, X86_64Addr) -> T,
		asm_mov_m_r: (X86_64Addr, X86_64Gpr) -> T) {
		bindHandler(opcode);
		def data : X86_64Addr = vsph[-1].value;
		def idx : X86_64Gpr = r_tmp0;
		def val : X86_64Gpr = r_tmp1;
		def mem_addr = decode_memarg(vsph[-2].value, idx, val);
		load_imm8(idx);
		asm.q.lea(val, data); // load address of the vector, val as a temp variable
		asm_mov_r_m(val, X86_64Addr.new(val, idx, size, 0)); // extract the lane from the value stack
		asm_mov_m_r(mem_addr, val);
		endHandler();
	}
	def genSplat<T>(opcode: Opcode,
		asm_mov_r_m: (X86_64Gpr, X86_64Addr) -> T,
		masm_emit: (X86_64Xmmr, X86_64Gpr) -> void) {
		bindHandler(opcode);
		asm_mov_r_m(r_tmp0, vsph[-1].value);
		masm_emit(r_xmm0, r_tmp0);
		asm.movdqu_m_s(vsph[-1].value, r_xmm0);
		genTagUpdate(BpTypeCode.V128.code);
		endHandler();
	}
	def genLoadSplat(opcode: Opcode,
		load_memarg: (X86_64Gpr, X86_64Addr, X86_64Gpr) -> void,
		masm_emit: (X86_64Xmmr, X86_64Gpr) -> void) {
		bindHandler(opcode);
		load_memarg(r_tmp0, vsph[-1].value, r_tmp1);
		masm_emit(r_xmm0, r_tmp0);
		asm.movdqu_m_s(vsph[-1].value, r_xmm0);
		genTagUpdate(BpTypeCode.V128.code);
		endHandler();
	}
	def genSimdInstrs() {
		// V128 const
		bindHandler(Opcode.V128_CONST); {
			asm.movdqu_s_m(r_xmm0, ip_ptr);
			asm.q.add_r_i(r_ip, 16);
			asm.movdqu_m_s(vsph[0].value, r_xmm0);
			genTagPush(BpTypeCode.V128.code);
			incrementVsp();
			endHandler();
		}
		// V128 splat
		genSplat(Opcode.I8X16_SPLAT, asm.q.movb_r_m, masm.emit_i8x16_splat(_, _, r_xmm1));
		genSplat(Opcode.I16X8_SPLAT, asm.q.movw_r_m, masm.emit_i16x8_splat);
		genSplat(Opcode.I32X4_SPLAT, asm.q.movd_r_m, masm.emit_i32x4_splat);
		genSplat(Opcode.I64X2_SPLAT, asm.q.movq_r_m, masm.emit_i64x2_splat);
		genSplat(Opcode.F32X4_SPLAT, asm.q.movd_r_m, masm.emit_f32x4_splat(_, _, r_xmm1));
		genSplat(Opcode.F64X2_SPLAT, asm.q.movq_r_m, masm.emit_f64x2_splat(_, _, r_xmm1));
		// V128 load
		bindHandler(Opcode.V128_LOAD); {
			computeCurIpForTrap(-1);
			load_memarg128(r_xmm0, vsph[-1].value, r_tmp0, r_tmp1);
			if (valuerep.tagged) genTagUpdate(BpTypeCode.V128.code); // update tag if necessary
			asm.movdqu_m_s(vsph[-1].value, r_xmm0);
			endHandler();
		}
		// V128 load_lane
		genLoadLane(Opcode.V128_LOAD_8_LANE, 1, load_memarg8, asm.q.movb_m_r);
		genLoadLane(Opcode.V128_LOAD_16_LANE, 2, load_memarg16, asm.q.movw_m_r);
		genLoadLane(Opcode.V128_LOAD_32_LANE, 4, load_memarg32, asm.q.movd_m_r);
		genLoadLane(Opcode.V128_LOAD_64_LANE, 8, load_memarg64, asm.q.movq_m_r);
		// V128 load_lane with zero-extension
		genV128LoadZero(Opcode.V128_LOAD_32_ZERO, load_memarg32, asm.pinsrd_s_r_i);
		genV128LoadZero(Opcode.V128_LOAD_64_ZERO, load_memarg64, asm.pinsrq_s_r_i);
		// V128 load_extend
		genV128LoadExtend(Opcode.V128_LOAD_8X8_S, asm.pmovsxbw_s_m);
		genV128LoadExtend(Opcode.V128_LOAD_8X8_U, asm.pmovzxbw_s_m);
		genV128LoadExtend(Opcode.V128_LOAD_16X4_S, asm.pmovsxwd_s_m);
		genV128LoadExtend(Opcode.V128_LOAD_16X4_U, asm.pmovzxwd_s_m);
		genV128LoadExtend(Opcode.V128_LOAD_32X2_S, asm.pmovsxdq_s_m);
		genV128LoadExtend(Opcode.V128_LOAD_32X2_U, asm.pmovzxdq_s_m);
		// V128 load_splat
		genLoadSplat(Opcode.V128_LOAD_8_SPLAT, load_memarg8, masm.emit_i8x16_splat(_, _, r_xmm1));
		genLoadSplat(Opcode.V128_LOAD_16_SPLAT, load_memarg16, masm.emit_i16x8_splat);
		genLoadSplat(Opcode.V128_LOAD_32_SPLAT, load_memarg32, masm.emit_i32x4_splat);
		genLoadSplat(Opcode.V128_LOAD_64_SPLAT, load_memarg64, masm.emit_i64x2_splat);
		// V128 lane replacement
		genReplaceLane(Opcode.I8X16_REPLACELANE, 1, asm.movb_r_m, asm.movb_m_r);
		genReplaceLane(Opcode.I16X8_REPLACELANE, 2, asm.movw_r_m, asm.movw_m_r);
		genReplaceLane(Opcode.I32X4_REPLACELANE, 4, asm.movd_r_m, asm.movd_m_r);
		genReplaceLane(Opcode.I64X2_REPLACELANE, 8, asm.movq_r_m, asm.movq_m_r);
		genReplaceLane(Opcode.F32X4_REPLACELANE, 4, asm.movd_r_m, asm.movd_m_r);
		genReplaceLane(Opcode.F64X2_REPLACELANE, 8, asm.movq_r_m, asm.movq_m_r);
		// V128 store
		bindHandler(Opcode.V128_STORE); {
			computeCurIpForTrap(-1);
			asm.movdqu_s_m(r_xmm0, vsph[-1].value);
			store_memarg128(vsph[-2].value, r_xmm0, r_tmp0, r_tmp1);
			adjustVsp(-2);
			endHandler();
		}
		// V128 store_lane
		genStoreLane(Opcode.V128_STORE_8_LANE, 1, asm.q.movb_r_m, asm.q.movb_m_r);
		genStoreLane(Opcode.V128_STORE_16_LANE, 2, asm.q.movw_r_m, asm.q.movw_m_r);
		genStoreLane(Opcode.V128_STORE_32_LANE, 4, asm.q.movd_r_m, asm.q.movd_m_r);
		genStoreLane(Opcode.V128_STORE_64_LANE, 8, asm.q.movq_r_m, asm.q.movq_m_r);
		// V128 extract_lane
		genExtractLane(Opcode.I8X16_EXTRACTLANE_U, BpTypeCode.I32.code, 1, asm.movb_r_m, asm.movb_m_r, false);
		genExtractLane(Opcode.I16X8_EXTRACTLANE_U, BpTypeCode.I32.code, 2, asm.movw_r_m, asm.movw_m_r, false);
		genExtractLane(Opcode.I8X16_EXTRACTLANE_S, BpTypeCode.I32.code, 1, asm.movb_r_m, asm.movd_m_r, true);
		genExtractLane(Opcode.I16X8_EXTRACTLANE_S, BpTypeCode.I32.code, 2, asm.movw_r_m, asm.movd_m_r, true);
		genExtractLane(Opcode.I32X4_EXTRACTLANE, BpTypeCode.I32.code, 4, asm.movd_r_m, asm.movd_m_r, false);
		genExtractLane(Opcode.I64X2_EXTRACTLANE, BpTypeCode.I64.code, 8, asm.movq_r_m, asm.movq_m_r, false);
		genExtractLane(Opcode.F32X4_EXTRACTLANE, BpTypeCode.F32.code, 4, asm.movd_r_m, asm.movd_m_r, false);
		genExtractLane(Opcode.F64X2_EXTRACTLANE, BpTypeCode.F64.code, 8, asm.movq_r_m, asm.movq_m_r, false);

		// V128 boolean instructions
		genSimdBinop(Opcode.V128_AND, asm.andps_s_s);
		genSimdBinop(Opcode.V128_XOR, asm.xorps_s_s);
		genSimdBinop(Opcode.V128_OR, asm.orps_s_s);
		genSimdUnop_xxtmp_x(Opcode.V128_NOT, masm.emit_v128_not);
		genSimdBinopCommute(Opcode.V128_ANDNOT, asm.andnps_s_s);
		genSimdUnop_x_x(Opcode.I8X16_POPCNT, masm.emit_i8x16_popcnt(_, r_tmp0, r_xmm1, r_xmm2, r_xmm3));
		bindHandler(Opcode.V128_BITSELECT); {
			// v128.bitselect(v1: v128, v2: v128, c: v128) -> v128
			// Use the bits in the control mask c to select the corresponding bit
			// from v1 when 1 and v2 when 0.
			// This operation is equivalent to v128.or(v128.and(v1, c), v128.and(v2, v128.not(c)))
			asm.movdqu_s_m(r_xmm0, vsph[-3].value); // v1
			asm.movdqu_s_m(r_xmm1, vsph[-2].value); // v2
			asm.movdqu_s_m(r_xmm2, vsph[-1].value); // c
			masm.emit_v128_bitselect(r_xmm0, r_xmm1, r_xmm2, r_xmm3);
			asm.movdqu_m_s(vsph[-3].value, r_xmm0);
			adjustVsp(-2);
			endHandler();
		}
		genSimdUnop_x_r(Opcode.V128_ANYTRUE, masm.emit_v128_anytrue);
		genSimdUnop_xxtmp_r(Opcode.I8X16_ALLTRUE, masm.emit_i8x16_alltrue);
		genSimdUnop_xxtmp_r(Opcode.I16X8_ALLTRUE, masm.emit_i16x8_alltrue);
		genSimdUnop_xxtmp_r(Opcode.I32X4_ALLTRUE, masm.emit_i32x4_alltrue);
		genSimdUnop_xxtmp_r(Opcode.I64X2_ALLTRUE, masm.emit_i64x2_alltrue);
		genSimdUnop_x_r(Opcode.I8X16_BITMASK, asm.pmovmskb_r_s);
		genSimdUnop_x_r(Opcode.I32X4_BITMASK, asm.movmskps_r_s);
		genSimdUnop_x_r(Opcode.I64X2_BITMASK, asm.movmskpd_r_s);
		genSimdUnop_x_r(Opcode.I16X8_BITMASK, masm.emit_i16x8_bitmask);

		// V128 shifts
		for (t in [
			(Opcode.I8X16_SHL, masm.emit_i8x16_shl),
			(Opcode.I8X16_SHR_S, masm.emit_i8x16_shr_s),
			(Opcode.I8X16_SHR_U, masm.emit_i8x16_shr_u),
			(Opcode.I64X2_SHR_S, masm.emit_i64x2_shr_s)
		]) {
			bindHandler(t.0);
			load_v128_xmm0_tmp0();
			t.1(r_xmm0, r_tmp0, r_tmp1, r_xmm1, r_xmm2);
			asm.movdqu_m_s(vsph[-2].value, r_xmm0);
			decrementVsp();
			endHandler();
		}
		for (t in [
			(Opcode.I16X8_SHL, asm.psllw_s_s, 4),
			(Opcode.I16X8_SHR_S, asm.psraw_s_s, 4),
			(Opcode.I16X8_SHR_U, asm.psrlw_s_s, 4),
			(Opcode.I32X4_SHL, asm.pslld_s_s, 5),
			(Opcode.I32X4_SHR_S, asm.psrad_s_s, 5),
			(Opcode.I32X4_SHR_U, asm.psrld_s_s, 5),
			(Opcode.I64X2_SHL, asm.psllq_s_s, 6),
			(Opcode.I64X2_SHR_U, asm.psrlq_s_s, 6)
		]) {
			bindHandler(t.0);
			load_v128_xmm0_tmp0();
			masm.emit_v128_shift(r_xmm0, r_tmp0, byte.view(t.2), r_tmp1, r_xmm1, t.1);
			asm.movdqu_m_s(vsph[-2].value, r_xmm0);
			decrementVsp();
			endHandler();
		}

		// V128 lane-wise general binary operations
		genSimdBinop(Opcode.I8X16_ADD, asm.paddb_s_s);
		genSimdBinop(Opcode.I8X16_SUB, asm.psubb_s_s);
		genSimdBinop(Opcode.I8X16_NARROW_I16X8_S, asm.packsswb_s_s);
		genSimdBinop(Opcode.I8X16_NARROW_I16X8_U, asm.packuswb_s_s);
		genSimdBinop(Opcode.I8X16_MIN_S, asm.pminsb_s_s);
		genSimdBinop(Opcode.I8X16_MIN_U, asm.pminub_s_s);
		genSimdBinop(Opcode.I8X16_MAX_S, asm.pmaxsb_s_s);
		genSimdBinop(Opcode.I8X16_MAX_U, asm.pmaxub_s_s);
		genSimdBinop(Opcode.I8X16_AVGR_U, asm.pavgb_s_s);
		genSimdBinop(Opcode.I8X16_EQ, asm.pcmpeqb_s_s);
		genSimdBinop(Opcode.I8X16_NE, masm.emit_i8x16_ne);
		genSimdBinop(Opcode.I8X16_ADD_SAT_S, asm.paddsb_s_s);
		genSimdBinop(Opcode.I8X16_ADD_SAT_U, asm.paddusb_s_s);
		genSimdBinop(Opcode.I8X16_SUB_SAT_S, asm.psubsb_s_s);
		genSimdBinop(Opcode.I8X16_SUB_SAT_U, asm.psubusb_s_s);
		genSimdBinop(Opcode.I8X16_GE_S, masm.emit_i8x16_ge_s);
		genSimdBinop(Opcode.I8X16_GE_U, masm.emit_i8x16_ge_u);
		genSimdBinop(Opcode.I8X16_GT_S, asm.pcmpgtb_s_s);
		genSimdBinop(Opcode.I8X16_GT_U, masm.emit_i8x16_gt_u(_, _, r_xmm2));
		genSimdBinopCommute(Opcode.I8X16_LE_S, masm.emit_i8x16_ge_s);
		genSimdBinopCommute(Opcode.I8X16_LE_U, masm.emit_i8x16_ge_u);
		genSimdBinopCommute(Opcode.I8X16_LT_S, asm.pcmpgtb_s_s);
		genSimdBinopCommute(Opcode.I8X16_LT_U, masm.emit_i8x16_gt_u(_, _, r_xmm2));
		genSimdBinop(Opcode.I16X8_ADD, asm.paddw_s_s);
		genSimdBinop(Opcode.I16X8_SUB, asm.psubw_s_s);
		genSimdBinop(Opcode.I16X8_MUL, asm.pmullw_s_s);
		genSimdBinop(Opcode.I16X8_NARROW_I32X4_S, asm.packssdw_s_s);
		genSimdBinop(Opcode.I16X8_NARROW_I32X4_U, asm.packusdw_s_s);
		genSimdBinop(Opcode.I16X8_MIN_S, asm.pminsw_s_s);
		genSimdBinop(Opcode.I16X8_MIN_U, asm.pminuw_s_s);
		genSimdBinop(Opcode.I16X8_MAX_S, asm.pmaxsw_s_s);
		genSimdBinop(Opcode.I16X8_MAX_U, asm.pmaxuw_s_s);
		genSimdBinop(Opcode.I16X8_AVGR_U, asm.pavgw_s_s);
		genSimdBinop(Opcode.I16X8_EQ, asm.pcmpeqw_s_s);
		genSimdBinop(Opcode.I16X8_NE, masm.emit_i16x8_ne);
		genSimdBinop(Opcode.I16X8_ADD_SAT_S, asm.paddsw_s_s);
		genSimdBinop(Opcode.I16X8_ADD_SAT_U, asm.paddusw_s_s);
		genSimdBinop(Opcode.I16X8_SUB_SAT_S, asm.psubsw_s_s);
		genSimdBinop(Opcode.I16X8_SUB_SAT_U, asm.psubusw_s_s);
		genSimdBinop(Opcode.I16X8_Q15MULRSAT_S, masm.emit_i16x8_q15mulrsat_s(_, _, r_xmm2));
		genSimdBinop(Opcode.I16X8_EXTMUL_LOW_I8X16_S, masm.emit_i16x8_extmul_low(_, _, r_xmm2, true));
		genSimdBinop(Opcode.I16X8_EXTMUL_LOW_I8X16_U, masm.emit_i16x8_extmul_low(_, _, r_xmm2, false));
		genSimdBinop(Opcode.I16X8_EXTMUL_HIGH_I8X16_S, masm.emit_i16x8_extmul_high_s(_, _, r_xmm2));
		genSimdBinop(Opcode.I16X8_EXTMUL_HIGH_I8X16_U, masm.emit_i16x8_extmul_high_u(_, _, r_xmm2));
		genSimdBinop(Opcode.I16X8_GE_S, masm.emit_i16x8_ge_s);
		genSimdBinop(Opcode.I16X8_GE_U, masm.emit_i16x8_ge_u);
		genSimdBinop(Opcode.I16X8_GT_S, asm.pcmpgtw_s_s);
		genSimdBinop(Opcode.I16X8_GT_U, masm.emit_i16x8_gt_u(_, _, r_xmm2));
		genSimdBinopCommute(Opcode.I16X8_LE_S, masm.emit_i16x8_ge_s);
		genSimdBinopCommute(Opcode.I16X8_LE_U, masm.emit_i16x8_ge_u);
		genSimdBinopCommute(Opcode.I16X8_LT_S, asm.pcmpgtw_s_s);
		genSimdBinopCommute(Opcode.I16X8_LT_U, masm.emit_i16x8_gt_u(_, _, r_xmm2));
		genSimdBinop(Opcode.I32X4_ADD, asm.paddd_s_s);
		genSimdBinop(Opcode.I32X4_SUB, asm.psubd_s_s);
		genSimdBinop(Opcode.I32X4_MUL, asm.pmulld_s_s);
		genSimdBinop(Opcode.I32X4_DOT_I16X8_S, asm.pmaddwd_s_s);
		genSimdBinop(Opcode.I32X4_MIN_S, asm.pminsd_s_s);
		genSimdBinop(Opcode.I32X4_MIN_U, asm.pminud_s_s);
		genSimdBinop(Opcode.I32X4_MAX_S, asm.pmaxsd_s_s);
		genSimdBinop(Opcode.I32X4_MAX_U, asm.pmaxud_s_s);
		genSimdBinop(Opcode.I32X4_EQ, asm.pcmpeqd_s_s);
		genSimdBinop(Opcode.I32X4_NE, masm.emit_i32x4_ne);
		genSimdBinop(Opcode.I32X4_EXTMUL_LOW_I16X8_S, masm.emit_i32x4_extmul(_, _, r_xmm2, true, true));
		genSimdBinop(Opcode.I32X4_EXTMUL_LOW_I16X8_U, masm.emit_i32x4_extmul(_, _, r_xmm2, true, false));
		genSimdBinop(Opcode.I32X4_EXTMUL_HIGH_I16X8_S, masm.emit_i32x4_extmul(_, _, r_xmm2, false, true));
		genSimdBinop(Opcode.I32X4_EXTMUL_HIGH_I16X8_U, masm.emit_i32x4_extmul(_, _, r_xmm2, false, false));
		genSimdBinop(Opcode.I32X4_GE_S, masm.emit_i32x4_ge_s);
		genSimdBinop(Opcode.I32X4_GE_U, masm.emit_i32x4_ge_u);
		genSimdBinop(Opcode.I32X4_GT_S, asm.pcmpgtd_s_s);
		genSimdBinop(Opcode.I32X4_GT_U, masm.emit_i32x4_gt_u(_, _, r_xmm2));
		genSimdBinopCommute(Opcode.I32X4_LE_S, masm.emit_i32x4_ge_s);
		genSimdBinopCommute(Opcode.I32X4_LE_U, masm.emit_i32x4_ge_u);
		genSimdBinopCommute(Opcode.I32X4_LT_S, asm.pcmpgtd_s_s);
		genSimdBinopCommute(Opcode.I32X4_LT_U, masm.emit_i32x4_gt_u(_, _, r_xmm2));
		genSimdBinop(Opcode.I64X2_ADD, asm.paddq_s_s);
		genSimdBinop(Opcode.I64X2_SUB, asm.psubq_s_s);
		genSimdBinop(Opcode.I64X2_MUL, masm.emit_i64x2_mul(_, _, r_xmm2, r_xmm3));
		genSimdBinop(Opcode.I64X2_EQ, asm.pcmpeqq_s_s);
		genSimdBinop(Opcode.I64X2_NE, masm.emit_i64x2_ne);
		genSimdBinop(Opcode.I64X2_EXTMUL_LOW_I32X4_S, masm.emit_i64x2_extmul(_, _, r_xmm2, true, true));
		genSimdBinop(Opcode.I64X2_EXTMUL_LOW_I32X4_U, masm.emit_i64x2_extmul(_, _, r_xmm2, true, false));
		genSimdBinop(Opcode.I64X2_EXTMUL_HIGH_I32X4_S, masm.emit_i64x2_extmul(_, _, r_xmm2, false, true));
		genSimdBinop(Opcode.I64X2_EXTMUL_HIGH_I32X4_U, masm.emit_i64x2_extmul(_, _, r_xmm2, false, false));
		genSimdBinop(Opcode.I64X2_GT_S, asm.pcmpgtq_s_s);
		genSimdBinop(Opcode.I64X2_GE_S, masm.emit_i64x2_ge_s(_, _, r_xmm2));
		genSimdBinopCommute(Opcode.I64X2_LE_S, masm.emit_i64x2_ge_s(_, _, r_xmm2));
		genSimdBinopCommute(Opcode.I64X2_LT_S, asm.pcmpgtq_s_s);
		genSimdBinop(Opcode.F32X4_ADD, asm.addps_s_s);
		genSimdBinop(Opcode.F32X4_SUB, asm.subps_s_s);
		genSimdBinop(Opcode.F32X4_MUL, asm.mulps_s_s);
		genSimdBinop(Opcode.F32X4_DIV, asm.divps_s_s);
		genSimdBinop(Opcode.F32X4_EQ, asm.cmpeqps_s_s);
		genSimdBinop(Opcode.F32X4_NE, asm.cmpneqps_s_s);
		genSimdBinop(Opcode.F32X4_MIN, masm.emit_f32x4_min(_, _, r_xmm2));
		genSimdBinop(Opcode.F32X4_MAX, masm.emit_f32x4_max(_, _, r_xmm2));
		genSimdBinop(Opcode.F32X4_LT, asm.cmpltps_s_s);
		genSimdBinop(Opcode.F32X4_LE, asm.cmpleps_s_s);
		genSimdBinopCommute(Opcode.F32X4_PMIN, asm.minps_s_s);
		genSimdBinopCommute(Opcode.F32X4_PMAX, asm.maxps_s_s);
		genSimdBinopCommute(Opcode.F32X4_GT, asm.cmpltps_s_s);
		genSimdBinopCommute(Opcode.F32X4_GE, asm.cmpleps_s_s);
		genSimdBinop(Opcode.F64X2_ADD, asm.addpd_s_s);
		genSimdBinop(Opcode.F64X2_SUB, asm.subpd_s_s);
		genSimdBinop(Opcode.F64X2_MUL, asm.mulpd_s_s);
		genSimdBinop(Opcode.F64X2_DIV, asm.divpd_s_s);
		genSimdBinop(Opcode.F64X2_EQ, asm.cmpeqpd_s_s);
		genSimdBinop(Opcode.F64X2_NE, asm.cmpneqpd_s_s);
		genSimdBinop(Opcode.F64X2_MIN, masm.emit_f64x2_min(_, _, r_xmm2));
		genSimdBinop(Opcode.F64X2_MAX, masm.emit_f64x2_max(_, _, r_xmm2));
		genSimdBinop(Opcode.F64X2_LT, asm.cmpltpd_s_s);
		genSimdBinop(Opcode.F64X2_LE, asm.cmplepd_s_s);
		genSimdBinopCommute(Opcode.F64X2_PMIN, asm.minpd_s_s);
		genSimdBinopCommute(Opcode.F64X2_PMAX, asm.maxpd_s_s);
		genSimdBinopCommute(Opcode.F64X2_GT, asm.cmpltpd_s_s);
		genSimdBinopCommute(Opcode.F64X2_GE, asm.cmplepd_s_s);

		// v128 unary operations
		genSimdUnop_xxtmp_x(Opcode.I8X16_NEG, masm.emit_i8x16_neg);
		genSimdUnop_xx_x(Opcode.I8X16_ABS, asm.pabsb_s_s);
		genSimdUnop_xx_x(Opcode.I16X8_EXTEND_LOW_I8X16_S, asm.pmovsxbw_s_s);
		genSimdUnop_xx_x(Opcode.I16X8_EXTEND_LOW_I8X16_U, asm.pmovzxbw_s_s);
		genSimdUnop_xxtmp_x(Opcode.I16X8_NEG, masm.emit_i16x8_neg);
		genSimdUnop_xx_x(Opcode.I16X8_ABS, asm.pabsw_s_s);
		genSimdUnop_xx_x(Opcode.I32X4_EXTEND_LOW_I16X8_S, asm.pmovsxwd_s_s);
		genSimdUnop_xx_x(Opcode.I32X4_EXTEND_LOW_I16X8_U, asm.pmovzxwd_s_s);
		genSimdUnop_xxtmp_x(Opcode.I32X4_NEG, masm.emit_i32x4_neg);
		genSimdUnop_xx_x(Opcode.I32X4_ABS, asm.pabsd_s_s);
		genSimdUnop_xx_x(Opcode.I64X2_EXTEND_LOW_I32X4_S, asm.pmovsxdq_s_s);
		genSimdUnop_xx_x(Opcode.I64X2_EXTEND_LOW_I32X4_U, asm.pmovzxdq_s_s);
		genSimdUnop_xxtmp_x(Opcode.I64X2_NEG, masm.emit_i64x2_neg);
		genSimdUnop_xxtmp_x(Opcode.I64X2_ABS, masm.emit_i64x2_abs);
		genSimdUnop_xx_x(Opcode.F32X4_SQRT, asm.sqrtps_s_s);
		genSimdUnop_xxtmp_x(Opcode.F32X4_NEG, masm.emit_v128_negps(_, r_tmp0, _));
		genSimdUnop_xxtmp_x(Opcode.F32X4_ABS, masm.emit_v128_absps(_, r_tmp0, _));
		genSimdUnop_xx_x(Opcode.F32X4_CEIL, asm.roundps_s_s(_, _, X86_64Rounding.TO_POS_INF));
		genSimdUnop_xx_x(Opcode.F32X4_FLOOR, asm.roundps_s_s(_, _, X86_64Rounding.TO_NEG_INF));
		genSimdUnop_xx_x(Opcode.F32X4_TRUNC, asm.roundps_s_s(_, _, X86_64Rounding.TO_ZERO));
		genSimdUnop_xx_x(Opcode.F32X4_NEAREST, asm.roundps_s_s(_, _, X86_64Rounding.TO_NEAREST));
		genSimdUnop_xx_x(Opcode.F64X2_SQRT, asm.sqrtpd_s_s);
		genSimdUnop_xxtmp_x(Opcode.F64X2_NEG, masm.emit_v128_negpd(_, r_tmp0, _));
		genSimdUnop_xxtmp_x(Opcode.F64X2_ABS, masm.emit_v128_abspd(_, r_tmp0, _));
		genSimdUnop_xx_x(Opcode.F64X2_CEIL, asm.roundpd_s_s(_, _, X86_64Rounding.TO_POS_INF));
		genSimdUnop_xx_x(Opcode.F64X2_FLOOR, asm.roundpd_s_s(_, _, X86_64Rounding.TO_NEG_INF));
		genSimdUnop_xx_x(Opcode.F64X2_TRUNC, asm.roundpd_s_s(_, _, X86_64Rounding.TO_ZERO));
		genSimdUnop_xx_x(Opcode.F64X2_NEAREST, asm.roundpd_s_s(_, _, X86_64Rounding.TO_NEAREST));

		genSimdUnop_xxtmp_x(Opcode.I16X8_EXTADDPAIRWISE_I8X16_S, masm.emit_i16x8_extadd_pairwise_i8x16_s(_, r_tmp0, _));
		genSimdUnop_xxtmp_x(Opcode.I16X8_EXTADDPAIRWISE_I8X16_U, masm.emit_i16x8_extadd_pairwise_i8x16_u(_, r_tmp0, _));
		genSimdUnop_xxtmp_x(Opcode.I32X4_EXTADDPAIRWISE_I16X8_S, masm.emit_i32x4_extadd_pairwise_i16x8_s(_, r_tmp0, _));
		genSimdUnop_xxtmp_x(Opcode.I32X4_EXTADDPAIRWISE_I16X8_U, masm.emit_i32x4_extadd_pairwise_i16x8_u);

		// v128 type conversions
		genSimdUnop_xx_x(Opcode.F32X4_CONVERT_I32X4_S, asm.cvtdq2ps_s_s);
		genSimdUnop_xxtmp_x(Opcode.F32X4_CONVERT_I32X4_U, masm.emit_f32x4_convert_i32x4_u);
		genSimdUnop_xx_x(Opcode.F64X2_CONVERT_LOW_I32X4_S, asm.cvtdq2pd_s_s);
		genSimdUnop_xx_x(Opcode.F64X2_PROMOTE_LOW_F32X4, asm.cvtps2pd_s_s);
		genSimdUnop_xx_x(Opcode.F32X4_DEMOTE_F64X2_ZERO, asm.cvtpd2ps_s_s);
		genSimdUnop_x_x(Opcode.I32X4_TRUNC_SAT_F32X4_S, masm.emit_i32x4_trunc_sat_f32x4_s(_, r_xmm1));
		genSimdUnop_x_x(Opcode.I32X4_TRUNC_SAT_F32X4_U, masm.emit_i32x4_trunc_sat_f32x4_u(_, r_xmm1, r_xmm2));
		genSimdUnop_x_x(Opcode.I32X4_TRUNC_SAT_F64X2_S_ZERO, masm.emit_i32x4_trunc_sat_f64x2_s_zero(_, r_tmp0, r_xmm1, r_xmm2));
		genSimdUnop_x_x(Opcode.I32X4_TRUNC_SAT_F64X2_U_ZERO, masm.emit_i32x4_trunc_sat_f64x2_u_zero(_, r_tmp0, r_xmm1, r_xmm2));
		genSimdUnop_xxtmp_x(Opcode.F64X2_CONVERT_LOW_I32X4_U, masm.emit_f64x2_convert_low_i32x4_u(_, r_tmp0, _));
		genSimdUnop_x_x(Opcode.I16X8_EXTEND_HIGH_I8X16_S, masm.emit_i16x8_s_convert_i8x16_high);
		genSimdUnop_x_x(Opcode.I32X4_EXTEND_HIGH_I16X8_S, masm.emit_i32x4_s_convert_i16x8_high);
		genSimdUnop_x_x(Opcode.I64X2_EXTEND_HIGH_I32X4_S, masm.emit_i64x2_s_convert_i32x4_high);
		genSimdUnop_xxtmp_x(Opcode.I16X8_EXTEND_HIGH_I8X16_U, masm.emit_i16x8_u_convert_i8x16_high);
		genSimdUnop_xxtmp_x(Opcode.I32X4_EXTEND_HIGH_I16X8_U, masm.emit_i32x4_u_convert_i16x8_high);
		genSimdUnop_xxtmp_x(Opcode.I64X2_EXTEND_HIGH_I32X4_U, masm.emit_i64x2_u_convert_i32x4_high);


		bindHandler(Opcode.I8X16_SHUFFLE); {
			var RHS = X86_64Label.new(), LOOP_PRO = X86_64Label.new(), LOOP_EPI = X86_64Label.new();
			incrementVsp(); // make room for a local variable dst (the result)
			def dst = vsph[-1].value;
			def dst_addr = r_tmp3;
			asm.q.lea(dst_addr, dst);
			def idx = r_tmp0; // idx's bound is [0, 31]
			def val_addr = r_tmp1; // address of lhs/rhs
			def i = r_tmp2; // loop counter
			def SIMD_128_SIZE: byte = 16;
			// vectors from the value stack
			def lhs = vsph[-3].value;
			def rhs = vsph[-2].value;
			// for (i: byte < 16)
			asm.movq_r_i(i, 0);
			// LOOP_PROLOGUE:
			asm.bind(LOOP_PRO);
			load_imm8(idx);
			asm.q.cmp_r_i(idx, SIMD_128_SIZE); // idx >= SIMD_128_SIZE?
			asm.jc_rel_near(C.GE, RHS); // if so, jump to RHS
			// Load from lhs
			asm.q.lea(val_addr, lhs);
			// LOOP_EPILOGUE:
			asm.bind(LOOP_EPI);
			asm.movb_r_m(idx, X86_64Addr.new(val_addr, idx, 1, 0)); // load value to idx, since the index is no longer needed
			asm.movb_m_r(X86_64Addr.new(dst_addr, i, 1, 0), idx); // store value to dst
			asm.inc_r(i);
			asm.q.cmp_r_i(i, SIMD_128_SIZE); // loop when 0 <= i < 16
			asm.jc_rel_near(C.L, LOOP_PRO);
			// Return
			asm.movdqu_s_m(r_xmm0, vsph[-1].value);
			asm.movdqu_m_s(vsph[-3].value, r_xmm0);
			adjustVsp(-2);
			endHandler();
			// RHS:
			asm.bind(RHS);
			// Load from rhs
			asm.q.sub_r_i(idx, SIMD_128_SIZE); // idx -= 16
			asm.q.lea(val_addr, rhs);
			asm.jmp_rel_near(LOOP_EPI);
		}
		bindHandler(Opcode.I8X16_SWIZZLE); {
			load_v128_xmm0_xmm1();
			masm.emit_i8x16_swizzle(r_xmm0, r_xmm1, r_tmp0, r_xmm2);
			asm.movdqu_m_s(vsph[-2].value, r_xmm0);
			decrementVsp();
			endHandler();
		}
	}
	def genSimdBinop<T>(opcode: Opcode, f: (X86_64Xmmr, X86_64Xmmr) -> T) {
		bindHandler(opcode);
		asm.movdqu_s_m(r_xmm0, vsph[-2].value);
		asm.movdqu_s_m(r_xmm1, vsph[-1].value);
		f(r_xmm0, r_xmm1);
		asm.movdqu_m_s(vsph[-2].value, r_xmm0);
		decrementVsp();
		endHandler();
	}
	def genSimdBinopCommute<T>(opcode: Opcode, f: (X86_64Xmmr, X86_64Xmmr) -> T) {
		bindHandler(opcode);
		asm.movdqu_s_m(r_xmm1, vsph[-2].value);
		asm.movdqu_s_m(r_xmm0, vsph[-1].value);
		f(r_xmm0, r_xmm1);
		asm.movdqu_m_s(vsph[-2].value, r_xmm0);
		decrementVsp();
		endHandler();
	}
	def genSimdUnop_xx_x<T>(opcode: Opcode, f: (X86_64Xmmr, X86_64Xmmr) -> T) {
		bindHandler(opcode);
		asm.movdqu_s_m(r_xmm0, vsph[-1].value);
		f(r_xmm0, r_xmm0);
		asm.movdqu_m_s(vsph[-1].value, r_xmm0);
		endHandler();
	}
	def genSimdUnop_x_x<T>(opcode: Opcode, f: (X86_64Xmmr) -> T) {
		bindHandler(opcode);
		asm.movdqu_s_m(r_xmm0, vsph[-1].value);
		f(r_xmm0);
		asm.movdqu_m_s(vsph[-1].value, r_xmm0);
		endHandler();
	}
	def genSimdUnop_xxtmp_x<T>(opcode: Opcode, f: (X86_64Xmmr, X86_64Xmmr) -> T) {
		bindHandler(opcode);
		asm.movdqu_s_m(r_xmm0, vsph[-1].value);
		f(r_xmm0, r_xmm1);
		asm.movdqu_m_s(vsph[-1].value, r_xmm0);
		endHandler();
	}
	def genSimdUnop_x_r<T>(opcode: Opcode, f: (X86_64Gpr, X86_64Xmmr) -> T) {
		bindHandler(opcode);
		asm.movdqu_s_m(r_xmm0, vsph[-1].value);
		f(r_tmp0, r_xmm0);
		asm.movd_m_r(vsph[-1].value, r_tmp0);
		genTagUpdate(BpTypeCode.I32.code);
		endHandler();
	}
	def genSimdUnop_xxtmp_r<T>(opcode: Opcode, f: (X86_64Gpr, X86_64Xmmr, X86_64Xmmr) -> T) {
		bindHandler(opcode);
		asm.movdqu_s_m(r_xmm0, vsph[-1].value);
		f(r_tmp0, r_xmm0, r_xmm1);
		asm.movd_m_r(vsph[-1].value, r_tmp0);
		genTagUpdate(BpTypeCode.I32.code);
		endHandler();
	}
	def bindHandler(opcode: Opcode) {
		if (FastIntTuning.handlerAlignment > 1) w.align(FastIntTuning.handlerAlignment);
		var pos = w.atEnd().pos;
		patchDispatchTable(opcode, pos);
		if (Debug.interpreter) {
			Trace.OUT.put1("\thandle_%s:", opcode.name).pad(' ', 35)
				.put1("break *0x%x", (ic.start + pos) - Pointer.NULL)
				.ln();
		}
	}
	def bindHandlerNoAlign(opcode: Opcode) {
		patchDispatchTable(opcode, w.atEnd().pos);
	}
	def decrementVsp() {
		adjustVsp(-1);
	}
	def incrementVsp() {
		adjustVsp(1);
	}
	def adjustVsp(slots: int) {
		if (slots < 0) asm.q.sub_r_i(r_vsp, (0 - slots) * valuerep.slot_size);
		else asm.q.add_r_i(r_vsp, slots * valuerep.slot_size);
	}
	def saveCallerIVars() {
		saveIVar(r_ip);
		saveIVar(r_stp);
		if (!FeatureDisable.stacktraces) saveIVar(r_curpc);
	}
	def restoreCurPcFromFrame() {
		if (!FeatureDisable.stacktraces) restoreReg(r_curpc);
	}
	def restoreCallerIVars() {
		restoreReg(r_ip);
		restoreReg(r_stp);
		restoreReg(r_eip);
		restoreReg(r_instance);
		restoreReg(r_func_decl);
		restoreReg(r_mem0_base);
		restoreReg(r_vfp);
	}
	def restoreDispatchTableReg() {
		if (!FeatureDisable.globalProbes) {
			// restore dispatch table from Interpreter.dispatchTable
			asm.movq_r_m(r_dispatch, masm.absPointer(offsets.Interpreter_dispatchTable));
		}
	}
	def callRuntime(abs: Pointer, args: Array<X86_64Gpr>, canTrap: bool) {
		saveIVar(r_vsp);
		// save a copy of VSP into valueStack.sp
		asm.movq_r_m(r_scratch, masm.absPointer(offsets.X86_64Runtime_curStack));
		asm.movq_m_r(r_scratch.plus(offsets.X86_64Stack_vsp), r_vsp);
		// Generate parallel moves from args into param gprs; assume each src register used only once
		var dst = Array<X86_64Gpr>.new(GPRs.length);
		for (i < args.length) {
			var sreg = args[i];
			var dreg = Target.V3_PARAM_GPRS[i + 1];
			if (sreg != dreg) dst[sreg.regnum] = dreg;
		}
		var stk = Array<i8>.new(GPRs.length);
		for (i < dst.length) orderMoves(dst, stk, i);
		// emit actual call
		asm.callr(int.!((abs - (ic.start + w.pos + 5)))); // TODO: handle 64-bit {abs} with movq_r_l
		restoreDispatchTableReg();
		// restore VSP from valueStack.sp
		asm.movq_r_m(r_vsp, masm.absPointer(offsets.X86_64Runtime_curStack));
		asm.movq_r_m(r_vsp, r_vsp.plus(offsets.X86_64Stack_vsp));
	}
	def orderMoves(dst: Array<X86_64Gpr>, stk: Array<i8>, i: int) {
		// XXX: use move ordering logic from macro assembler?
		var dreg = dst[i];
		if (dreg == null) return;		// no moves here
		if (stk[i] > 0) return;			// this node already done
		stk[i] = -1;				// mark as on stack
		if (stk[dreg.regnum] < 0) {		// destination on stack => cycle
			asm.movq_r_r(r_scratch, dreg);	// save destination first
			stk[dreg.regnum] = -2;		// mark as cycle
		} else {
			orderMoves(dst, stk, dreg.regnum);	// recurse on destination
		}
		asm.movq_r_r(dreg, if(stk[i] == -2, r_scratch, GPRs[i]));	// emit post-order move
		stk[i] = 1;				// mark as done
	}
	def genTagUpdate(tag: byte) {
		if (valuerep.tagged) asm.movq_m_i(vsph[-1].tag, tag);
	}
	def genTagPush(tag: byte) {
		if (valuerep.tagged) asm.movq_m_i(vsph[0].tag, i7.view(tag));
	}
	def genTagPushR(r: X86_64Gpr) {
		if (valuerep.tagged) asm.movq_m_r(vsph[0].tag, r);
	}
	def genCopySlot(dst: X86_64Addr, src: X86_64Addr) {
		match (valuerep.slot_size) {
			8 => {
				asm.movq_r_m(r_scratch, src);
				asm.movq_m_r(dst, r_scratch);
			}
			16 => {
				asm.movdqu_s_m(r_xmm0, src);
				asm.movdqu_m_s(dst, r_xmm0);
			}
			32 => {
				asm.movdqu_s_m(r_xmm0, src);
				asm.movdqu_m_s(dst, r_xmm0);
				asm.movdqu_s_m(r_xmm0, src.plus(16));
				asm.movdqu_m_s(dst.plus(16), r_xmm0);
			}
			_ => {
				fatal(Strings.format1("unsupported value slot size: %d", valuerep.slot_size));
			}
		}
	}
	def saveIVar(r: X86_64Gpr) {
		for (t in all_ivars) {
			if (t.0 == r) asm.movq_m_r(t.1, r);
		}
	}
	def spillReg(r: X86_64Gpr) {
		for (t in mutable_ivars) {
			if (t.0 == r) asm.movq_m_r(t.1, r);
		}
	}
	def restoreReg(r: X86_64Gpr) {
		for (t in all_ivars) {
			if (t.0 == r) asm.movq_r_m(r, t.1);
		}
	}
	def genLoad<T>(opcode: Opcode, tag: byte, gen: (X86_64Gpr, X86_64Addr) -> T) {
		bindHandler(opcode);
		computeCurIpForTrap(-1);
		var finish = asm.newLabel(), has_index: X86_64Label;
		if (!FeatureDisable.multiMemory) { // dynamically check for memory index
			has_index = asm.newLabel();
			asm.q.inc_r(r_ip);				// skip flags byte
			asm.test_m_i(r_ip.plus(-1), BpConstants.MEMARG_INDEX_FLAG); // XXX: test byte
			asm.jc_rel_near(C.NZ, has_index);
		} else {
			asm.q.inc_r(r_ip);				// skip flags byte
		}
		genReadUleb32(r_tmp0);				// decode offset
		asm.movd_r_m(r_tmp1, vsph[-1].value);		// read index off value stack
		asm.q.add_r_r(r_tmp0, r_tmp1);			// add index + offset
		gen(r_tmp1, r_mem0_base.plusR(r_tmp0, 1, 0));
		asm.bind(finish);
		if (valuerep.tagged && tag != BpTypeCode.I32.code) genTagUpdate(tag); // update tag if necessary
		asm.movq_m_r(vsph[-1].value, r_tmp1);
		endHandler();
		if (has_index != null) {
			asm.bind(has_index);
			genReadUleb32(r_tmp0);			// decode memory index
			var memN = r_tmp3;
			asm.movq_r_m(memN, r_instance.plus(offsets.Instance_memories));
			asm.movq_r_m(memN, memN.plusR(r_tmp0, 8, offsets.Array_contents));
			asm.movq_r_m(memN, memN.plus(offsets.X86_64Memory_start));
			genReadUleb32(r_tmp0);			// decode offset
			asm.movd_r_m(r_tmp1, vsph[-1].value);	// read index off value stack
			asm.q.add_r_r(r_tmp0, r_tmp1);		// add index + offset
			gen(r_tmp1, memN.plusR(r_tmp0, 1, 0));
			asm.jmp_rel_near(finish);
		}
	}
	def genStore<T>(gen: (X86_64Addr, X86_64Gpr) -> T) {
		computeCurIpForTrap(-1);
		var finish = asm.newLabel(), has_index: X86_64Label;
		if (!FeatureDisable.multiMemory) { // dynamically check for memory index
			has_index = asm.newLabel();
			asm.q.inc_r(r_ip);				// skip flags byte
			asm.test_m_i(r_ip.plus(-1), BpConstants.MEMARG_INDEX_FLAG); // XXX: test byte
			asm.jc_rel_near(C.NZ, has_index);
		} else {
			asm.q.inc_r(r_ip);				// skip flags byte
		}
		genReadUleb32(r_tmp0);			// decode offset
		asm.movd_r_m(r_tmp1, vsph[-2].value);	// read index
		asm.q.add_r_r(r_tmp0, r_tmp1);		// add index + offset
		asm.movq_r_m(r_tmp1, vsph[-1].value);	// read value
		gen(r_mem0_base.plusR(r_tmp0, 1, 0), r_tmp1);
		asm.bind(finish);
		adjustVsp(-2);
		endHandler();
		if (has_index != null) {
			asm.bind(has_index);
			genReadUleb32(r_tmp0);			// decode memory index
			var memN = r_tmp3;
			asm.movq_r_m(memN, r_instance.plus(offsets.Instance_memories));
			asm.movq_r_m(memN, memN.plusR(r_tmp0, 8, offsets.Array_contents));
			asm.movq_r_m(memN, memN.plus(offsets.X86_64Memory_start));
			genReadUleb32(r_tmp0);			// decode offset
			asm.movd_r_m(r_tmp1, vsph[-2].value);	// read index off value stack
			asm.q.add_r_r(r_tmp0, r_tmp1);		// add index + offset
			asm.movq_r_m(r_tmp1, vsph[-1].value);	// read value
			gen(memN.plusR(r_tmp0, 1, 0), r_tmp1);
			asm.jmp_rel_near(finish);
		}
	}
	def genPopFrameAndRet() {
		genInvalidateFrameAccessor();
		asm.q.add_r_i(r_sp, k_frame_size);
		asm.ret();
	}
	def genInvalidateFrameAccessor() {
		if (!FeatureDisable.frameAccess) asm.movq_m_i(m_accessor, 0);
	}

	// Generate a read of a 32-bit unsigned LEB.
	def genReadUleb32(dest: X86_64Gpr) {
		var ool_leb: OutOfLineLEB;
		if (!FastIntTuning.inlineAllLEBs) {
			ool_leb = OutOfLineLEB.new(dest);
			oolULeb32Sites.put(ool_leb);
		}
		var asm = this.asm.d;
		asm.movbzx_r_m(dest, ip_ptr);	// load first byte
		asm.q.inc_r(r_ip);			// increment pointer
		asm.test_r_i(dest, LEB_UPPER_BIT);	// test most-significant bit
		if (FastIntTuning.inlineAllLEBs) {
			var leb_done = X86_64Label.new();
			asm.jc_rel_near(C.Z, leb_done);
			genReadLEBext(dest);
			asm.bind(leb_done);
		} else {
			asm.jc_rel_addr(C.NZ, ool_leb);
			ool_leb.retOffset = asm.pos();
		}
	}
	// Generate a read of a 32-bit signed LEB.
	def genReadSleb32_inline(dest: X86_64Gpr) {
		var done = X86_64Label.new(), sext = X86_64Label.new(), loop = X86_64Label.new();
		asm.movd_r_i(R.RCX, 0);
		asm.movd_r_i(dest, 0);

		asm.bind(loop);
		asm.movbzx_r_m(r_scratch, ip_ptr);	// load byte
		asm.q.inc_r(r_ip);			// increment pointer
		asm.d.test_r_i(r_scratch, LEB_UPPER_BIT);	// test most-significant bit
		asm.jc_rel_near(C.Z, sext);		// break if not set
		asm.d.and_r_i(r_scratch, 0x7F);		// mask off upper bit
		asm.d.shl_r_cl(r_scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, r_scratch);		// merge byte into val
		asm.d.add_r_i(R.RCX, 7);		// compute next bit pos
		asm.jmp_rel_near(loop);			// loop

		asm.bind(sext);
		asm.d.shl_r_cl(r_scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, r_scratch);		// merge byte into val
		asm.d.sub_r_i(R.RCX, 25);		// compute 25 - shift
		asm.d.neg_r(R.RCX);
		asm.jc_rel_near(C.S, done);		// if shift > 25, done
		asm.d.shl_r_cl(dest);			// sign extension
		asm.d.sar_r_cl(dest);
		asm.bind(done);
	}
	// Generate a read of a 64-bit signed LEB.
	def genReadSleb64_inline(dest: X86_64Gpr) {
		var done = X86_64Label.new(), sext = X86_64Label.new(), loop = X86_64Label.new();
		asm.movd_r_i(R.RCX, 0);
		asm.movd_r_i(dest, 0);

		asm.bind(loop);
		asm.movbzx_r_m(r_scratch, ip_ptr);	// load byte
		asm.q.inc_r(r_ip);			// increment pointer
		asm.d.test_r_i(r_scratch, LEB_UPPER_BIT);	// test most-significant bit
		asm.jc_rel_near(C.Z, sext);		// break if not set
		asm.d.and_r_i(r_scratch, 0x7F);		// mask off upper bit
		asm.q.shl_r_cl(r_scratch);		// shift byte into correct bit pos
		asm.q.or_r_r(dest, r_scratch);		// merge byte into val
		asm.d.add_r_i(R.RCX, 7);		// compute next bit pos
		asm.jmp_rel_near(loop);			// loop

		asm.bind(sext);
		asm.q.shl_r_cl(r_scratch);		// shift byte into correct bit pos
		asm.q.or_r_r(dest, r_scratch);		// merge byte into val
		asm.d.sub_r_i(R.RCX, 57);		// compute 57 - shift
		asm.d.neg_r(R.RCX);
		asm.jc_rel_near(C.S, done);		// if shift > 57, done
		asm.q.shl_r_cl(dest);			// sign extension
		asm.q.sar_r_cl(dest);
		asm.bind(done);
	}
	def genSkipBlockType() {
		if (FeatureDisable.complexBlockTypes) return genSkipLeb();
		var tmp = r_tmp0;
		var done = X86_64Label.new();
		asm.d.movbzx_r_m(tmp, ip_ptr);		// load first byte
		asm.q.inc_r(r_ip);			// increment pointer
		// check for extended LEB, abstract type, or type with immediate
		var typeTableAddr = int.!((ic.start + typeTagTableOffset) - Pointer.NULL);
		asm.test_m_i(tmp.plus(typeTableAddr), TYPE_HAS_IMM | TYPE_IS_LEB);
		asm.jc_rel_near(C.Z, done);
		// handle extended LEB, types with immediates, and abstract types
		var type_not_leb = X86_64Label.new();
		asm.d.test_r_i(tmp, LEB_UPPER_BIT);		// check for LEB first
		asm.jc_rel_near(C.Z, type_not_leb);
		genSkipLeb0(r_tmp3);
		asm.bind(type_not_leb);
		// check for types that have an immediate
		asm.d.test_m_i(tmp.plus(typeTableAddr), TYPE_HAS_IMM);
		asm.jc_rel_near(C.Z, done);
		genSkipLeb();
		asm.bind(done);
	}
	def genSkipSidetableEntry() {
		asm.add_r_i(r_stp, Sidetable_BrEntry.size);
	}
	// Generate code which skips over an LEB.
	def genSkipLeb() {
		genSkipLeb0(r_scratch);
	}
	def genSkipLeb0(scratch: X86_64Gpr) {
		var more = X86_64Label.new();
		asm.bind(more);
		asm.movbzx_r_m(scratch, ip_ptr);	// load first byte
		asm.q.inc_r(r_ip);			// increment pointer
		asm.test_r_i(scratch, LEB_UPPER_BIT);	// test most-significant bit
		asm.jc_rel_near(C.NZ, more);
	}
	// End the handler for the current bytecode
	def endHandler() {
		genDispatchOrJumpToDispatch();
	}
	// Generate an inline dispatch or a jump to the dispatch loop, depending on config.
	def genDispatchOrJumpToDispatch() {
		var gen = FastIntTuning.threadedDispatch;
		if (firstDispatchOffset == 0) {
			firstDispatchOffset = w.pos;
			gen = true;
		}
		if (gen) genDispatch();
		else genJumpToDispatch();
	}
	// Generate a jump to the first dispatch.
	def genJumpToDispatch() {
		asm.jmp_rel(firstDispatchOffset - w.atEnd().pos);
	}
	// Generate a dispatch from the main dispatch table.
	def genDispatch() {
		genDispatch0(ip_ptr, if (FeatureDisable.globalProbes, dispatchTables[0].1), true);
	}
	// Generate a load of the next bytecode and a dispatch through the dispatch table.
	def genDispatch0(ptr: X86_64Addr, table: IcCodeRef, increment: bool) {
		var opcode = r_tmp0;
		var base = r_tmp1;
		if (ptr != null) asm.movbzx_r_m(opcode, ptr);
		if (increment) asm.inc_r(r_ip);
		match (FastIntTuning.dispatchEntrySize) {
			2 => {
				if (table == null) asm.movq_r_r(base, r_dispatch);
				else asm.lea(base, table); // RIP-relative LEA
				asm.movwsx_r_m(opcode, base.plusR(opcode, 2, 0)); // load 16-bit offset
				asm.add_r_r(base, opcode);
				if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
				asm.ijmp_r(base);
			}
			4 => {
				if (table == null) {
					asm.movd_r_m(base, r_dispatch.plusR(opcode, 4, 0));
				} else {
					var addr = ic.start + table.offset;
					asm.movd_r_m(base, X86_64Addr.new(null, opcode, 4, int.!(addr - Pointer.NULL)));
				}
				if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
				asm.ijmp_r(base);
			}
			8 => {
				if (table == null) {
					if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
					asm.ijmp_m(r_dispatch.plusR(opcode, 8, 0));
				} else {
					var addr = ic.start + table.offset;
					if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
					asm.ijmp_m(X86_64Addr.new(null, opcode, 8, int.!(addr - Pointer.NULL)));
				}
			}
		}
	}
	// Patch the dispatch table for the given opcode to go to the given position.
	def patchDispatchTable(opcode: Opcode, pos: int) {
		for (t in dispatchTables) {
			if (t.0 != opcode.prefix) continue;
			var ref1 = t.1;
			if (opcode.prefix == 0 || opcode.code < 128) writeDispatchEntry(ref1, opcode.code, pos);
			var ref2 = t.2;
			if (ref2 != null) writeDispatchEntry(ref2, opcode.code, pos);
			w.atEnd();
			return;
		}
		fatal("no dispatch table found for prefix");
	}
	// Generate the out-of-line LEB decoding code.
	def genOutOfLineLEBs() { // XXX: use a separate out-of-line assembler on the end of the buffer
		for (i < oolULeb32Sites.length) {
			var o = oolULeb32Sites[i];
			var pos = w.atEnd().pos;
			w.at(o.pos).put_b32(pos - (o.pos + o.delta));
			w.atEnd();
			// XXX: share code between out-of-line LEB cases
			genReadLEBext(o.dest);
			asm.jmp_rel(o.retOffset - w.atEnd().pos);
		}
		oolULeb32Sites = null;
	}
	// Generate code for > 1 byte LEB cases
	def genReadLEBext(dest: X86_64Gpr) {
		var destRcx = dest == R.RCX;
		asm.d.and_r_i(dest, 0x7F);		// mask off upper bit of first byte
		if (destRcx) {
			asm.movd_r_r(r_tmp3, dest);
			dest = r_tmp3;
		} else {
			asm.movd_r_r(r_tmp3, R.RCX);	// save RCX
		}
		asm.movd_r_i(R.RCX, 7);
		var loop = X86_64Label.new(), nomore = X86_64Label.new();
		asm.bind(loop);
		asm.movbzx_r_m(r_scratch, ip_ptr);	// load byte
		asm.q.inc_r(r_ip);			// increment pointer
		asm.d.test_r_i(r_scratch, 0x80);	// test most-significant bit
		asm.jc_rel_near(C.Z, nomore);		// break if not set
		asm.d.and_r_i(r_scratch, 0x7F);		// mask off upper bit
		asm.d.shl_r_cl(r_scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, r_scratch);		// merge byte into val
		asm.d.add_r_i(R.RCX, 7);		// compute next bit pos
		asm.jmp_rel_near(loop);			// loop

		asm.bind(nomore);
		asm.d.shl_r_cl(r_scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, r_scratch);		// merge byte into val
		if (destRcx) asm.movd_r_r(R.RCX, dest);
		else asm.movd_r_r(R.RCX, r_tmp3);	// restore RCX
	}
	// Runtime calls and traps need CURPC register to be valid.
	def computeCurIpForTrap(delta: int) {
		if (!FeatureDisable.stacktraces) computeCurIpFromIp(delta);
	}
	def computeCurIpFromIp(delta: int) {
		asm.q.lea(r_curpc, X86_64Addr.new(r_ip, null, 1, delta - offsets.Array_contents));
	}
	def restoreIpFromCurIp(delta: int) {
		asm.q.lea(r_ip, r_curpc.plus(0 - (delta - offsets.Array_contents)));
	}
	def computePcFromCurIp() {
		if (!FeatureDisable.stacktraces) asm.q.sub_r_m(r_curpc, m_code);
	}
	def computePc(delta: int) {
		asm.q.lea(r_curpc, X86_64Addr.new(r_ip, null, 1, delta - offsets.Array_contents));
		asm.q.sub_r_m(r_curpc, m_code);
	}
	// All traps are generated out-of-line and call into the runtime.
	def genTraps() {
		w.atEnd();

		var call_runtime_TRAP = X86_64Label.new();
		asm.bind(call_runtime_TRAP);
		computePcFromCurIp();
		saveCallerIVars();
		asm.movq_r_m(r_tmp1, m_wasm_func);
		// XXX: load runtime arg registers directly
		callRuntime(refRuntimeCall(RT.runtime_TRAP), [r_tmp1, r_curpc, r_tmp4], true);

		for (reason in TrapReason) {
			if (reason == TrapReason.STACK_OVERFLOW) continue; // must be special
			if (reason == TrapReason.DIV_BY_ZERO) continue; // must be special
			var label = newTrapLabel(reason);
			if (label == null) continue;
			asm.bind(label);
			asm.movd_r_i(r_tmp4, reason.tag);
			asm.jmp_rel_near(call_runtime_TRAP);
		}
		// divide by zero happens when RAX and RDX are clobbered
		var divzero_label = newTrapLabel(TrapReason.DIV_BY_ZERO);
		asm.bind(divzero_label);
		computePcFromCurIp();
		saveIVar(r_stp);
		if (!FeatureDisable.stacktraces) saveIVar(r_curpc);
		// XXX: load runtime arg registers directly
		asm.movq_r_m(r_tmp1, m_wasm_func);
		asm.movd_r_i(r_tmp4, TrapReason.DIV_BY_ZERO.tag);
		callRuntime(refRuntimeCall(RT.runtime_TRAP), [r_tmp1, r_curpc, r_tmp4], true);
		// does not return

		// stack overflow cannot call into runtime, because it might be out of stack (!)
		var stackoverflow_label = newTrapLabel(TrapReason.STACK_OVERFLOW);
		asm.bind(stackoverflow_label);
		var trapObj = Pointer.atObject(Execute.trapObjects[TrapReason.STACK_OVERFLOW.tag]);
		asm.movq_r_l(Target.V3_RET_GPRS[0], trapObj - Pointer.NULL);
		genPopFrameAndRet();
		ic.header.stackOverflowHandlerOffset = stackoverflow_label.pos;
		ic.header.oobMemoryHandlerOffset = newTrapLabel(TrapReason.MEM_OUT_OF_BOUNDS).pos;
		ic.header.divZeroHandlerOffset = newTrapLabel(TrapReason.DIV_BY_ZERO).pos;

		// Generate the code for the CRASH opcode.
//TODO		bindHandler(Opcode.CRASH_EXEC);
//TODO		bindHandler(Opcode.CRASH_COMPILER);
		asm.invalid();
		endHandler();
	}
	def newTrapLabel(reason: TrapReason) -> X86_64Label {
		return X86_64MasmLabel.!(masm.newTrapLabel(reason)).label;
	}
	def refRuntimeCall<P, R>(f: P -> R) -> Pointer {
		var ptr = CiRuntime.unpackClosure<X86_64Interpreter, P, R>(f).0;
		var abs = ptr - Pointer.NULL;
		if (abs > u32.max) fatal("runtime call address not in 4GB");
		return ptr;
	}
	def reportOom(w: DataWriter, nlength: int) -> DataWriter {
		fatal("ran out of buffer space");
		return w;
	}
	private def genOnResumeFinish(skip_tag: bool) {
		var m_curStack = X86_64Addr.new(null, null, 1, int.view(offsets.X86_64Runtime_curStack - Pointer.NULL));
		var done = X86_64Label.new();
		// check for error (in %rax)
		asm.q.cmp_r_i(r_ret_throw, 0);
		asm.jc_rel_near(X86_64Conds.Z, done);
		callRuntime(refRuntimeCall(RT.runtime_THROW_REF), [r_ret_throw], true);

		asm.bind(done);
		restoreCallerIVars();
		restoreDispatchTableReg();
		var r_stack = r_tmp1;
		// load %stack
		asm.movq_r_m(r_stack, m_curStack);
		// mov %vsp, [%stack.vsp]
		asm.movq_r_m(r_vsp, r_stack.plus(offsets.X86_64Stack_vsp));

		// skip the handler table
		var handler_length = r_tmp1;
		genSkipLeb(); // skip cont signature
		if (skip_tag) { genSkipLeb(); } // skip throw tag
		genReadUleb32(handler_length); // read handler length

		// skip handler table
		var cnt = r_tmp3;
		asm.movq_r_r(cnt, handler_length);
		// loop for `cnt` times
		var read_start = X86_64Label.new(), read_done = X86_64Label.new(), read_switch = X86_64Label.new();
		asm.bind(read_start);
		// while (--cnt >= 0)
		asm.d.dec_r(cnt);
		// goto read_done
		asm.jc_rel_near(C.S, read_done);
		// read handler type
		genReadUleb32(r_scratch);
		asm.q.cmp_r_i(r_scratch, 0);
		// if {handler_type} is 0x01, read switch handler
		asm.jc_rel_near(X86_64Conds.NZ, read_switch);
		// read normal handler
		genSkipLeb();
		genSkipLeb();
		asm.jmp_rel_near(read_start);
		// read switch handler
		asm.bind(read_switch);
		genSkipLeb();
		asm.jmp_rel_near(read_start);
		asm.bind(read_done);

		// adjust sidetable
		asm.q.add_r_i(r_stp, Sidetable_ResumeEntry.size);
		// calculate length of handler entries in sidetable
		asm.q.imul_r_i(handler_length, Sidetable_SuspendHandlerEntry.size);
		asm.q.add_r_r(r_stp, handler_length);
	}
}

// Assembler patching support for out-of-line LEBs and other code refs.
def ABS_MARKER = 0x55443322;
def REL_MARKER = 0x44332211;
class OutOfLineLEB(dest: X86_64Gpr) extends X86_64Addr {
	var retOffset: int; // where OOB code should "return"
	var pos: int = -1;
	var delta: int;

	new() super(null, null, 1, REL_MARKER) { }
}
class IcCodeRef(var offset: int) extends X86_64Addr {
	new() super(null, null, 1, REL_MARKER) { }
}
class Patcher(w: DataWriter) extends X86_64AddrPatcher {
	new() super(ABS_MARKER, REL_MARKER) { }
	def recordRel32(pos: int, delta: int, addr: X86_64Addr) {
		match (addr) {
			x: OutOfLineLEB => {
				x.pos = pos;
				x.delta = delta;
			}
			x: IcCodeRef => {
				if (x.offset < 0) System.error("InterpreterGen", "unbound forward code ref");
				w.at(pos).put_b32(x.offset - (pos + delta));
				w.atEnd();
			}
		}
	}
}
