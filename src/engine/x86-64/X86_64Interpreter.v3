// Copyright 2021 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// Implements a Wasm interpreter by running handwritten x86-64 interpreter loop.
def GLOBAL_BUFFER_MARKER = 0x7ACEBEEF778899AA;
def INTERPRETER_CODE_MARKER = 0x7FAACCEE;
def GLOBAL_BUFFER_HEADER_SIZE = 64;
def tuning = X86_64InterpreterTuning.new(); // XXX: reduce duplication with target
def RT: X86_64Runtime;

component X86_64Interpreter {
	var interpreterCode: X86_64InterpreterCode; // Dynamically-generated interpreter code.
	var asmEntry: (/*wf: */ WasmFunction, /*sp: */ Pointer) -> AbruptReturn;
	var dispatchTable: Pointer;

	def serializeInterpreterCodeIntoExecutable(executable: Array<byte>) -> bool {
		if (interpreterCode == null) genInterpreterCode();
		// try to find {global_buffer} in {data}
		var d = DataReader.new(executable);
		// the global buffer contents will be at the same page alignment in the executable
		var page_offset = int.!((Pointer.atContents(global_buffer) - Pointer.NULL) & (PAGE_SIZE - 1));
		var found = -1;
		for (pos = page_offset; pos < d.limit; pos += PAGE_SIZE) {
			var val = d.at(pos).read_u64();
			if (val == GLOBAL_BUFFER_MARKER) { found = pos; break; }
		}
		if (found < 0) return false;
		// Write the executable code and the offsets of {InterpreterCode} into the executable.
		var w = DataWriter.new().reset(executable, found, found);
		w.puta(global_buffer); // write machine code
		interpreterCode.serialize(w.at(found + 8)); // write interpreter offsets
		return true;
	}
	def genInterpreterCode() {
		var start = System.ticksUs();
		interpreterCode = deserializeOrGenerateInterpreterCode();
		RiRuntime.registerUserCode(interpreterCode);
		var diff = System.ticksUs() - start;
		if (Debug.interpreter) Trace.OUT.put1("Created interpreter in %d \xCE\xBCs.\n", diff).outln();
		asmEntry = CiRuntime.forgeClosure<
			X86_64Interpreter,				// closure type
			(/*wf: */ WasmFunction, /*sp: */ Pointer),	// parameter types
			AbruptReturn>(			// return types
				interpreterCode.start + interpreterCode.v3EntryOffset, this);
		dispatchTable = interpreterCode.start +
			if(Execute.probes.elem != null,
				interpreterCode.probedDispatchTableOffset,
				interpreterCode.fastDispatchTableOffset);
	}
	def onProbeEnable() {
		if (interpreterCode != null) dispatchTable = interpreterCode.start + interpreterCode.probedDispatchTableOffset;
	}
	def onProbeDisable() {
		if (interpreterCode != null) dispatchTable = interpreterCode.start + interpreterCode.fastDispatchTableOffset;
	}
	def reset() {
		if (RT.valueStack != null) RT.valueStack.sp = RT.valueStack.mapping.range.start;
	}
	def inCode(p: Pointer) -> bool {
		return p >= interpreterCode.start && p < interpreterCode.end;
	}
	// =======================================================================================
	// INTERPRETER RUNTIME CALLBACKS
	// =======================================================================================
}
def computePCFromFrame(sp: Pointer) -> int {
	if (Debug.interpreter) dumpFrame(sp);
	if (tuning.recordCurIpForTraps) return (sp + IVarConfig.frame.CURPC.disp).load<int>();
	var ip   = (sp + IVarConfig.frame.IP.disp).load<Pointer>();
	var code = Pointer.atContents((sp + IVarConfig.frame.CODE.disp).load<Array<byte>>());
	return int.!(ip - code - 1);
}
def dumpFrame(sp: Pointer) {
	Trace.OUT.put1("WASM_FUNC = %x\n", (sp + IVarConfig.frame.WASM_FUNC.disp).load<u64>());
	Trace.OUT.put1("MEM0_BASE = %x\n", (sp + IVarConfig.frame.MEM0_BASE.disp).load<u64>());
	Trace.OUT.put1("VFP       = %x\n", (sp + IVarConfig.frame.VFP.disp).load<u64>());
	Trace.OUT.put1("VSP       = %x\n", (sp + IVarConfig.frame.VSP.disp).load<u64>());
	Trace.OUT.put1("SIDETABLE = %x\n", (sp + IVarConfig.frame.SIDETABLE.disp).load<u64>());
	Trace.OUT.put1("STP       = %x\n", (sp + IVarConfig.frame.STP.disp).load<u64>());
	Trace.OUT.put1("CODE      = %x\n", (sp + IVarConfig.frame.CODE.disp).load<u64>());
	Trace.OUT.put1("IP        = %x\n", (sp + IVarConfig.frame.IP.disp).load<u64>());
	Trace.OUT.put1("EIP       = %x\n", (sp + IVarConfig.frame.EIP.disp).load<u64>());
	Trace.OUT.put1("FUNC_DECL = %x\n", (sp + IVarConfig.frame.FUNC_DECL.disp).load<u64>());
	Trace.OUT.put1("INSTANCE  = %x\n", (sp + IVarConfig.frame.INSTANCE.disp).load<u64>());
	Trace.OUT.put1("CURPC     = %x\n", (sp + IVarConfig.frame.CURPC.disp).load<u64>());
	Trace.OUT.put1("ACCESSOR  = %x\n", (sp + IVarConfig.frame.ACCESSOR.disp).load<u64>());
	Trace.OUT.outln();
}

// FrameAccessor for probing and debugging.
class X86_64InterpreterFrameAccessor extends X86_64BaseFrameAccessor {
	new(sp: Pointer) super(sp) {
		decl = (sp + IVarConfig.frame.FUNC_DECL.disp).load<FuncDecl>();
	}

	// Returns {true} if this frame has been unwound, either due to returning, a trap, or exception.
	def isUnwound() -> bool {
		if (!tuning.cacheFrameAccessor) return false; // TODO: proper unwound check
		return this != (sp + IVarConfig.frame.ACCESSOR.disp).load<X86_64InterpreterFrameAccessor>();
	}
	// Returns the current program counter.
	def pc() -> int {
		checkNotUnwound();
		return computePCFromFrame(sp);
	}
	// Set the value of a local variable. (dynamically typechecked).
	def setLocal(i: int, v: Value);
	// Set operand at depth {i}, with 0 being the top of the stack, -1 being one lower, etc. (dynamically typechecked).
	def setOperand(i: int, v: Value);
}

// Signal-handling for traps
def ucontext_rip_offset = 168;
def ucontext_rsp_offset = 160;
def SIGFPE  = 8;
def SIGBUS  = 10;
def SIGSEGV = 11;

// Implements the RiUserCode interface in order to add generated machine code to the V3 runtime.
// Also stores several important offsets needed in handling signals.
class X86_64InterpreterCode extends RiUserCode {
	def frameSize = IVarConfig.frameSize;
	var fastDispatchTableOffset: int;	// dispatch table when probes disabled
	var probedDispatchTableOffset: int;	// dispatch table when probes enabled
	var codeStart: int;			// start of all executable code
	var v3EntryOffset: int;			// entry from V3 calling code
	var oobMemoryHandlerOffset: int;	// handler for signals caused by OOB memory access
	var divZeroHandlerOffset: int;		// handler for signals caused by divide by zero
	var stackOverflowHandlerOffset: int;	// handler for signals caused by (value- or call-) stack overflow
	var codeEnd: int;			// end of all executable code
	var buf = StringBuilder.new().grow(128);  // avoid allocations when describing frames

	new(start: Pointer, end: Pointer) super(start, end) { }

	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: (Array<byte>, int, int) -> ()) {
		var msg = "\tin [fast-int] ";
		out(msg, 0, msg.length);
		var instance = (sp + IVarConfig.frame.INSTANCE.disp).load<Instance>();
		var func = (sp + IVarConfig.frame.FUNC_DECL.disp).load<FuncDecl>();
		// TODO: lazy parse of names section may allocate; must avoid this in OOM situation
		func.render(instance.module.names, buf);
		buf.ln().out(out);
		buf.reset();
	}

	// Called from V3 runtime for a frame where {ip} is in interpreter code.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp += frameSize;	 // assume frame is allocated
		ip = sp.load<Pointer>(); // return address on stack
		return (ip + -1, sp + Pointer.SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}

	// Called from V3 runtime when the garbage collector needs to scan an interpreter stack frame.
	def scanFrame(ip: Pointer, sp: Pointer) {
		// Handle code and interior pointers
		var code_loc = (sp + IVarConfig.frame.CODE.disp);
		var code = code_loc.load<Pointer>();
		var ip_delta = (sp + IVarConfig.frame.IP.disp).load<Pointer>() - code;
		var eip_delta = (sp + IVarConfig.frame.EIP.disp).load<Pointer>() - code;
		RiGc.scanRoot(code_loc);
		var new_code = code_loc.load<Pointer>();
		if (new_code != code) {
			(sp + IVarConfig.frame.IP.disp).store<Pointer>(new_code + ip_delta);
			(sp + IVarConfig.frame.EIP.disp).store<Pointer>(new_code + eip_delta);
		}

		// Handle sidetable and interior pointer
		var sidetable_loc = (sp + IVarConfig.frame.SIDETABLE.disp);
		var sidetable = sidetable_loc.load<Pointer>();
		var xip_delta = (sp + IVarConfig.frame.STP.disp).load<Pointer>() - sidetable;
		RiGc.scanRoot(sidetable_loc);
		var new_sidetable = sidetable_loc.load<Pointer>();
		if (new_sidetable != sidetable) {
			(sp + IVarConfig.frame.STP.disp).store<Pointer>(new_sidetable + xip_delta);
		}

		// Handle other roots in the frame
		RiGc.scanRoot(sp + IVarConfig.frame.FUNC_DECL.disp);
		RiGc.scanRoot(sp + IVarConfig.frame.INSTANCE.disp);
	}

	// Called from V3 runtime to handle an OS-level signal that occurred while {ip} was in interpreter code.
	def handleSignal(signum: int, siginfo: Pointer, ucontext: Pointer, ip: Pointer, sp: Pointer) -> bool {
		var pip = ucontext + ucontext_rip_offset;
		var ip = pip.load<Pointer>();
		if (Debug.interpreter) {
			Trace.OUT.put2("  !signal %d in interpreter @ 0x%x", signum, ip - Pointer.NULL).outln();
		}
		match (signum) {
			SIGFPE => {
				// presume divide/modulus by zero
				pip.store<Pointer>(start + divZeroHandlerOffset);
				return true;
			}
			SIGBUS, SIGSEGV => {
				var addr = RiOs.getAccessAddress(siginfo, ucontext);
				if (RedZones.isInRedZone(addr)) {
					pip.store<Pointer>(start + stackOverflowHandlerOffset);
					return true;
				}
				pip.store<Pointer>(start + oobMemoryHandlerOffset);
				return true;
			}
		}
		return true;
	}
	// Initializes the interpreter code object from serialized memory (e.g. global buffer).
	def deserialize(d: DataReader) -> bool {
		if (d.read_u32() != INTERPRETER_CODE_MARKER) return false;
		fastDispatchTableOffset = int.view(d.read_u32());
		probedDispatchTableOffset = int.view(d.read_u32());
		codeStart = int.view(d.read_u32());
		v3EntryOffset = int.view(d.read_u32());
		oobMemoryHandlerOffset = int.view(d.read_u32());
		divZeroHandlerOffset = int.view(d.read_u32());
		stackOverflowHandlerOffset = int.view(d.read_u32());
		codeEnd = int.view(d.read_u32());
		return true;
	}
	// Writes the interpreter code object fields to serialized memory (e.g. a file with global buffer).
	def serialize(w: DataWriter) {
		w.put_b32(INTERPRETER_CODE_MARKER);
		w.put_b32(fastDispatchTableOffset);
		w.put_b32(probedDispatchTableOffset);
		w.put_b32(codeStart);
		w.put_b32(v3EntryOffset);
		w.put_b32(oobMemoryHandlerOffset);
		w.put_b32(divZeroHandlerOffset);
		w.put_b32(stackOverflowHandlerOffset);
		w.put_b32(codeEnd);
	}
	// Get a frame accessor for the probe API.
	def getFrameAccessor(sp: Pointer) -> X86_64InterpreterFrameAccessor {
		var retip = (sp + -Pointer.SIZE).load<Pointer>();
		if (!X86_64Interpreter.inCode(retip)) return null;
		if (tuning.cacheFrameAccessor) {
			var prev = (sp + IVarConfig.frame.ACCESSOR.disp).load<X86_64InterpreterFrameAccessor>();
			if (prev != null) return prev;
			var n = X86_64InterpreterFrameAccessor.new(sp);
			(sp + IVarConfig.frame.ACCESSOR.disp).store<X86_64InterpreterFrameAccessor>(n);
			return n;
		}
		return X86_64InterpreterFrameAccessor.new(sp);
	}
}

def fatal(msg: string) {
	System.error("X86_64InterpreterError", msg);
}


//------------------------------------------------------------------------------------------------
//-- Begin Interpreter Generator
//------------------------------------------------------------------------------------------------

// Internal register configuration for variables live in the interpreter execution context.
def R: X86_64Regs, G = X86_64Regs.GPRs, C: X86_64Conds;

// Space needed for the machine code of the interpreter
def PAGE_SIZE = 4 * 1024;
def INL_SIZE = 28 * 1024;
def OOL_SIZE = 4 * 1024;
def TOTAL_SIZE = INL_SIZE + OOL_SIZE;
// Statically-allocated buffer (in compiled binary).
def global_buffer = allocGlobalBufferWithMarker(TOTAL_SIZE + PAGE_SIZE);
def allocGlobalBufferWithMarker(size: int) -> Array<byte> {
	var result = Array<byte>.new(size);
	var w = DataWriter.new().reset(result, 0, result.length);
	w.put_b64(GLOBAL_BUFFER_MARKER);
	return result;
}
def deserializeOrGenerateInterpreterCode() -> X86_64InterpreterCode {
	def r = DataReader.new(global_buffer);

	var start = Pointer.atContents(global_buffer);
	var range = MemoryRange.new(start, start + global_buffer.length);

	var ic = X86_64InterpreterCode.new(range.start, range.end);

	// try deserializing the interpreter code object directly from the global buffer
	if (ic.deserialize(r.at(8))) {
		if (Debug.interpreter) {
			Trace.OUT.put2("Deserialized asm interpreter into [0x%x ... 0x%x]",
				(range.start - Pointer.NULL),
				(range.end - Pointer.NULL));
			Trace.OUT.outln();
		}

	} else {
		//         global buffer v   [0 ...                                         TOTAL_SIZE  ]
		//      |xxxxxxxxxxxxxxxx|h|l|marker|global_header|...|dispatch|...|inline_code|ool_code|___
		//      ^----elem0_offset----^
		// page ^                                       1KiB  ^       page ^      page ^
		var mask = 4095L;
		var elem0_offset = (start - Pointer.NULL) & mask;
		var alloc_offset = elem0_offset + 8 + GLOBAL_BUFFER_HEADER_SIZE;
		var aligned_offset = (alloc_offset + mask) & ~mask;
		var skip = int.!(aligned_offset - elem0_offset);

		if (Debug.interpreter) {
			Trace.OUT.put3("Generating asm interpreter into [0x%x ... 0x%x], skipping %d bytes",
				(range.start - Pointer.NULL),
				(range.end - Pointer.NULL),
				skip);
			Trace.OUT.outln();
		}


		var w_inl = DataWriter.new().reset(global_buffer, skip, skip);
		var w_ool = DataWriter.new().reset(global_buffer, skip + INL_SIZE,  skip + INL_SIZE);

		X86_64InterpreterGen.new(ic, w_inl).gen(range);
	}

	// Write-protect the executable code for security and debugging
	Mmap.protect(range.start + ic.codeStart, u64.!(ic.codeEnd - ic.codeStart), Mmap.PROT_READ | Mmap.PROT_EXEC);
	// Trace results to help in debugging
	if (Debug.interpreter) {
		var s = range.start - Pointer.NULL;
		Trace.OUT
			.put1("\tcode start     = 0x%x\n", s + ic.codeStart)
			.put1("\tv3 entry       = 0x%x\n", s + ic.v3EntryOffset)
			.put1("\tfast dispatch  = 0x%x\n", s + ic.fastDispatchTableOffset)
			.put1("\tprobed dispatch= 0x%x\n", s + ic.probedDispatchTableOffset)
			.put1("\toob mem        = 0x%x\n", s + ic.oobMemoryHandlerOffset)
			.put1("\tdivzero        = 0x%x\n", s + ic.divZeroHandlerOffset)
			.put1("\tstack ovflw    = 0x%x\n", s + ic.stackOverflowHandlerOffset)
			.put1("\tcode end       = 0x%x\n", s + ic.codeEnd)
			.outln();
	}
	return ic;
}

type SlotAddrs(tag: X86_64Addr, value: X86_64Addr, upper: X86_64Addr) #unboxed { }
class VspHelper(vsp: X86_64Gpr, valuerep: Tagging, depth: int) {
	private def slots = Array<SlotAddrs>.new(depth + 1);
	new() {
		for (i < slots.length) {
			var offset = -1 * i * valuerep.slot_size;
			slots[i] = SlotAddrs(
				vsp.plus(offset),
				vsp.plus(offset + valuerep.tag_size),
				vsp.plus(offset + valuerep.tag_size + 4));
		}
	}
	def [i: int] -> SlotAddrs {
		return slots[0 - i]; // so caller can supply -1
	}
}
def TYPE_HAS_IMM: byte = 0x80;
def TYPE_IS_LEB: byte = 0x40;
def TYPE_IS_ABSTRACT: byte = 0x20;
def LEB_UPPER_BIT: byte = 0x80;
// Generates {X86_64InterpreterCode} for X86-64.
class X86_64InterpreterGen(ic: X86_64InterpreterCode, w: DataWriter) {
	def masm = X86_64MacroAssembler.new(w, X86_64Regs2.buildIntAlloc());
	def asm = masm.asm;
	def regs = IVarConfig.regs;
	def frame = IVarConfig.frame;

	def offsets = V3Offsets.new();
	def valuerep = Target.tagging;

	var oolULeb32Sites = Vector<OutOfLineLEB>.new();
	var firstDispatchOffset: int;
	var dispatchJmpOffset: int = -1;
	var callEntryOffset: int;
	var handlerEndOffset: int;
	var callReentryRef: IcCodeRef;
	var tailCallReentryRef: IcCodeRef;
	var abruptRetLabel = X86_64Label.new();
	var controlFallThruLabel = X86_64Label.new();
	var controlTransferLabel = X86_64Label.new();
	var controlSkipSidetableAndDispatchLabel = X86_64Label.new();
	var probedDispatchTableRef: IcCodeRef;
	var typeTagTableOffset: int;

	def vsp = regs.VSP;
	def vsph = VspHelper.new(regs.VSP, valuerep, 3);

	def dispatchTables = Array<(byte, IcCodeRef, IcCodeRef)>.new(Opcodes.code_pages.length + 1);

	def ivar_MEM0_BASE	= (regs.MEM0_BASE, frame.MEM0_BASE);
	def ivar_VFP		= (regs.VFP, frame.VFP);
	def ivar_VSP		= (regs.VSP, frame.VSP);
	def ivar_STP		= (regs.STP, frame.STP);
	def ivar_IP		= (regs.IP, frame.IP);
	def ivar_EIP		= (regs.EIP, frame.EIP);
	def ivar_FUNC_DECL	= (regs.FUNC_DECL, frame.FUNC_DECL);
	def ivar_INSTANCE	= (regs.INSTANCE, frame.INSTANCE);
	def ivar_CURPC		= (regs.CURPC, frame.CURPC);

	def mutable_ivars = [
		ivar_VSP,
		ivar_STP,
		ivar_IP
	];
	def all_ivars = [
		ivar_MEM0_BASE,
		ivar_VFP,
		ivar_VSP,
		ivar_STP,
		ivar_IP,
		ivar_EIP,
		ivar_FUNC_DECL,
		ivar_INSTANCE,
		ivar_CURPC
	];

	new() {
		w.refill = reportOom;
		var p = Patcher.new(w);
		asm.patcher = asm.d.patcher = p;
	}

	def gen(range: MemoryRange) {
		if (tuning.dispatchEntrySize == 4 && (range.start - Pointer.NULL) > int.max) {
			fatal(Strings.format1("global buffer start address of 0x%x out of 31-bit range", (range.start - Pointer.NULL)));
		}
		// Record code start
		ic.codeStart = w.atEnd().pos; // XXX: don't make dispatch tables executable

		// Reserve the type tag table.
		genTypeTagTable();
		// Reserve space for the dispatch tables.
		reserveDispatchTables();
		// Begin code generation
		genInterpreterEntry();
		genOpcodeHandlers();
		handlerEndOffset = w.atEnd().pos;
		// Generate out-of-line code
		genOutOfLineLEBs();
		genTraps();
		ic.codeEnd = w.atEnd().pos;

		if (Debug.interpreter) {
			var s = range.start - Pointer.NULL;
			Trace.OUT
				.put3("Finished asm interpreter @ (0x%x ... 0x%x), used %d bytes\n",
					s, (range.end - Pointer.NULL), w.pos)
				.put1("\tcall entry     = 0x%x\n", s + callEntryOffset);
			for (t in dispatchTables) Trace.OUT.put2("\tdispatch %x    = 0x%x\n", byte.view(t.0), s + t.1.offset);
			Trace.OUT
				.put1("\tfirst dispatch = 0x%x\n", s + firstDispatchOffset)
				.put1("\thandlers end   = 0x%x\n", s + handlerEndOffset)
				.put1("break *0x%x\n", s + dispatchJmpOffset)
				.outln();
		}
		if (w.pos > TOTAL_SIZE) fatal(Strings.format2("need %d bytes for interpreter code, only allocated %d", w.pos, TOTAL_SIZE));
	}
	def genTypeTagTable() {
		if (!tuning.useTypeTagTable) return;
		// Reserve space for the type tag table and fill it out
		typeTagTableOffset = w.pos;
		w.skipN(256);
		for (t in BpTypeCode) {
			var offset = typeTagTableOffset + t.code;
			w.at(offset + LEB_UPPER_BIT).putb(TYPE_IS_LEB);
		}
		for (t in [BpTypeCode.REF_NULL, BpTypeCode.REF]) {
			var offset = typeTagTableOffset + t.code;
			w.at(offset).putb(TYPE_HAS_IMM);
			w.at(offset + LEB_UPPER_BIT).putb(TYPE_HAS_IMM | TYPE_IS_LEB);
		}
		for (t in [BpTypeCode.ABS]) {
			var offset = typeTagTableOffset + t.code;
			w.at(offset).putb(TYPE_HAS_IMM | TYPE_IS_ABSTRACT);
			w.at(offset + LEB_UPPER_BIT).putb(TYPE_IS_ABSTRACT | TYPE_HAS_IMM | TYPE_IS_LEB);
		}
		w.atEnd();
	}
	def reserveDispatchTables() {
		{ // table #0
			w.align(tuning.dispatchEntrySize);
			var ref = IcCodeRef.new(-1);
			ref.offset = ic.fastDispatchTableOffset = w.pos;
			w.skipN(256 * tuning.dispatchEntrySize);
			dispatchTables[0] = (0, ref, null);
		}
		if (tuning.dispatchTableReg) {
			w.align(tuning.dispatchEntrySize);
			probedDispatchTableRef = IcCodeRef.new(-1);
			probedDispatchTableRef.offset = ic.probedDispatchTableOffset = w.pos;
			w.skipN(256 * tuning.dispatchEntrySize);
		}
		for (i < Opcodes.code_pages.length) {
			var page = Opcodes.code_pages[i];
			var ref = IcCodeRef.new(-1);
			w.align(tuning.dispatchEntrySize);
			ref.offset = w.pos;
			w.skipN(256 * tuning.dispatchEntrySize);
			var ref2: IcCodeRef;

			if (!page.oneByte) {
				ref2 = IcCodeRef.new(-1);
				ref2.offset = w.pos;
				w.skipN(256 * tuning.dispatchEntrySize);
			}

			dispatchTables[i + 1] = (page.prefix, ref, ref2);
		}
	}
	def genInterpreterEntry() {
		var shared_entry = X86_64Label.new();
		var tmp = regs.scratch;
		{ // Entrypoint for calls coming from V3
			ic.v3EntryOffset = w.pos;

			// Allocate and initialize interpreter stack frame from incoming V3 params.
			asm.q.sub_r_i(R.RSP, ic.frameSize);
			genInvalidateFrameAccessor();

			// Spill VSP (value stack pointer)
			asm.movq_m_r(frame.VSP, regs.v3_VSP);
			// load dispatch table into register
			if (tuning.dispatchTableReg) {
				asm.movq_r_m(regs.DISPATCH_TABLE, absPointer(offsets.Interpreter_dispatchTable));
			}
			// move WasmFunction into tmp
			asm.movq_r_r(tmp, regs.v3_WASM_FUNC);
			restoreReg(regs.VSP);
			asm.jmp_rel_near(shared_entry);
		}

		{ // Re-entry for calls within the interpreter itself
			callReentryRef = IcCodeRef.new(w.pos);
			// Allocate actual stack frame
			asm.q.sub_r_i(R.RSP, ic.frameSize);
			genInvalidateFrameAccessor();
			// Spill the (valid) stack pointer
			saveIVar(regs.VSP);
			// WasmFunction is in r1 for interpreter reentry
			asm.movq_r_r(tmp, regs.tmp1);
		}

		asm.bind(shared_entry);
		// Load wf.instance, wf.decl and spill
		asm.movq_m_r(frame.WASM_FUNC, tmp);
		asm.movq_r_m(regs.INSTANCE, tmp.plus(offsets.WasmFunction_instance));
		saveIVar(regs.INSTANCE);
		asm.movq_r_m(regs.FUNC_DECL, tmp.plus(offsets.WasmFunction_decl));
		saveIVar(regs.FUNC_DECL);

		// Compute VFP = VSP - func.sig.params.length * SLOT_SIZE
		asm.movq_r_m(tmp, regs.FUNC_DECL.plus(offsets.FuncDecl_sig));
		asm.movq_r_m(tmp, tmp.plus(offsets.SigDecl_params));
		asm.movd_r_m(tmp, tmp.plus(offsets.Array_length));
		asm.q.shl_r_i(tmp, valuerep.slot_size_log);
		asm.movq_r_r(regs.VFP, regs.VSP);
		asm.q.sub_r_r(regs.VFP, tmp);
		saveIVar(regs.VFP);

		tailCallReentryRef = IcCodeRef.new(w.pos);
		// Load &func.cur_bytecode[0] into IP
		asm.movq_r_m(tmp, regs.FUNC_DECL.plus(offsets.FuncDecl_cur_bytecode));
		asm.movq_m_r(frame.CODE, tmp); // save CODE
		asm.lea(regs.IP, tmp.plus(offsets.Array_contents));
		saveIVar(regs.IP);
		// Load IP + code.length into EIP
		asm.movd_r_m(regs.EIP, tmp.plus(offsets.Array_length));
		asm.q.add_r_r(regs.EIP, regs.IP);
		saveIVar(regs.EIP);
		// Load &func.sidetable[0] into STP
		asm.movq_r_m(regs.STP, regs.FUNC_DECL.plus(offsets.FuncDecl_sidetable));
		asm.movq_m_r(frame.SIDETABLE, regs.STP); // save SIDETABLE
		asm.q.add_r_i(regs.STP, offsets.Array_contents);
		saveIVar(regs.STP);

		// Load instance.memories[0].start into MEM0_BASE
		var mem0 = regs.MEM0_BASE;
		asm.movq_r_m(mem0, regs.INSTANCE.plus(offsets.Instance_memories));
		var no_mem = X86_64Label.new();
		asm.movd_r_m(regs.tmp0, mem0.plus(offsets.Array_length)); // XXX: always have a memories[0].start to avoid a branch?
		asm.d.cmp_r_i(regs.tmp0, 0);
		asm.jc_rel_near(C.Z, no_mem);
		asm.movq_r_m(mem0, mem0.plus(offsets.Array_contents));
		asm.movq_r_m(mem0, mem0.plus(offsets.X86_64Memory_start));
		asm.bind(no_mem);
		saveIVar(regs.MEM0_BASE);

		callEntryOffset = w.pos;
		// Decode locals and initialize them. (XXX: special-case 0 locals)
		var countGpr = regs.tmp0;
		genReadUleb32(countGpr);
		var start = X86_64Label.new(), done = X86_64Label.new();
		// gen: if (count != 0) do
		asm.d.cmp_r_i(countGpr, 0);
		asm.jc_rel_near(C.Z, done);
		asm.bind(start);
		// gen: var num = read_uleb32()
		var numGpr = regs.tmp1;
		genReadUleb32(numGpr);
		// gen: var type = read_type();
		var type_done = X86_64Label.new();
		var typeGpr = regs.tmp2;
		asm.d.movbzx_r_m(typeGpr, regs.IP_ptr);	// load first byte
		asm.q.inc_r(regs.IP);			// increment pointer
		// check for extended LEB, abstract type, or type with immediate
		var typeTableAddr = int.!((ic.start + typeTagTableOffset) - Pointer.NULL);
		asm.test_m_i(typeGpr.plus(typeTableAddr), TYPE_HAS_IMM | TYPE_IS_LEB);
		var complex_type_decode = X86_64Label.new();
		asm.jc_rel_far(C.NZ, complex_type_decode);
		asm.bind(type_done);

		// gen: if(num != 0) do
		var start2 = X86_64Label.new(), done2 = X86_64Label.new();
		asm.d.cmp_r_i(numGpr, 0);
		asm.jc_rel_near(C.Z, done2);
		asm.bind(start2);
		genTagPushR(typeGpr);			// *(sp) = type
		asm.movq_m_i(vsph[0].value, 0);		// *(sp + 8) = 0
		asm.add_r_i(vsp, valuerep.slot_size);	// sp += 1 slot
		// gen: while (--num != 0)
		asm.d.dec_r(numGpr);
		asm.jc_rel_near(C.NZ, start2);

		// gen: while (--count != 0)
		asm.d.dec_r(countGpr);
		asm.jc_rel_near(C.NZ, start);
		asm.bind(done);

		// execute first instruction
		genDispatchOrJumpToDispatch();

		// handle extended LEB, types with immediates, and abstract types
		var type_not_leb = X86_64Label.new();
		asm.bind(complex_type_decode);
		asm.d.test_r_i(typeGpr, LEB_UPPER_BIT);		// check for LEB first
		asm.jc_rel_near(C.Z, type_not_leb);
		genSkipLeb0(regs.tmp3);
		asm.bind(type_not_leb);
		// check for types that have an immediate
		asm.d.test_m_i(typeGpr.plus(typeTableAddr), TYPE_HAS_IMM);
		asm.jc_rel_near(C.Z, type_done);
		genReadUleb32(regs.tmp3);			// decode offset
		asm.d.test_m_i(typeGpr.plus(typeTableAddr), TYPE_IS_ABSTRACT);
		asm.jc_rel_near(C.Z, type_done);
		// typeGpr = instance.abscodes[index]
		asm.movq_r_m(regs.tmp4, regs.INSTANCE.plus(offsets.Instance_abscodes));
		asm.movbzx_r_m(typeGpr, regs.tmp4.plusR(regs.tmp3, 1, offsets.Array_contents));
		asm.jmp_rel_near(type_done);
	}

	// Generate all the opcode handlers.
	def genOpcodeHandlers() {
		// Generate the default handler and initialize dispatch tables
		var pos = w.atEnd().pos;
		computeCurIpForTrap(-1);
		asm.jmp_rel_far(newTrapLabel(TrapReason.INVALID_OPCODE));
		for (t in dispatchTables) {
			var ref = t.1;
			for (i < 256) writeDispatchEntry(ref, i, pos);
		}

		// Generate the secondary dispatch tables and point main table at them
		var ref0 = dispatchTables[0].1;
		for (t in dispatchTables) {
			if (t.0 == 0) continue; // main dispatch table
			var pos = w.atEnd().pos;
			computeCurIpForTrap(-1);
			genDispatch(regs.IP_ptr, t.1, true);
			writeDispatchEntry(ref0, t.0, pos);
		}

		// Generate extended LEB landing pads for secondary dispatch tables
		for (i = 1; i < dispatchTables.length; i++) {
			var t = dispatchTables[i];
			var pos = w.pos;
			if (t.2 != null) {
				// TODO: some code in this page are in the upper 128; read extended LEB
				genDispatch(null, t.2, false);
			} else {
				// all codes in this page are in the lower 128; just skip extended LEB
				genSkipLeb();
				asm.d.and_r_i(regs.tmp0, 0x7F);
				genDispatch(null, t.1, false);
			}
			for (i = 128; i < 256; i++) {
				writeDispatchEntry(t.1, i, pos);
			}
		}

		genConsts();
		genControlFlow();
		genLocals();
		genCallsAndRet();
		genLoadsAndStores();
		genCompares();
		genI32Arith();
		genI64Arith();
		genExtensions();
		genF32Arith();
		genF64Arith();
		genFloatCmps();
		genGcInstrs();
		genFloatMinAndMax();
		genFloatTruncs();
		genFloatConversions();
		genRuntimeCallOps();
		genMisc();
	}
	def writeDispatchEntry(ref: IcCodeRef, opcode: int, offset: int) {
		match (tuning.dispatchEntrySize) {
			2 => w.at(ref.offset + 2 * opcode).put_b16(offset - ref.offset);
			4 => w.at(ref.offset + 4 * opcode).put_b32(int.!((ic.start + offset) - Pointer.NULL));
			8 => w.at(ref.offset + 8 * opcode).put_b64((ic.start + offset) - Pointer.NULL);
		}
		w.atEnd();
	}
	def genConsts() {
		bindHandler(Opcode.I32_CONST); {
			genReadSleb32_inline(regs.tmp1);
			genTagPush(BpTypeCode.I32.code);
			asm.movq_m_r(vsph[0].value, regs.tmp1);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.I64_CONST); {
			genReadSleb64_inline(regs.tmp1);
			genTagPush(BpTypeCode.I64.code);
			asm.movq_m_r(vsph[0].value, regs.tmp1);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_CONST); {
			asm.movd_r_m(regs.tmp0, regs.IP_ptr);
			asm.add_r_i(regs.IP, 4);
			genTagPush(BpTypeCode.F32.code);
			asm.movq_m_r(vsph[0].value, regs.tmp0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_CONST); {
			asm.movq_r_m(regs.tmp0, regs.IP_ptr);
			asm.add_r_i(regs.IP, 8);
			genTagPush(BpTypeCode.F64.code);
			asm.movq_m_r(vsph[0].value, regs.tmp0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
	}
	def genControlFlow() {
		// NOP: just goes directly back to the dispatch loop
		patchDispatchTable(Opcode.NOP, firstDispatchOffset);

		// UNREACHABLE: abrupt return
		bindHandler(Opcode.UNREACHABLE);
		computeCurIpForTrap(-1);
		asm.jmp_rel_far(newTrapLabel(TrapReason.UNREACHABLE));

		// BLOCK, LOOP, and TRY are nops except skipping the LEB
		bindHandler(Opcode.BLOCK);
		bindHandler(Opcode.LOOP);
		bindHandler(Opcode.TRY);
		genSkipBlockType();
		endHandler();

		var ctl_xfer_nostack = X86_64Label.new();

		// IF: check condition and either fall thru to next bytecode or ctl xfer (without stack copying)
		bindHandler(Opcode.IF);
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.d.cmp_m_i(vsph[0].value, 0);
		asm.jc_rel_far(C.Z, ctl_xfer_nostack); // XXX: can be near if no complex block types
		asm.bind(controlFallThruLabel);
		genSkipBlockType();
		asm.bind(controlSkipSidetableAndDispatchLabel);
		genSkipSidetableEntry();
		endHandler();

		// BR_IF: check condition and either fall thru to next bytecode or ctl xfer (with stack copying)
		bindHandler(Opcode.BR_IF);
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.d.cmp_m_i(vsph[0].value, 0);
		asm.jc_rel_near(C.Z, controlFallThruLabel);
		// fallthru to BR

		// BR: unconditional ctl xfer with stack copying
		bindHandlerNoAlign(Opcode.BR);
		asm.bind(controlTransferLabel);
		var popcount = regs.tmp0;
		var valcount = regs.tmp1;
		// if popcount > 0
		asm.movd_r_m(popcount, regs.STP.plus(offsets.STP_popcount));
		asm.d.cmp_r_i(popcount, 0);
		asm.jc_rel_near(C.Z, ctl_xfer_nostack);
		// load valcount
		asm.movd_r_m(valcount, regs.STP.plus(offsets.STP_valcount));
		// popcount = popcount * SLOT_SIZE
		asm.d.shl_r_i(popcount, valuerep.slot_size_log);
		// vsp -= valcount + popcount (XXX: save an instruction here?)
		asm.q.sub_r_r(vsp, popcount);
		asm.movd_r_r(regs.scratch, valcount);
		asm.d.shl_r_i(regs.scratch, valuerep.slot_size_log);
		asm.q.sub_r_r(vsp, regs.scratch);
		// do { [vsp] = [vsp + popcount]; vsp++; valcount--; } while (valcount != 0)
		var loop = X86_64Label.new();
		asm.bind(loop);
		genCopySlot(vsp.plus(0), vsp.plusR(popcount, 1, 0));
		asm.q.add_r_i(vsp, valuerep.slot_size);
		asm.d.dec_r(valcount);
		asm.jc_rel_near(C.G, loop);

		// ELSE: unconditional ctl xfer without stack copying
		bindHandlerNoAlign(Opcode.ELSE);
		asm.bind(ctl_xfer_nostack);
		asm.movwsx_r_m(regs.tmp0, regs.STP.plus(offsets.STP_pc_delta)); // TODO: 4 bytes
		asm.q.lea(regs.IP, regs.IP.plusR(regs.tmp0, 1, -1)); // adjust ip
		asm.movwsx_r_m(regs.tmp1, regs.STP.plus(offsets.STP_xip_delta)); // TODO: 4 bytes
		asm.q.lea(regs.STP, regs.STP.plusR(regs.tmp1, 4, 0)); // adjust xip XXX: preshift?
		endHandler();

		// BR_TABLE: adjust STP based on input value and then ctl xfer with stack copying
		bindHandler(Opcode.BR_TABLE);
		var max = regs.tmp0, key = regs.tmp1;
		asm.movd_r_m(max, regs.STP.plus(offsets.STP_pc_delta));
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.movd_r_m(key, vsph[0].value);
		asm.d.cmp_r_r(key, max);
		var ok = X86_64Label.new();
		asm.jc_rel_near(C.NC, ok);
		asm.d.inc_r(key);
		asm.movd_r_r(max, key);
		asm.bind(ok);
		asm.q.add_r_r(regs.IP, max);
		asm.shl_r_i(max, offsets.STP_entry_size_log);
		asm.q.add_r_r(regs.STP, max);
		asm.jmp_rel_near(controlTransferLabel);

		// BR_ON_NULL: check condition and either fall thru to next bytecode or ctl xfer (with stack copying)
		bindHandler(Opcode.BR_ON_NULL);
		asm.q.cmp_m_i(vsph[-1].value, 0);
		asm.jc_rel_near(C.NZ, controlFallThruLabel);
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.jmp_rel_near(controlTransferLabel);

		// BR_ON_NON_NULL: check condition and either fall thru to next bytecode or ctl xfer (with stack copying)
		bindHandler(Opcode.BR_ON_NON_NULL);
		asm.q.cmp_m_i(vsph[-1].value, 0);
		asm.jc_rel_near(C.NZ, controlTransferLabel);
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.jmp_rel_near(controlFallThruLabel);

		bindHandler(Opcode.SELECT); {
			var label = X86_64Label.new();
			asm.d.cmp_m_i(vsph[-1].value, 0);
			asm.jc_rel_near(C.NZ, label);
			// false case; copy false value down
			asm.movq_r_m(regs.tmp0, vsph[-2].value);
			asm.movq_m_r(vsph[-3].value, regs.tmp0);
			// true case, nothing to do
			asm.bind(label);
			asm.sub_r_i(vsp, 2 * valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.SELECT_T); {
			genReadUleb32(regs.tmp0); // load # values
			var skip = X86_64Label.new();
			asm.movd_r_r(regs.tmp1, regs.tmp0);
			asm.bind(skip);  // skip value types
			genSkipLeb();
			asm.dec_r(regs.tmp1);
			asm.jc_rel_near(C.NZ, skip);

			asm.d.shl_r_i(regs.tmp0, valuerep.slot_size_log);
			asm.movd_r_m(regs.tmp1, vsph[-1].value);
			asm.sub_r_r(vsp, regs.tmp0);
			asm.sub_r_i(vsp, valuerep.slot_size); // XXX: combine with above using lea
			asm.d.cmp_r_i(regs.tmp1, 0);
			var label = X86_64Label.new();
			asm.jc_rel_near(C.NZ, label);
			// false case; copy false values down
			asm.movq_r_r(regs.tmp1, vsp);
			asm.q.sub_r_r(regs.tmp1, regs.tmp0);
			var copy = X86_64Label.new();
			asm.bind(copy);
			asm.movq_r_m(regs.tmp2, vsp.plusR(regs.tmp0, 1, - Pointer.SIZE));
			asm.movq_m_r(regs.tmp1.plusR(regs.tmp0, 1, - Pointer.SIZE), regs.tmp2);
			asm.d.sub_r_i(regs.tmp0, valuerep.slot_size);
			asm.jc_rel_near(C.NZ, copy);
			// true case, nothing to do
			asm.bind(label);
			endHandler();
		}
	}
	def genLocals() {
		bindHandler(Opcode.DROP);
		asm.sub_r_i(vsp, valuerep.slot_size);
		endHandler();

		bindHandler(Opcode.LOCAL_GET);
		genReadUleb32(regs.tmp0);
		asm.d.shl_r_i(regs.tmp0, valuerep.slot_size_log);
		genCopySlot(vsp.indirect(), regs.VFP.plusR(regs.tmp0, 1, 0));
		asm.add_r_i(vsp, valuerep.slot_size);
		endHandler();

		bindHandler(Opcode.LOCAL_SET);
		genReadUleb32(regs.tmp0);
		asm.d.shl_r_i(regs.tmp0, valuerep.slot_size_log);
		asm.sub_r_i(vsp, valuerep.slot_size);
		asm.movq_r_m(regs.tmp1, vsph[0].value);
		asm.movq_m_r(regs.VFP.plusR(regs.tmp0, 1, valuerep.tag_size), regs.tmp1);
		endHandler();

		bindHandler(Opcode.LOCAL_TEE);
		genReadUleb32(regs.tmp0);
		asm.d.shl_r_i(regs.tmp0, valuerep.slot_size_log);
		asm.movq_r_m(regs.tmp1, vsph[-1].value);
		asm.movq_m_r(regs.VFP.plusR(regs.tmp0, 1, valuerep.tag_size), regs.tmp1);
		endHandler();
	}
	def genCallsAndRet() {
		bindHandler(Opcode.END);
		asm.q.cmp_r_r(regs.IP, regs.EIP);
		// XXX: END: jump over inlined dispatch?
		asm.jc_rel(C.L, firstDispatchOffset - w.pos); // jump to dispatch (loop)
		// end falls through to return bytecode

		var callFunction = X86_64Label.new();
		var targetFunc = regs.tmp1;
		bindHandlerNoAlign(Opcode.RETURN); {
			// Copy return values from stack to overwrite locals
			var cnt = regs.tmp0;
			asm.movq_r_m(cnt, regs.FUNC_DECL.plus(offsets.FuncDecl_sig));
			asm.movq_r_m(cnt, cnt.plus(offsets.SigDecl_results));
			asm.movd_r_m(cnt, cnt.plus(offsets.Array_length));
			genCopyStackValsToVfp(cnt, regs.tmp1);
			// Deallocate interpreter frame and return to calling code.
			asm.movd_r_i(Target.V3_RET_GPRS[0], 0);
			genPopFrameAndRet();

			bindHandler(Opcode.CALL);
			computeCurIpForTrap(-1);
			genReadUleb32(regs.tmp1);

			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_functions));
			asm.movq_r_m(targetFunc, regs.tmp0.plusR(regs.tmp1, offsets.REF_SIZE, offsets.Array_contents));

			// call_indirect jumps here
			asm.bind(callFunction);
			computePcFromCurIp();
			saveCallerIVars();
			var call_host = X86_64Label.new();
			asm.d.cmp_m_i(targetFunc.plus(0), offsets.WasmFunction_typeId);
			asm.jc_rel_near(C.NZ, call_host);

			// WasmFunction: call into interpreter reentry
			asm.callr_addr(callReentryRef);
			genAbruptRetCheck();
			restoreCallerIVars();
			genDispatchOrJumpToDispatch();

			// HostFunction: call into interpreter runtimeCall to enter into V3 code
			asm.bind(call_host);
			callRuntime(refRuntimeCall(RT.runtime_callHost), [targetFunc], true);
			restoreCallerIVars();
			genDispatchOrJumpToDispatch();
		}

		var trap_func_invalid = newTrapLabel(TrapReason.FUNC_INVALID);
		var check_sig_mismatch = X86_64Label.new();
		bindHandler(Opcode.CALL_INDIRECT); {
			computeCurIpForTrap(-1);
			var sig_index = regs.tmp1, table_index = regs.tmp2, func_index = regs.tmp0;
			genReadUleb32(sig_index);
			genReadUleb32(table_index);

			asm.sub_r_i(vsp, valuerep.slot_size);
			asm.movd_r_m(func_index, vsph[0].value);

			var tmp = regs.tmp3;
			// load instance.sig_ids[sig_index] into sig_index
			asm.movq_r_m(tmp, regs.INSTANCE.plus(offsets.Instance_sig_ids));
			asm.movd_r_m(sig_index, tmp.plusR(sig_index, offsets.INT_SIZE, offsets.Array_contents));
			// Bounds-check table.ids[func_index]
			asm.movq_r_m(tmp, regs.INSTANCE.plus(offsets.Instance_tables));
			var table = table_index;
			asm.movq_r_m(table, tmp.plusR(table_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(tmp, table.plus(offsets.Table_ids));
			asm.d.cmp_r_m(func_index, tmp.plus(offsets.Array_length));
			asm.jc_rel_far(C.NC, trap_func_invalid);
			// Check table.ids[func_index] == sig_index
			asm.d.cmp_r_m(sig_index, tmp.plusR(func_index, offsets.INT_SIZE, offsets.Array_contents));
			asm.jc_rel_near(C.NZ, check_sig_mismatch);
			// Load table.funcs[func_index] into {targetFunc} and jump to calling sequence
			asm.movq_r_m(tmp, table.plus(offsets.Table_funcs));
			asm.movq_r_m(targetFunc, tmp.plusR(func_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.jmp_rel_near(callFunction);  // XXX: duplicate call sequence here?
			// Signature check failed. Mismatch or invalid function?
			asm.bind(check_sig_mismatch);
			asm.d.cmp_m_i(tmp.plusR(func_index, offsets.INT_SIZE, offsets.Array_contents), 0);
			asm.jc_rel_far(C.S, trap_func_invalid); // < 0 implies invalid function, not function sig mismatch
			asm.jmp_rel_far(newTrapLabel(TrapReason.FUNC_SIG_MISMATCH));
		}

		bindHandler(Opcode.CALL_REF); {
			computeCurIpForTrap(-1);
			genSkipLeb(); // skip signature index
			asm.sub_r_i(vsp, valuerep.slot_size);
			asm.movq_r_m(targetFunc, vsph[0].value);
			asm.q.cmp_r_i(targetFunc, 0);
			asm.jc_rel_near(X86_64Conds.NZ, callFunction);
			asm.jmp_rel_far(newTrapLabel(TrapReason.NULL_DEREF));
		}

		var tailCallFunction = X86_64Label.new();
		bindHandler(Opcode.RETURN_CALL); {
			genReadUleb32(regs.tmp1);

			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_functions));
			asm.movq_r_m(targetFunc, regs.tmp0.plusR(regs.tmp1, offsets.REF_SIZE, offsets.Array_contents));

			// return_tail_call jumps here
			asm.bind(tailCallFunction);
			// Overwrite current locals with outgoing arguments
			var cnt = regs.tmp0;
			asm.movq_r_m(cnt, targetFunc.plus(offsets.Function_sig));
			asm.movq_r_m(cnt, cnt.plus(offsets.SigDecl_params));
			asm.movd_r_m(cnt, cnt.plus(offsets.Array_length));
			genCopyStackValsToVfp(cnt, regs.tmp2);

			// Check if the target function is a WasmFunction or HostFunction
			var tail_call_host = X86_64Label.new();
			asm.d.cmp_m_i(targetFunc.plus(0), offsets.WasmFunction_typeId);
			asm.jc_rel_near(C.NZ, tail_call_host);

			// WasmFunction: jump into interpreter reentry
			asm.q.lea(regs.VSP, regs.VFP.plusR(cnt, 1, 0)); // set VSP properly
			asm.movq_r_m(regs.INSTANCE, targetFunc.plus(offsets.WasmFunction_instance));
			saveIVar(regs.INSTANCE);
			asm.movq_r_m(regs.FUNC_DECL, targetFunc.plus(offsets.WasmFunction_decl));
			saveIVar(regs.FUNC_DECL);
			asm.jmp_rel_addr(tailCallReentryRef);

			// HostFunction: jump into interpreter runtimeCall to enter into V3 code
			asm.bind(tail_call_host);

			// Custom code to tail-call the runtime
			asm.movq_r_r(regs.tmp3, regs.VSP); // save VSP from being overwritten
			var abs = refRuntimeCall(RT.runtime_callHost);
			var args = [targetFunc];
			// Generate parallel moves from args into param gprs; assume each src register used only once
			var dst = Array<X86_64Gpr>.new(G.length);
			for (i < args.length) {
				var sreg = args[i];
				var dreg = Target.V3_PARAM_GPRS[i + 1];
				if (sreg != dreg) dst[sreg.regnum] = dreg;
			}
			var stk = Array<i8>.new(G.length);
			for (i < dst.length) orderMoves(dst, stk, i);
			// load interpreter into first arg register
			var interp = Target.V3_PARAM_GPRS[0];
			// save a copy of VSP into interpreter.valueStack.sp
			asm.movq_r_m(regs.scratch, absPointer(offsets.Interpreter_valueStack));
			asm.movq_m_r(regs.scratch.plus(offsets.ValueStack_sp), regs.tmp3);
			genInvalidateFrameAccessor();
			asm.q.add_r_i(R.RSP, ic.frameSize); // deallocate interpreter frame
			asm.jmp_rel(int.!(abs - (ic.start + w.pos))); // tail-call into runtime
		}

		bindHandler(Opcode.RETURN_CALL_INDIRECT); {
			computeCurIpForTrap(-1);
			var sig_index = regs.tmp1, table_index = regs.tmp2, func_index = regs.tmp0;
			genReadUleb32(sig_index);
			genReadUleb32(table_index);

			asm.sub_r_i(vsp, valuerep.slot_size);
			asm.movd_r_m(func_index, vsph[0].value);

			var tmp = regs.tmp3;
			// load instance.sig_ids[sig_index] into sig_index
			asm.movq_r_m(tmp, regs.INSTANCE.plus(offsets.Instance_sig_ids));
			asm.movd_r_m(sig_index, tmp.plusR(sig_index, offsets.INT_SIZE, offsets.Array_contents));
			// Bounds-check table.ids[func_index]
			asm.movq_r_m(tmp, regs.INSTANCE.plus(offsets.Instance_tables));
			var table = table_index;
			asm.movq_r_m(table, tmp.plusR(table_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(tmp, table.plus(offsets.Table_ids));
			asm.d.cmp_r_m(func_index, tmp.plus(offsets.Array_length));
			asm.jc_rel_far(C.NC, trap_func_invalid);
			// Check table.ids[func_index] == sig_index
			asm.d.cmp_r_m(sig_index, tmp.plusR(func_index, offsets.INT_SIZE, offsets.Array_contents));
			asm.jc_rel_near(C.NZ, check_sig_mismatch);
			// Load table.funcs[func_index] into r1 and jump to calling sequence
			asm.movq_r_m(tmp, table.plus(offsets.Table_funcs));
			asm.movq_r_m(targetFunc, tmp.plusR(func_index, offsets.REF_SIZE, offsets.Array_contents));
			asm.jmp_rel_near(tailCallFunction); // XXX: duplicate tail call sequence here?
		}

		bindHandler(Opcode.RETURN_CALL_REF); {
			computeCurIpForTrap(-1);
			genSkipLeb(); // skip signature index
			asm.sub_r_i(vsp, valuerep.slot_size);
			asm.movq_r_m(targetFunc, vsph[0].value);
			asm.q.cmp_r_i(targetFunc, 0);
			asm.jc_rel_near(X86_64Conds.NZ, tailCallFunction);
			asm.jmp_rel_far(newTrapLabel(TrapReason.NULL_DEREF));
		}
	}
	def genCopyStackValsToVfp(cnt: X86_64Gpr, i: X86_64Gpr) {
		var done = X86_64Label.new();
		// Copy argument(s) from VSP to VFP.
		asm.cmp_r_i(cnt, 0);
		asm.jc_rel_near(C.Z, done);
		asm.movd_r_i(i, 0);
		asm.d.shl_r_i(cnt, valuerep.slot_size_log);
		asm.q.sub_r_r(regs.VSP, cnt);
		var loop = X86_64Label.new();
		// while (i < cnt)
		asm.bind(loop);
		genCopySlot(regs.VFP.plusR(i, 1, 0), regs.VSP.plusR(i, 1, 0));
		asm.q.add_r_i(i, valuerep.slot_size);
		asm.q.cmp_r_r(i, cnt);
		asm.jc_rel_near(C.L, loop);
		asm.bind(done);
		// set VSP properly
		asm.q.lea(regs.VSP, regs.VFP.plusR(cnt, 1, 0));
	}
	def genLoadsAndStores() {
		genLoad(Opcode.I32_LOAD, BpTypeCode.I32.code, asm.movd_r_m);
		genLoad(Opcode.I64_LOAD, BpTypeCode.I64.code, asm.movq_r_m);
		genLoad(Opcode.F32_LOAD, BpTypeCode.F32.code, asm.movd_r_m);
		genLoad(Opcode.F64_LOAD, BpTypeCode.F64.code, asm.movq_r_m);
		genLoad(Opcode.I32_LOAD8_S, BpTypeCode.I32.code, asm.movbsx_r_m);
		genLoad(Opcode.I32_LOAD8_U, BpTypeCode.I32.code, asm.movbzx_r_m);
		genLoad(Opcode.I32_LOAD16_S, BpTypeCode.I32.code, asm.movwsx_r_m);
		genLoad(Opcode.I32_LOAD16_U, BpTypeCode.I32.code, asm.movwzx_r_m);
		genLoad(Opcode.I64_LOAD8_S, BpTypeCode.I64.code, asm.movbsx_r_m);
		genLoad(Opcode.I64_LOAD8_U, BpTypeCode.I64.code, asm.movbzx_r_m);
		genLoad(Opcode.I64_LOAD16_S, BpTypeCode.I64.code, asm.movwsx_r_m);
		genLoad(Opcode.I64_LOAD16_U, BpTypeCode.I64.code, asm.movwzx_r_m);
		bindHandler(Opcode.I64_LOAD32_S); {
			asm.q.inc_r(regs.IP); 				// skip flags byte
			genReadUleb32(regs.tmp0);				// decode offset
			asm.movd_r_m(regs.tmp1, vsph[-1].value);			// read index
			asm.q.add_r_r(regs.tmp0, regs.tmp1);			// add index + offset
			asm.movd_r_m(regs.tmp1, regs.MEM0_BASE.plusR(regs.tmp0, 1, 0));
			asm.q.shl_r_i(regs.tmp1, 32); 			// special sign-extension necessary
			asm.q.sar_r_i(regs.tmp1, 32);
			genTagUpdate(BpTypeCode.I64.code);
			asm.movq_m_r(vsph[-1].value, regs.tmp1);
			endHandler();
		}
		genLoad(Opcode.I64_LOAD32_U, BpTypeCode.I64.code, asm.movd_r_m);

		bindHandler(Opcode.I32_STORE);
		bindHandler(Opcode.F32_STORE);
		bindHandler(Opcode.I64_STORE32);
		genStore(asm.movd_m_r);

		bindHandler(Opcode.I64_STORE);
		bindHandler(Opcode.F64_STORE);
		genStore(asm.movq_m_r);

		bindHandler(Opcode.I32_STORE8);
		bindHandler(Opcode.I64_STORE8);
		genStore(asm.movb_m_r);

		bindHandler(Opcode.I32_STORE16);
		bindHandler(Opcode.I64_STORE16);
		genStore(asm.movw_m_r);
	}
	def genCompares() {
		// 32-bit integer compares
		for (t in [
			(Opcode.I32_EQ, C.Z),
			(Opcode.I32_NE, C.NZ),
			(Opcode.I32_LT_S, C.L),
			(Opcode.I32_LT_U, C.C),
			(Opcode.I32_GT_S, C.G),
			(Opcode.I32_GT_U, C.A),
			(Opcode.I32_LE_S, C.LE),
			(Opcode.I32_LE_U, C.NA),
			(Opcode.I32_GE_S, C.GE),
			(Opcode.I32_GE_U, C.NC)
		]) {
			bindHandler(t.0);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.d.cmp_m_r(vsph[-2].value, regs.tmp0);
			asm.set_r(t.1, regs.tmp0);
			asm.movbzx_r_r(regs.tmp0, regs.tmp0);
			asm.movd_m_r(vsph[-2].value, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		// 64-bit integer compares
		bindHandler(Opcode.REF_EQ); // share handler with I64_EQ
		for (t in [
			(Opcode.I64_EQ, C.Z),
			(Opcode.I64_NE, C.NZ),
			(Opcode.I64_LT_S, C.L),
			(Opcode.I64_LT_U, C.C),
			(Opcode.I64_GT_S, C.G),
			(Opcode.I64_GT_U, C.A),
			(Opcode.I64_LE_S, C.LE),
			(Opcode.I64_LE_U, C.NA),
			(Opcode.I64_GE_S, C.GE),
			(Opcode.I64_GE_U, C.NC)
		]) {
			bindHandler(t.0);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			asm.q.cmp_m_r(vsph[-2].value, regs.tmp0);
			asm.set_r(t.1, regs.tmp0);
			asm.movbzx_r_r(regs.tmp0, regs.tmp0);
			asm.movq_m_r(vsph[-2].value, regs.tmp0);
			if (valuerep.tagged) asm.movq_m_i(vsph[-2].tag, BpTypeCode.I32.code);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}

	}
	def genI32Arith() {
		bindHandler(Opcode.I32_EQZ); {
			asm.d.test_m_i(vsph[-1].value, -1);
			asm.set_r(C.Z, regs.tmp0);
			asm.movbzx_r_r(regs.tmp0, regs.tmp0);
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I32_CLZ); {
			asm.movd_r_i(regs.tmp1, -1);
			asm.d.bsr_r_m(regs.tmp0, vsph[-1].value);
			asm.d.cmov_r(C.Z, regs.tmp0, regs.tmp1);
			asm.movd_r_i(regs.tmp1, 31);
			asm.d.sub_r_r(regs.tmp1, regs.tmp0);
			asm.movd_m_r(vsph[-1].value, regs.tmp1);
			endHandler();
		}
		bindHandler(Opcode.I32_CTZ); {
			asm.d.bsf_r_m(regs.tmp0, vsph[-1].value);
			asm.movd_r_i(regs.tmp1, 32);
			asm.d.cmov_r(C.Z, regs.tmp0, regs.tmp1);
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I32_POPCNT); {
			asm.d.popcnt_r_m(regs.tmp0, vsph[-1].value);
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I32_MUL); {
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.d.imul_r_m(regs.tmp0, vsph[-2].value);
			asm.movd_m_r(vsph[-2].value, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I32_DIV_S, masm.emit_i32_div_s, R.RAX),
			(Opcode.I32_DIV_U, masm.emit_i32_div_u, R.RAX),
			(Opcode.I32_REM_S, masm.emit_i32_rem_s, R.RDX),
			(Opcode.I32_REM_U, masm.emit_i32_rem_u, R.RDX)
		]) {
			bindHandler(t.0);
			computeCurIpForTrap(-1);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			spillReg(R.RAX);
			spillReg(R.RDX);
			asm.movd_r_m(R.RAX, vsph[-2].value);
			t.1(regs.tmp0);
			asm.movd_m_r(vsph[-2].value, t.2);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I32_ADD, asm.d.add_m_r),
			(Opcode.I32_SUB, asm.d.sub_m_r),
			(Opcode.I32_AND, asm.d.and_m_r),
			(Opcode.I32_OR, asm.d.or_m_r),
			(Opcode.I32_XOR, asm.d.xor_m_r)
		]) {
			bindHandler(t.0);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			t.1(vsph[-2].value, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I32_SHL, asm.d.shl_m_cl),
			(Opcode.I32_SHR_S, asm.d.sar_m_cl),
			(Opcode.I32_SHR_U, asm.d.shr_m_cl),
			(Opcode.I32_ROTL, asm.d.rol_m_cl),
			(Opcode.I32_ROTR, asm.d.ror_m_cl)
		]) {
			bindHandler(t.0);
			asm.movd_r_m(R.RCX, vsph[-1].value);
			t.1(vsph[-2].value);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
	}
	def genI64Arith() {
		bindHandler(Opcode.I64_EQZ); {
			asm.q.test_m_i(vsph[-1].value, -1);
			asm.set_r(C.Z, regs.tmp0);
			asm.movbzx_r_r(regs.tmp0, regs.tmp0);
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			if (valuerep.tagged) asm.movd_m_i(vsph[-1].tag, BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I64_CLZ); {
			asm.movq_r_i(regs.tmp1, -1);
			asm.q.bsr_r_m(regs.tmp0, vsph[-1].value);
			asm.q.cmov_r(C.Z, regs.tmp0, regs.tmp1);
			asm.movd_r_i(regs.tmp1, 63);
			asm.q.sub_r_r(regs.tmp1, regs.tmp0);
			asm.movq_m_r(vsph[-1].value, regs.tmp1);
			endHandler();
		}
		bindHandler(Opcode.I64_CTZ); {
			asm.q.bsf_r_m(regs.tmp0, vsph[-1].value);
			asm.movd_r_i(regs.tmp1, 64);
			asm.q.cmov_r(C.Z, regs.tmp0, regs.tmp1);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_POPCNT); {
			asm.q.popcnt_r_m(regs.tmp0, vsph[-1].value);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_MUL); {
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			asm.q.imul_r_m(regs.tmp0, vsph[-2].value);
			asm.movq_m_r(vsph[-2].value, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I64_DIV_S, masm.emit_i64_div_s, R.RAX),
			(Opcode.I64_DIV_U, masm.emit_i64_div_u, R.RAX),
			(Opcode.I64_REM_S, masm.emit_i64_rem_s, R.RDX),
			(Opcode.I64_REM_U, masm.emit_i64_rem_u, R.RDX)
		]) {
			bindHandler(t.0);
			computeCurIpForTrap(-1);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			spillReg(R.RAX);
			spillReg(R.RDX);
			asm.movq_r_m(R.RAX, vsph[-2].value);
			t.1(regs.tmp0);
			asm.movq_m_r(vsph[-2].value, t.2);
			restoreReg(R.RAX);
			restoreReg(R.RDX);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I64_ADD, asm.q.add_m_r),
			(Opcode.I64_SUB, asm.q.sub_m_r),
			(Opcode.I64_AND, asm.q.and_m_r),
			(Opcode.I64_OR, asm.q.or_m_r),
			(Opcode.I64_XOR, asm.q.xor_m_r)
		]) {
			bindHandler(t.0);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			t.1(vsph[-2].value, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.I64_SHL, asm.q.shl_m_cl),
			(Opcode.I64_SHR_S, asm.q.sar_m_cl),
			(Opcode.I64_SHR_U, asm.q.shr_m_cl),
			(Opcode.I64_ROTL, asm.q.rol_m_cl),
			(Opcode.I64_ROTR, asm.q.ror_m_cl)
		]) {
			bindHandler(t.0);
			asm.movq_r_m(R.RCX, vsph[-1].value);
			t.1(vsph[-2].value);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
	}
	def genExtensions() {
		bindHandler(Opcode.I32_WRAP_I64); {
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I32_REINTERPRET_F32); {
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I64_REINTERPRET_F64); {
			genTagUpdate(BpTypeCode.I64.code);
			endHandler();
		}
		bindHandler(Opcode.F32_REINTERPRET_I32); {
			genTagUpdate(BpTypeCode.F32.code);
			endHandler();
		}
		bindHandler(Opcode.F64_REINTERPRET_I64); {
			genTagUpdate(BpTypeCode.F64.code);
			endHandler();
		}
		bindHandler(Opcode.I32_EXTEND8_S); {
			asm.d.movbsx_r_m(regs.tmp0, vsph[-1].value);
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I32_EXTEND16_S); {
			asm.d.movwsx_r_m(regs.tmp0, vsph[-1].value);
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND8_S); {
			asm.q.movbsx_r_m(regs.tmp0, vsph[-1].value);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND16_S); {
			asm.q.movwsx_r_m(regs.tmp0, vsph[-1].value);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND_I32_S);
		bindHandler(Opcode.I64_EXTEND32_S); {
			genTagUpdate(BpTypeCode.I64.code);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.q.shl_r_i(regs.tmp0, 32);
			asm.q.sar_r_i(regs.tmp0, 32);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.I64_EXTEND_I32_U); {
			genTagUpdate(BpTypeCode.I64.code);
			asm.movd_m_i(regs.VSP.plus(-4), 0); // zero upper portion
			endHandler();
		}
	}
	def genF32Arith() {
		bindHandler(Opcode.F32_ABS); {
			asm.d.and_m_i(vsph[-1].value, 0x7FFFFFFF); // explicit update of upper word
			endHandler();
		}
		bindHandler(Opcode.F32_NEG); {
			asm.d.xor_m_i(vsph[-1].value, 0x80000000); // explicit update of upper word
			endHandler();
		}
		bindHandler(Opcode.F32_ADD); {
			asm.movss_s_m(regs.xmm0, vsph[-2].value);
			asm.addss_s_m(regs.xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_SUB); {
			asm.movss_s_m(regs.xmm0, vsph[-2].value);
			asm.subss_s_m(regs.xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_MUL); {
			asm.movss_s_m(regs.xmm0, vsph[-2].value);
			asm.mulss_s_m(regs.xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_DIV); {
			asm.movss_s_m(regs.xmm0, vsph[-2].value);
			asm.divss_s_m(regs.xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F32_SQRT); {
			asm.sqrtss_s_m(regs.xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_COPYSIGN); {
			asm.movd_r_m(regs.tmp0, vsph[-2].value); // XXX: tradeoff between memory operands and extra regs?
			asm.d.and_r_i(regs.tmp0, 0x7FFFFFFF);
			asm.movd_r_m(regs.tmp1, vsph[-1].value);
			asm.d.and_r_i(regs.tmp1, 0x80000000);
			asm.d.or_r_r(regs.tmp0, regs.tmp1);
			asm.movd_m_r(vsph[-2].value, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.F32_CEIL, X86_64Rounding.TO_POS_INF),
			(Opcode.F32_FLOOR, X86_64Rounding.TO_NEG_INF),
			(Opcode.F32_TRUNC, X86_64Rounding.TO_ZERO),
			(Opcode.F32_NEAREST, X86_64Rounding.TO_NEAREST)
		]) {
			bindHandler(t.0);
			asm.roundss_s_m(regs.xmm0, vsph[-1].value, t.1);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
	}
	def genF64Arith() {
		bindHandler(Opcode.F64_ABS); {
			asm.d.and_m_i(vsph[-1].upper, 0x7FFFFFFF);
			endHandler();
		}
		bindHandler(Opcode.F64_NEG); {
			asm.d.xor_m_i(vsph[-1].upper, 0x80000000);
			endHandler();
		}
		bindHandler(Opcode.F64_ADD); {
			asm.movsd_s_m(regs.xmm0, vsph[-2].value);
			asm.addsd_s_m(regs.xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_SUB); {
			asm.movsd_s_m(regs.xmm0, vsph[-2].value);
			asm.subsd_s_m(regs.xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_MUL); {
				asm.movsd_s_m(regs.xmm0, vsph[-2].value);
				asm.mulsd_s_m(regs.xmm0, vsph[-1].value);
				asm.movsd_m_s(vsph[-2].value, regs.xmm0);
				asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_DIV); {
			asm.movsd_s_m(regs.xmm0, vsph[-2].value);
			asm.divsd_s_m(regs.xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-2].value, regs.xmm0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.F64_SQRT); {
			asm.sqrtsd_s_m(regs.xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_COPYSIGN); {
			asm.movd_r_m(regs.tmp0, vsph[-2].upper); // XXX: tradeoff between memory operands and extra regs?
			asm.d.and_r_i(regs.tmp0, 0x7FFFFFFF);
			asm.movd_r_m(regs.tmp1, vsph[-1].upper);
			asm.d.and_r_i(regs.tmp1, 0x80000000);
			asm.d.or_r_r(regs.tmp0, regs.tmp1);
			asm.movd_m_r(vsph[-2].upper, regs.tmp0);
			asm.q.sub_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		for (t in [
			(Opcode.F64_CEIL, X86_64Rounding.TO_POS_INF),
			(Opcode.F64_FLOOR, X86_64Rounding.TO_NEG_INF),
			(Opcode.F64_TRUNC, X86_64Rounding.TO_ZERO),
			(Opcode.F64_NEAREST, X86_64Rounding.TO_NEAREST)
		]) {
			bindHandler(t.0);
			asm.roundsd_s_m(regs.xmm0, vsph[-1].value, t.1);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
	}
	def genFloatCmps() {
		var ret_zero = X86_64Label.new(), ret_one = X86_64Label.new();
		for (t in [
			(Opcode.F32_EQ, C.NZ),
			(Opcode.F32_NE, C.Z),
			(Opcode.F32_LT, C.NC),
			(Opcode.F32_GT, C.NA),
			(Opcode.F32_LE, C.A),
			(Opcode.F32_GE, C.C)]) {
			bindHandler(t.0);
			asm.movss_s_m(regs.xmm0, vsph[-2].value);
			asm.ucomiss_s_m(regs.xmm0, vsph[-1].value);
			asm.jc_rel_near(C.P, if(t.0 == Opcode.F32_NE, ret_one, ret_zero));
			asm.jc_rel_near(t.1, ret_zero);
			asm.jmp_rel_near(ret_one);
		}

		asm.bind(ret_zero);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsph[-1].value, 0);
		endHandler();

		asm.bind(ret_one);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsph[-1].value, 1);
		endHandler();

		// XXX: too far of a near jump to share these between f32 and f64
		ret_zero = X86_64Label.new();
		ret_one = X86_64Label.new();
		for (t in [
			(Opcode.F64_EQ, C.NZ),
			(Opcode.F64_NE, C.Z),
			(Opcode.F64_LT, C.NC),
			(Opcode.F64_GT, C.NA),
			(Opcode.F64_LE, C.A),
			(Opcode.F64_GE, C.C)]) {
			bindHandler(t.0);
			asm.movsd_s_m(regs.xmm0, vsph[-2].value);
			asm.ucomisd_s_m(regs.xmm0, vsph[-1].value);
			asm.jc_rel_near(C.P, if(t.0 == Opcode.F64_NE, ret_one, ret_zero));
			asm.jc_rel_near(t.1, ret_zero);
			asm.jmp_rel_near(ret_one);
		}

		asm.bind(ret_zero);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsph[-1].value, 0);
		genDispatchOrJumpToDispatch();

		asm.bind(ret_one);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		genTagUpdate(BpTypeCode.I32.code);
		asm.movd_m_i(vsph[-1].value, 1);
		genDispatchOrJumpToDispatch();
	}
	def genGcInstrs() {
		bindHandler(Opcode.I31_NEW); {
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.d.shl_r_i(regs.tmp0, 1);
			asm.d.or_r_i(regs.tmp0, 1);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			genTagUpdate(BpTypeCode.I31REF.code);
			endHandler();
		}
		bindHandler(Opcode.I31_GET_S); {
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.d.cmp_r_i(regs.tmp0, 0);
			asm.jc_rel_far(X86_64Conds.Z, newTrapLabel(TrapReason.NULL_DEREF));
			asm.d.sar_r_i(regs.tmp0, 1);
			asm.movq_m_r(vsph[-1].value, regs.tmp0);
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.I31_GET_U); {
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.d.cmp_r_i(regs.tmp0, 0);
			asm.jc_rel_far(X86_64Conds.Z, newTrapLabel(TrapReason.NULL_DEREF));
			asm.d.shr_r_i(regs.tmp0, 1);
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.ARRAY_LEN); {
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			asm.q.cmp_r_i(regs.tmp0, 0);
			asm.jc_rel_far(X86_64Conds.Z, newTrapLabel(TrapReason.NULL_DEREF));
			asm.movq_r_m(regs.tmp0, regs.tmp0.plus(offsets.HeapArray_vals));
			asm.movd_r_m(regs.tmp0, regs.tmp0.plus(offsets.Array_length));
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
		}
		bindHandler(Opcode.REF_TEST); {
			var nullable_reg = regs.tmp3;
			var shared = X86_64Label.new();
			asm.movd_r_i(nullable_reg, 0);
			asm.bind(shared); // shared code between ref.test and ref.test_null
			genReadSleb32_inline(regs.tmp1);
			saveCallerIVars();
			callRuntime(refRuntimeCall(RT.runtime_doCast), [regs.INSTANCE, nullable_reg, regs.tmp1], false);
			asm.movbzx_r_r(regs.tmp0, Target.V3_RET_GPRS[0]); // XXX: restore just VSP and update first?
			restoreCallerIVars();
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			genTagUpdate(BpTypeCode.I32.code);
			endHandler();
			// ref.test_null jumps back to ref.test
			bindHandler(Opcode.REF_TEST_NULL);
			asm.movd_r_i(nullable_reg, 1);
			asm.jmp_rel_near(shared);
		}
		bindHandler(Opcode.REF_CAST); {
			var nullable_reg = regs.tmp3;
			var shared = X86_64Label.new();
			asm.movd_r_i(nullable_reg, 0);
			asm.bind(shared); // shared code between ref.cast and ref.cast_null
			genReadSleb32_inline(regs.tmp1);
			saveCallerIVars();
			callRuntime(refRuntimeCall(RT.runtime_doCast), [regs.INSTANCE, nullable_reg, regs.tmp1], false);
			asm.cmpb_r_i(Target.V3_RET_GPRS[0], 0);
			asm.jc_rel_far(C.Z, newTrapLabel(TrapReason.FAILED_CAST));
			restoreCallerIVars();
			endHandler();
			// ref.cast_null jumps back to ref.test
			bindHandler(Opcode.REF_CAST_NULL);
			asm.movd_r_i(nullable_reg, 1);
			asm.jmp_rel_near(shared);
		}
		for (t in [
			(true, Opcode.BR_ON_CAST, Opcode.BR_ON_CAST_NULL),
			(false, Opcode.BR_ON_CAST_FAIL, Opcode.BR_ON_CAST_FAIL_NULL)]) {
			bindHandler(t.1);
			var nullable_reg = regs.tmp3;
			var shared = X86_64Label.new();
			asm.movd_r_i(nullable_reg, 0);
			asm.bind(shared); 			// shared code between br_on_cast and br_on_cast_null
			genSkipLeb();            		// skip target label to get to heap type
			genReadSleb32_inline(regs.tmp1);	// read heaptype
			saveCallerIVars();
			callRuntime(refRuntimeCall(RT.runtime_doCast), [regs.INSTANCE, nullable_reg, regs.tmp1], false);
			asm.cmpb_r_i(Target.V3_RET_GPRS[0], 0);
			restoreCallerIVars();
			var cond = if(t.0, C.Z, C.NZ); // TODO: br_on_cast/fail only differ on condition
			asm.jc_rel_far(cond, controlSkipSidetableAndDispatchLabel); // XXX: relies on no changes to flags
			restoreIpFromCurIp(-1); // TODO: sidetable entries are relative to current IP; we are at nextIP
			asm.jmp_rel_far(controlTransferLabel);
			// br_on_cast_null jumps back to br_on_cast
			bindHandler(t.2);
			asm.movd_r_i(nullable_reg, 1);
			asm.jmp_rel_near(shared);
		}
		bindHandler(Opcode.REF_AS_NON_NULL); {
			asm.q.cmp_m_i(vsph[-1].value, 0);
			asm.jc_rel_far(C.Z, newTrapLabel(TrapReason.NULL_DEREF));
			endHandler();
		}
		// internalize/externalize are nops.
		patchDispatchTable(Opcode.EXTERN_INTERNALIZE, firstDispatchOffset);
		patchDispatchTable(Opcode.EXTERN_EXTERNALIZE, firstDispatchOffset);
	}
	def genMisc() {
		bindHandler(Opcode.MEMORY_SIZE); {
			genReadUleb32(regs.tmp1);
			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_memories));
			asm.movq_r_m(regs.tmp0, regs.tmp0.plusR(regs.tmp1, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(regs.tmp1, regs.tmp0.plus(offsets.X86_64Memory_limit));
			asm.movq_r_m(regs.tmp0, regs.tmp0.plus(offsets.X86_64Memory_start));
			asm.q.sub_r_r(regs.tmp1, regs.tmp0);
			asm.q.shr_r_i(regs.tmp1, 16);
			genTagPush(BpTypeCode.I32.code);
			asm.movq_m_r(vsph[0].value, regs.tmp1);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.REF_NULL); {
			genSkipLeb();
			genTagPush(BpTypeCode.REF_NULL.code);
			asm.movq_m_i(vsph[0].value, 0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.REF_IS_NULL); {
			asm.d.test_m_i(vsph[-1].value, -1);
			asm.set_r(C.Z, regs.tmp0);
			asm.movbzx_r_r(regs.tmp0, regs.tmp0);
			if (valuerep.tagged) asm.movd_m_i(vsph[-1].tag, i7.view(BpTypeCode.I32.code));
			asm.movd_m_r(vsph[-1].value, regs.tmp0);
			endHandler();
		}
		bindHandler(Opcode.REF_FUNC); {
			genReadUleb32(regs.tmp1);
			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_functions));
			asm.movq_r_m(regs.tmp0, regs.tmp0.plusR(regs.tmp1, offsets.REF_SIZE, offsets.Array_contents));
			genTagPush(BpTypeCode.FUNCREF.code);
			asm.movq_m_r(vsph[0].value, regs.tmp0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		bindHandler(Opcode.DATA_DROP); {
			genReadUleb32(regs.tmp1);
			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_dropped_data));
			asm.movb_m_i(regs.tmp0.plusR(regs.tmp1, 1, offsets.Array_contents), 1);
			endHandler();
		}
		bindHandler(Opcode.ELEM_DROP); {
			genReadUleb32(regs.tmp1);
			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_dropped_elems));
			asm.movb_m_i(regs.tmp0.plusR(regs.tmp1, 1, offsets.Array_contents), 1);
			endHandler();
		}
		bindHandler(Opcode.TABLE_SIZE); {
			genReadUleb32(regs.tmp1);
			asm.movq_r_m(regs.tmp0, regs.INSTANCE.plus(offsets.Instance_tables));
			asm.movq_r_m(regs.tmp0, regs.tmp0.plusR(regs.tmp1, offsets.REF_SIZE, offsets.Array_contents));
			asm.movq_r_m(regs.tmp0, regs.tmp0.plus(offsets.Table_elems));
			asm.movq_r_m(regs.tmp0, regs.tmp0.plus(offsets.Array_length));
			genTagPush(BpTypeCode.I32.code);
			asm.movq_m_r(vsph[0].value, regs.tmp0);
			asm.add_r_i(vsp, valuerep.slot_size);
			endHandler();
		}
		writeDispatchEntry(dispatchTables[0].1, InternalOpcode.PROBE.code, w.atEnd().pos); {
			computeCurIpFromIp(-1);
			computePcFromCurIp();
			saveCallerIVars();
			asm.movq_r_m(regs.tmp0, frame.WASM_FUNC); // XXX: compute func and pc directly in the right regs
			callRuntime(refRuntimeCall(RT.runtime_PROBE_instr), [regs.tmp0, regs.CURPC], true);
			restoreCallerIVars();
			// Compute a pointer to the original code at this pc offset
			var pc = regs.tmp1; // = IP - CODE
			asm.movq_r_r(pc, regs.IP);
			asm.sub_r_m(pc, frame.CODE);
			var origIp = regs.tmp0; // FUNC_DECL.orig_bytecode + pc - 1
			asm.movq_r_m(origIp, regs.FUNC_DECL.plus(offsets.FuncDecl_orig_bytecode));
			asm.add_r_r(origIp, pc);
			asm.sub_r_i(origIp, 1);
			genDispatch(origIp.indirect(), dispatchTables[0].1, false);
		}
		if (tuning.dispatchTableReg) {
			var offset = w.atEnd().pos;
			for (i < 256) {
				writeDispatchEntry(probedDispatchTableRef, i, offset);
			}
			computeCurIpForTrap(-1);
			computePcFromCurIp();
			saveCallerIVars();
			asm.movq_r_m(regs.tmp0, frame.WASM_FUNC); // XXX: compute func and pc directly in the right regs
			callRuntime(refRuntimeCall(RT.runtime_PROBE_loop), [regs.tmp0, regs.CURPC], true);
			restoreCallerIVars();
			// TODO: reload code from function, as local probes may have been inserted or removed
			asm.sub_r_i(regs.IP, 1);
			genDispatch(regs.IP.indirect(), dispatchTables[0].1, true);
		}
	}
	def genFloatMinAndMax() {
		var ret_b = X86_64Label.new(), ret_a = X86_64Label.new(), is_nan32 = X86_64Label.new(), is_nan64 = X86_64Label.new();
		bindHandler(Opcode.F32_MIN);
		asm.movss_s_m(regs.xmm0, vsph[-2].value);
		asm.movss_s_m(regs.xmm1, vsph[-1].value);
		asm.ucomiss_s_s(regs.xmm0, regs.xmm1);
		asm.jc_rel_far(C.P, is_nan32);
		asm.jc_rel_near(C.C, ret_a);
		asm.jc_rel_near(C.A, ret_b);
		asm.d.cmp_m_i(vsph[-1].value, 0);
		asm.jc_rel_near(C.S, ret_b); // handle min(-0, 0) == -0
		asm.jmp_rel_near(ret_a);

		bindHandler(Opcode.F32_MAX);
		asm.movss_s_m(regs.xmm0, vsph[-2].value);
		asm.movss_s_m(regs.xmm1, vsph[-1].value);
		asm.ucomiss_s_s(regs.xmm0, regs.xmm1);
		asm.jc_rel_far(C.P, is_nan32);
		asm.jc_rel_near(C.C, ret_b);
		asm.jc_rel_near(C.A, ret_a);
		asm.d.cmp_m_i(vsph[-1].value, 0);
		asm.jc_rel_near(C.NS, ret_b); // handle max(-0, 0) == 0
		asm.jmp_rel_near(ret_a);

		bindHandler(Opcode.F64_MIN);
		asm.movsd_s_m(regs.xmm0, vsph[-2].value);
		asm.movsd_s_m(regs.xmm1, vsph[-1].value);
		asm.ucomisd_s_s(regs.xmm0, regs.xmm1);
		asm.jc_rel_near(C.P, is_nan64);
		asm.jc_rel_near(C.C, ret_a);
		asm.jc_rel_near(C.A, ret_b);
		asm.d.cmp_m_i(vsph[-1].upper, 0);
		asm.jc_rel_near(C.S, ret_b); // handle min(-0, 0) == -0
		// fall through to ret_a
		asm.bind(ret_a);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		endHandler();

		bindHandler(Opcode.F64_MAX);
		asm.movsd_s_m(regs.xmm0, vsph[-2].value);
		asm.movsd_s_m(regs.xmm1, vsph[-1].value);
		asm.ucomisd_s_s(regs.xmm0, regs.xmm1);
		asm.jc_rel_near(C.P, is_nan64);
		asm.jc_rel_near(C.C, ret_b);
		asm.jc_rel_near(C.A, ret_a);
		asm.d.cmp_m_i(vsph[-1].upper, 0);
		asm.jc_rel_near(C.S, ret_a); // handle max(-0, 0) == 0
		// fall through to ret_b
		asm.bind(ret_b);
		asm.movsd_m_s(vsph[-2].value, regs.xmm1);
		asm.q.sub_r_i(vsp, valuerep.slot_size);
		endHandler();

		asm.bind(is_nan32);
		asm.movd_m_i(vsph[-2].value, int.view(Floats.f_nan));
		asm.jmp_rel_near(ret_a);

		asm.bind(is_nan64);
		asm.movd_m_i(vsph[-2].upper, int.view(Floats.d_nan >> 32));
		asm.movd_m_i(vsph[-2].value, 0);
		asm.jmp_rel_near(ret_a);
	}
	def genFloatTruncs() {
		for (opcode in [
			Opcode.I32_TRUNC_F32_S,
			Opcode.I32_TRUNC_F32_U,
			Opcode.I32_TRUNC_F64_S,
			Opcode.I32_TRUNC_F64_U,
			Opcode.I64_TRUNC_F32_S,
			Opcode.I64_TRUNC_F32_U,
			Opcode.I64_TRUNC_F64_S,
			Opcode.I64_TRUNC_F64_U,
			Opcode.I32_TRUNC_SAT_F32_S,
			Opcode.I32_TRUNC_SAT_F32_U,
			Opcode.I32_TRUNC_SAT_F64_S,
			Opcode.I32_TRUNC_SAT_F64_U,
			Opcode.I64_TRUNC_SAT_F32_S,
			Opcode.I64_TRUNC_SAT_F32_U,
			Opcode.I64_TRUNC_SAT_F64_S,
			Opcode.I64_TRUNC_SAT_F64_U]) {
			bindHandler(opcode);
			// XXX: don't load current IP for saturating conversions
			computeCurIpForTrap(-1);
			// load value from stack
			if (opcode.sig.params[0] == ValueType.F32) asm.movss_s_m(regs.xmm0, vsph[-1].value);
			else asm.movsd_s_m(regs.xmm0, vsph[-1].value);
			// emit conversion
			masm.emit_i_trunc_f(opcode, regs.tmp0, regs.xmm0, regs.xmm1);
			// store and update tag
			if (opcode.sig.results[0] == ValueType.I32) {
				asm.movd_m_r(vsph[-1].value, regs.tmp0);
				genTagUpdate(BpTypeCode.I32.code);
			} else {
				asm.movq_m_r(vsph[-1].value, regs.tmp0);
				genTagUpdate(BpTypeCode.I64.code);
			}
			endHandler();
		}
	}
	def genFloatConversions() {
		bindHandler(Opcode.F32_CONVERT_I32_S); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.q.shl_r_i(regs.tmp0, 32);
			asm.q.sar_r_i(regs.tmp0, 32); // sign-extend
			asm.cvtsi2ss_s_r(regs.xmm0, regs.tmp0);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_CONVERT_I32_U); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.cvtsi2ss_s_r(regs.xmm0, regs.tmp0);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_CONVERT_I64_S); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			asm.cvtsi2ss_s_r(regs.xmm0, regs.tmp0);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_CONVERT_I64_U); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			masm.emit_f32_convert_i64_u(regs.xmm0, regs.tmp0, regs.xmm1, regs.scratch);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F32_DEMOTE_F64); {
			genTagUpdate(BpTypeCode.F32.code);
			asm.cvtsd2ss_s_m(regs.xmm0, vsph[-1].value);
			asm.movss_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I32_S); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.q.shl_r_i(regs.tmp0, 32);
			asm.q.sar_r_i(regs.tmp0, 32); // sign-extend
			asm.cvtsi2sd_s_r(regs.xmm0, regs.tmp0);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I32_U); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movd_r_m(regs.tmp0, vsph[-1].value);
			asm.cvtsi2sd_s_r(regs.xmm0, regs.tmp0);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I64_S); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			asm.cvtsi2sd_s_r(regs.xmm0, regs.tmp0);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_CONVERT_I64_U); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.movq_r_m(regs.tmp0, vsph[-1].value);
			masm.emit_f64_convert_i64_u(regs.xmm0, regs.tmp0, regs.xmm1, regs.scratch);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
		bindHandler(Opcode.F64_PROMOTE_F32); {
			genTagUpdate(BpTypeCode.F64.code);
			asm.cvtss2sd_s_m(regs.xmm0, vsph[-1].value);
			asm.movsd_m_s(vsph[-1].value, regs.xmm0);
			endHandler();
		}
	}
	def genRuntimeCallOps() {
		// generate code for runtime calls with 1 LEB that cannot trap.
		var call_irt = asm.newLabel();
		for (t in [
			(Opcode.GLOBAL_GET, refRuntimeCall(RT.runtime_GLOBAL_GET)),
			(Opcode.GLOBAL_SET, refRuntimeCall(RT.runtime_GLOBAL_SET)),
			(Opcode.MEMORY_GROW, refRuntimeCall(RT.runtime_MEMORY_GROW)),
			(Opcode.TABLE_GROW, refRuntimeCall(RT.runtime_TABLE_GROW)),
			(Opcode.STRUCT_NEW_CANON, refRuntimeCall(RT.runtime_STRUCT_NEW_CANON)),
			(Opcode.STRUCT_NEW_CANON_DEFAULT, refRuntimeCall(RT.runtime_STRUCT_NEW_CANON_DEFAULT))
		]) {
			bindHandler(t.0);
			genReadUleb32(regs.tmp0);
			saveCallerIVars();
			callRuntime(t.1, [regs.INSTANCE, regs.tmp0], false);
			restoreCallerIVars();
			endHandler();
		}
		asm.bind(call_irt);

		// generate code for runtime calls with 1 LEB that can trap.
		call_irt = asm.newLabel();
		for (t in [
			(Opcode.TABLE_GET, refRuntimeCall(RT.runtime_TABLE_GET)),
			(Opcode.TABLE_SET, refRuntimeCall(RT.runtime_TABLE_SET)),
			(Opcode.MEMORY_FILL, refRuntimeCall(RT.runtime_MEMORY_FILL)),
			(Opcode.TABLE_FILL, refRuntimeCall(RT.runtime_TABLE_FILL)),
			(Opcode.ARRAY_NEW_CANON, refRuntimeCall(RT.runtime_ARRAY_NEW_CANON)),
			(Opcode.ARRAY_NEW_CANON_DEFAULT, refRuntimeCall(RT.runtime_ARRAY_NEW_CANON_DEFAULT)),
			(Opcode.ARRAY_GET, refRuntimeCall(RT.runtime_ARRAY_GET)),
			(Opcode.ARRAY_GET_S, refRuntimeCall(RT.runtime_ARRAY_GET_S)),
			(Opcode.ARRAY_GET_U, refRuntimeCall(RT.runtime_ARRAY_GET_U)),
			(Opcode.ARRAY_SET, refRuntimeCall(RT.runtime_ARRAY_SET))
		]) {
			bindHandler(t.0);
			genReadUleb32(regs.tmp0);
			saveCallerIVars();
			callRuntime(t.1, [regs.INSTANCE, regs.tmp0], true);
			restoreCallerIVars();
			endHandler();
		}
		asm.bind(call_irt);

		// generate code for runtime calls with 2 LEBS that can trap.
		call_irt = asm.newLabel();
		for (t in [
			(Opcode.TABLE_INIT, refRuntimeCall(RT.runtime_TABLE_INIT)),
			(Opcode.MEMORY_INIT, refRuntimeCall(RT.runtime_MEMORY_INIT)),
			(Opcode.MEMORY_COPY, refRuntimeCall(RT.runtime_MEMORY_COPY)),
			(Opcode.TABLE_COPY, refRuntimeCall(RT.runtime_TABLE_COPY)),
			(Opcode.STRUCT_GET, refRuntimeCall(RT.runtime_STRUCT_GET)),
			(Opcode.STRUCT_GET_S, refRuntimeCall(RT.runtime_STRUCT_GET_S)),
			(Opcode.STRUCT_GET_U, refRuntimeCall(RT.runtime_STRUCT_GET_U)),
			(Opcode.STRUCT_SET, refRuntimeCall(RT.runtime_STRUCT_SET)),
			(Opcode.ARRAY_NEW_CANON_FIXED, refRuntimeCall(RT.runtime_ARRAY_NEW_CANON_FIXED))
		]) {
			bindHandler(t.0);
			genReadUleb32(regs.tmp0);
			genReadUleb32(regs.tmp1);
			saveCallerIVars();
			callRuntime(t.1, [regs.INSTANCE, regs.tmp0, regs.tmp1], true);
			genAbruptRetCheck();
			restoreCallerIVars();
			endHandler();
		}
		asm.bind(call_irt);
	}
	def bindHandler(opcode: Opcode) {
		if (tuning.handlerAlignment > 1) w.align(tuning.handlerAlignment);
		patchDispatchTable(opcode, w.atEnd().pos);
	}
	def bindHandlerNoAlign(opcode: Opcode) {
		patchDispatchTable(opcode, w.atEnd().pos);
	}
	def genAbruptRetCheck() {
		asm.q.cmp_r_i(Target.V3_RET_GPRS[0], 0);
		asm.jc_rel_far(C.NZ, abruptRetLabel);
	}
	def saveCallerIVars() {
		saveIVar(regs.IP);
		saveIVar(regs.STP);
		if (tuning.recordCurIpForTraps) saveIVar(regs.CURPC);
	}
	def restoreCallerIVars() {
		restoreReg(regs.IP);
		restoreReg(regs.STP);
		restoreReg(regs.EIP);
		restoreReg(regs.INSTANCE);
		restoreReg(regs.FUNC_DECL);
		restoreReg(regs.MEM0_BASE);
		restoreReg(regs.VFP);
	}
	def callRuntime(abs: Pointer, args: Array<X86_64Gpr>, canTrap: bool) {
		saveIVar(regs.VSP);
		// save a copy of VSP into valueStack.sp
		asm.movq_r_m(regs.scratch, absPointer(offsets.Interpreter_valueStack));
		asm.movq_m_r(regs.scratch.plus(offsets.ValueStack_sp), regs.VSP);
		// Generate parallel moves from args into param gprs; assume each src register used only once
		var dst = Array<X86_64Gpr>.new(G.length);
		for (i < args.length) {
			var sreg = args[i];
			var dreg = Target.V3_PARAM_GPRS[i + 1];
			if (sreg != dreg) dst[sreg.regnum] = dreg;
		}
		var stk = Array<i8>.new(G.length);
		for (i < dst.length) orderMoves(dst, stk, i);
		// emit actual call
		asm.callr(int.!(abs - (ic.start + w.pos + 5)));
		// check for trap
		if (canTrap) genAbruptRetCheck();
		if (tuning.dispatchTableReg) {
			// restore dispatch table from interpreter.dispatchTable
			asm.movq_r_m(regs.DISPATCH_TABLE, absPointer(offsets.Interpreter_dispatchTable));
		}
		// restore VSP from valueStack.sp
		asm.movq_r_m(regs.VSP, absPointer(offsets.Interpreter_valueStack));
		asm.movq_r_m(regs.VSP, regs.VSP.plus(offsets.ValueStack_sp));
	}
	def absPointer(ptr: Pointer) -> X86_64Addr {
		return X86_64Addr.new(null, null, 1, int.!(ptr - Pointer.NULL));
	}
	def orderMoves(dst: Array<X86_64Gpr>, stk: Array<i8>, i: int) {
		var dreg = dst[i];
		if (dreg == null) return;		// no moves here
		if (stk[i] > 0) return;			// this node already done
		stk[i] = -1;				// mark as on stack
		if (stk[dreg.regnum] < 0) {		// destination on stack => cycle
			asm.movq_r_r(regs.scratch, dreg);	// save destination first
			stk[dreg.regnum] = -2;		// mark as cycle
		} else {
			orderMoves(dst, stk, dreg.regnum);	// recurse on destination
		}
		asm.movq_r_r(dreg, if(stk[i] == -2, regs.scratch, G[i]));	// emit post-order move
		stk[i] = 1;				// mark as done
	}
	def genTagUpdate(tag: byte) {
		if (valuerep.tagged) asm.movq_m_i(vsph[-1].tag, tag);
	}
	def genTagPush(tag: byte) {
		if (valuerep.tagged) asm.movq_m_i(vsph[0].tag, i7.view(tag));
	}
	def genTagPushR(r: X86_64Gpr) {
		if (valuerep.tagged) asm.movq_m_r(vsph[0].tag, r);
	}
	def genCopySlot(dst: X86_64Addr, src: X86_64Addr) {
		match (valuerep.slot_size) {
			8 => {
				asm.movq_r_m(regs.scratch, src);
				asm.movq_m_r(dst, regs.scratch);
			}
			16 => {
				asm.movdqu_s_m(regs.xmm0, src);
				asm.movdqu_m_s(dst, regs.xmm0);
			}
			32 => {
				asm.movdqu_s_m(regs.xmm0, src);
				asm.movdqu_m_s(dst, regs.xmm0);
				asm.movdqu_s_m(regs.xmm0, src.plus(16));
				asm.movdqu_m_s(dst.plus(16), regs.xmm0);
			}
			_ => {
				fatal(Strings.format1("unsupported value slot size: %d", valuerep.slot_size));
			}
		}
	}
	def saveIVar(r: X86_64Gpr) {
		for (t in all_ivars) {
			if (t.0 == r) asm.movq_m_r(t.1, r);
		}
	}
	def spillReg(r: X86_64Gpr) {
		for (t in mutable_ivars) {
			if (t.0 == r) asm.movq_m_r(t.1, r);
		}
	}
	def restoreReg(r: X86_64Gpr) {
		for (t in all_ivars) {
			if (t.0 == r) asm.movq_r_m(r, t.1);
		}
	}
	def genLoad(opcode: Opcode, tag: byte, gen: (X86_64Gpr, X86_64Addr) -> X86_64Assembler) {
		bindHandler(opcode);
		computeCurIpForTrap(-1);
		asm.q.inc_r(regs.IP);				// skip flags byte
		genReadUleb32(regs.tmp0);			// decode offset
		asm.movd_r_m(regs.tmp1, vsph[-1].value);	// read index
		asm.q.add_r_r(regs.tmp0, regs.tmp1);		// add index + offset
		gen(regs.tmp1, regs.MEM0_BASE.plusR(regs.tmp0, 1, 0));
		if (valuerep.tagged && tag != BpTypeCode.I32.code) genTagUpdate(tag); // update tag if necessary
		asm.movq_m_r(vsph[-1].value, regs.tmp1);
		endHandler();
	}
	def genStore(gen: (X86_64Addr, X86_64Gpr) -> X86_64Assembler) {
		computeCurIpForTrap(-1);
		asm.q.inc_r(regs.IP);				// skip flags byte
		genReadUleb32(regs.tmp0);			// decode offset
		asm.movd_r_m(regs.tmp1, vsph[-2].value);	// read index
		asm.q.add_r_r(regs.tmp0, regs.tmp1);		// add index + offset
		asm.movq_r_m(regs.tmp1, vsph[-1].value);	// read value
		gen(regs.MEM0_BASE.plusR(regs.tmp0, 1, 0), regs.tmp1);
		asm.q.sub_r_i(vsp, 2 * valuerep.slot_size);
		endHandler();
	}
	def genPopFrameAndRet() {
		genInvalidateFrameAccessor();
		asm.q.add_r_i(R.RSP, ic.frameSize);
		asm.ret();
	}
	def genInvalidateFrameAccessor() {
		if (tuning.cacheFrameAccessor) asm.movq_m_i(frame.ACCESSOR, 0);
	}

	// Generate a read of a 32-bit unsigned LEB.
	def genReadUleb32(dest: X86_64Gpr) {
		var ool_leb: OutOfLineLEB;
		if (!tuning.inlineAllLEBs) {
			ool_leb = OutOfLineLEB.new(dest);
			oolULeb32Sites.put(ool_leb);
		}
		var asm = this.asm.d;
		asm.movbzx_r_m(dest, regs.IP_ptr);	// load first byte
		asm.q.inc_r(regs.IP);			// increment pointer
		asm.test_r_i(dest, LEB_UPPER_BIT);	// test most-significant bit
		if (tuning.inlineAllLEBs) {
			var leb_done = X86_64Label.new();
			asm.jc_rel_near(C.Z, leb_done);
			genReadLEBext(dest);
			asm.bind(leb_done);
		} else {
			asm.jc_rel_addr(C.NZ, ool_leb);
			ool_leb.retOffset = asm.pos();
		}
	}
	// Generate a read of a 32-bit signed LEB.
	def genReadSleb32_inline(dest: X86_64Gpr) {
		var done = X86_64Label.new(), sext = X86_64Label.new(), loop = X86_64Label.new();
		asm.movd_r_i(R.RCX, 0);
		asm.movd_r_i(dest, 0);

		asm.bind(loop);
		asm.movbzx_r_m(regs.scratch, regs.IP_ptr);	// load byte
		asm.q.inc_r(regs.IP);			// increment pointer
		asm.d.test_r_i(regs.scratch, LEB_UPPER_BIT);	// test most-significant bit
		asm.jc_rel_near(C.Z, sext);		// break if not set
		asm.d.and_r_i(regs.scratch, 0x7F);	// mask off upper bit
		asm.d.shl_r_cl(regs.scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, regs.scratch);	// merge byte into val
		asm.d.add_r_i(R.RCX, 7);		// compute next bit pos
		asm.jmp_rel_near(loop);			// loop

		asm.bind(sext);
		asm.d.shl_r_cl(regs.scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, regs.scratch);	// merge byte into val
		asm.d.sub_r_i(R.RCX, 25);		// compute 25 - shift
		asm.d.neg_r(R.RCX);
		asm.jc_rel_near(C.S, done);		// if shift > 25, done
		asm.d.shl_r_cl(dest);			// sign extension
		asm.d.sar_r_cl(dest);
		asm.bind(done);
	}
	// Generate a read of a 64-bit signed LEB.
	def genReadSleb64_inline(dest: X86_64Gpr) {
		var done = X86_64Label.new(), sext = X86_64Label.new(), loop = X86_64Label.new();
		asm.movd_r_i(R.RCX, 0);
		asm.movd_r_i(dest, 0);

		asm.bind(loop);
		asm.movbzx_r_m(regs.scratch, regs.IP_ptr);	// load byte
		asm.q.inc_r(regs.IP);			// increment pointer
		asm.d.test_r_i(regs.scratch, LEB_UPPER_BIT);	// test most-significant bit
		asm.jc_rel_near(C.Z, sext);		// break if not set
		asm.d.and_r_i(regs.scratch, 0x7F);	// mask off upper bit
		asm.q.shl_r_cl(regs.scratch);		// shift byte into correct bit pos
		asm.q.or_r_r(dest, regs.scratch);	// merge byte into val
		asm.d.add_r_i(R.RCX, 7);		// compute next bit pos
		asm.jmp_rel_near(loop);			// loop

		asm.bind(sext);
		asm.q.shl_r_cl(regs.scratch);		// shift byte into correct bit pos
		asm.q.or_r_r(dest, regs.scratch);	// merge byte into val
		asm.d.sub_r_i(R.RCX, 57);		// compute 57 - shift
		asm.d.neg_r(R.RCX);
		asm.jc_rel_near(C.S, done);		// if shift > 57, done
		asm.q.shl_r_cl(dest);			// sign extension
		asm.q.sar_r_cl(dest);
		asm.bind(done);
	}
	def genSkipBlockType() {
		if (!tuning.complexBlockTypes) return genSkipLeb();
		var tmp = regs.tmp0;
		var done = X86_64Label.new();
		asm.d.movbzx_r_m(tmp, regs.IP_ptr);	// load first byte
		asm.q.inc_r(regs.IP);			// increment pointer
		// check for extended LEB, abstract type, or type with immediate
		var typeTableAddr = int.!((ic.start + typeTagTableOffset) - Pointer.NULL);
		asm.test_m_i(tmp.plus(typeTableAddr), TYPE_HAS_IMM | TYPE_IS_LEB);
		asm.jc_rel_near(C.Z, done);
		// handle extended LEB, types with immediates, and abstract types
		var type_not_leb = X86_64Label.new();
		asm.d.test_r_i(tmp, LEB_UPPER_BIT);		// check for LEB first
		asm.jc_rel_near(C.Z, type_not_leb);
		genSkipLeb0(regs.tmp3);
		asm.bind(type_not_leb);
		// check for types that have an immediate
		asm.d.test_m_i(tmp.plus(typeTableAddr), TYPE_HAS_IMM);
		asm.jc_rel_near(C.Z, done);
		genSkipLeb();
		asm.bind(done);
	}
	def genSkipSidetableEntry() {
		asm.add_r_i(regs.STP, offsets.STP_entry_size);
	}
	// Generate code which skips over an LEB.
	def genSkipLeb() {
		genSkipLeb0(regs.scratch);
	}
	def genSkipLeb0(scratch: X86_64Gpr) {
		var more = X86_64Label.new();
		asm.bind(more);
		asm.movbzx_r_m(scratch, regs.IP_ptr);	// load first byte
		asm.q.inc_r(regs.IP);			// increment pointer
		asm.test_r_i(scratch, LEB_UPPER_BIT);	// test most-significant bit
		asm.jc_rel_near(C.NZ, more);
	}
	// End the handler for the current bytecode
	def endHandler() {
		genDispatchOrJumpToDispatch();
	}
	// Generate an inline dispatch or a jump to the dispatch loop, depending on config.
	def genDispatchOrJumpToDispatch() {
		var gen = tuning.threadedDispatch;
		if (firstDispatchOffset == 0) {
			firstDispatchOffset = w.pos;
			gen = true;
		}
		if (gen) {
			genDispatch(regs.IP_ptr, if (!tuning.dispatchTableReg, dispatchTables[0].1), true);
		} else {
			asm.jmp_rel(firstDispatchOffset - w.atEnd().pos);
		}
	}
	// Generate a load of the next bytecode and a dispatch through the dispatch table.
	def genDispatch(ptr: X86_64Addr, table: IcCodeRef, increment: bool) {
		var opcode = regs.tmp0;
		var base = regs.tmp1;
		if (ptr != null) asm.movbzx_r_m(opcode, ptr);
		if (increment) asm.inc_r(regs.IP);
		match (tuning.dispatchEntrySize) {
			2 => {
				if (table == null) asm.movq_r_r(base, regs.DISPATCH_TABLE);
				else asm.lea(base, table); // RIP-relative LEA
				asm.movwsx_r_m(opcode, base.plusR(opcode, 2, 0)); // load 16-bit offset
				asm.add_r_r(base, opcode);
				if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
				asm.ijmp_r(base);
			}
			4 => {
				if (table == null) {
					asm.movd_r_m(base, regs.DISPATCH_TABLE.plusR(opcode, 4, 0));
				} else {
					var addr = ic.start + table.offset;
					asm.movd_r_m(base, X86_64Addr.new(null, opcode, 4, int.!(addr - Pointer.NULL)));
				}
				if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
				asm.ijmp_r(base);
			}
			8 => {
				if (table == null) {
					if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
					asm.ijmp_m(regs.DISPATCH_TABLE.plusR(opcode, 8, 0));
				} else {
					var addr = ic.start + table.offset;
					if (dispatchJmpOffset < 0) dispatchJmpOffset = w.pos;
					asm.ijmp_m(X86_64Addr.new(null, opcode, 8, int.!(addr - Pointer.NULL)));
				}
			}
		}
	}
	// Patch the dispatch table for the given opcode to go to the given position.
	def patchDispatchTable(opcode: Opcode, pos: int) {
		for (t in dispatchTables) {
			if (t.0 != opcode.prefix) continue;
			var ref1 = t.1;
			if (opcode.prefix == 0 || opcode.code < 128) writeDispatchEntry(ref1, opcode.code, pos);
			var ref2 = t.2;
			if (ref2 != null) writeDispatchEntry(ref2, opcode.code, pos);
			w.atEnd();
			return;
		}
		fatal("no dispatch table found for prefix");
	}
	// Generate the out-of-line LEB decoding code.
	def genOutOfLineLEBs() { // XXX: use a separate out-of-line assembler on the end of the buffer
		for (i < oolULeb32Sites.length) {
			var o = oolULeb32Sites[i];
			var pos = w.atEnd().pos;
			w.at(o.pos).put_b32(pos - (o.pos + o.delta));
			w.atEnd();
			// XXX: share code between out-of-line LEB cases
			genReadLEBext(o.dest);
			asm.jmp_rel(o.retOffset - w.atEnd().pos);
		}
		oolULeb32Sites = null;
	}
	// Generate code for > 1 byte LEB cases
	def genReadLEBext(dest: X86_64Gpr) {
		var destRcx = dest == R.RCX;
		asm.d.and_r_i(dest, 0x7F);		// mask off upper bit of first byte
		if (destRcx) {
			asm.movd_r_r(regs.tmp3, dest);
			dest = regs.tmp3;
		} else {
			asm.movd_r_r(regs.tmp3, R.RCX);	// save RCX
		}
		asm.movd_r_i(R.RCX, 7);
		var loop = X86_64Label.new(), nomore = X86_64Label.new();
		asm.bind(loop);
		asm.movbzx_r_m(regs.scratch, regs.IP_ptr);	// load byte
		asm.q.inc_r(regs.IP);				// increment pointer
		asm.d.test_r_i(regs.scratch, 0x80);		// test most-significant bit
		asm.jc_rel_near(C.Z, nomore);			// break if not set
		asm.d.and_r_i(regs.scratch, 0x7F);		// mask off upper bit
		asm.d.shl_r_cl(regs.scratch);			// shift byte into correct bit pos
		asm.d.or_r_r(dest, regs.scratch);		// merge byte into val
		asm.d.add_r_i(R.RCX, 7);			// compute next bit pos
		asm.jmp_rel_near(loop);				// loop

		asm.bind(nomore);
		asm.d.shl_r_cl(regs.scratch);		// shift byte into correct bit pos
		asm.d.or_r_r(dest, regs.scratch);	// merge byte into val
		if (destRcx) asm.movd_r_r(R.RCX, dest);
		else asm.movd_r_r(R.RCX, regs.tmp3);	// restore RCX
	}
	// Runtime calls and traps need CURPC register to be valid.
	def computeCurIpForTrap(delta: int) {
		if (tuning.recordCurIpForTraps) computeCurIpFromIp(delta);
	}
	def computeCurIpFromIp(delta: int) {
		asm.q.lea(regs.CURPC, X86_64Addr.new(regs.IP, null, 1, delta - offsets.Array_contents));
	}
	def restoreIpFromCurIp(delta: int) {
		asm.q.lea(regs.IP, regs.CURPC.plus(0 - (delta - offsets.Array_contents)));
	}
	def computePcFromCurIp() {
		if (tuning.recordCurIpForTraps) asm.q.sub_r_m(regs.CURPC, frame.CODE);
	}
	// All traps are generated out-of-line and call into the runtime.
	def genTraps() {
		w.atEnd();

		var call_runtime_TRAP = X86_64Label.new();
		asm.bind(call_runtime_TRAP);
		computePcFromCurIp();
		saveCallerIVars();
		asm.movq_r_m(regs.tmp1, frame.WASM_FUNC);
		// XXX: load runtime arg registers directly
		callRuntime(refRuntimeCall(RT.runtime_TRAP), [regs.tmp1, regs.CURPC, regs.tmp4], true);
		asm.bind(abruptRetLabel);
		genPopFrameAndRet();

		for (reason in TrapReason) {
			if (reason == TrapReason.STACK_OVERFLOW) continue; // must be special
			if (reason == TrapReason.DIV_BY_ZERO) continue; // must be special
			var label = newTrapLabel(reason);
			if (label == null) continue;
			asm.bind(label);
			asm.movd_r_i(regs.tmp4, reason.tag);
			asm.jmp_rel_near(call_runtime_TRAP);
		}
		// divide by zero happens when RAX and RDX are clobbered
		var divzero_label = newTrapLabel(TrapReason.DIV_BY_ZERO);
		asm.bind(divzero_label);
		computePcFromCurIp();
		saveIVar(regs.STP);
		if (tuning.recordCurIpForTraps) saveIVar(regs.CURPC);
		// XXX: load runtime arg registers directly
		asm.movq_r_m(regs.tmp1, frame.WASM_FUNC);
		asm.movd_r_i(regs.tmp4, TrapReason.DIV_BY_ZERO.tag);
		callRuntime(refRuntimeCall(RT.runtime_TRAP), [regs.tmp1, regs.CURPC, regs.tmp4], true);
		asm.jmp_rel_near(abruptRetLabel);

		// stack overflow cannot call into runtime, because it might be out of stack (!)
		var stackoverflow_label = newTrapLabel(TrapReason.STACK_OVERFLOW);
		asm.bind(stackoverflow_label);
		var addr = Pointer.atObject(Execute.trapObjects[TrapReason.STACK_OVERFLOW.tag]) - Pointer.NULL;
		asm.movd_r_i(Target.V3_RET_GPRS[0], int.view(u32.!(addr)));
		asm.jmp_rel_near(abruptRetLabel);
		ic.stackOverflowHandlerOffset = stackoverflow_label.pos;
		ic.oobMemoryHandlerOffset = newTrapLabel(TrapReason.MEM_OUT_OF_BOUNDS).pos;
		ic.divZeroHandlerOffset = newTrapLabel(TrapReason.DIV_BY_ZERO).pos;

	}
	def newTrapLabel(reason: TrapReason) -> X86_64Label {
		return X86_64MasmLabel.!(masm.newTrapLabel(reason)).label;
	}
	def refRuntimeCall<P, R>(f: P -> R) -> Pointer {
		var ptr = CiRuntime.unpackClosure<X86_64Interpreter, P, R>(f).0;
		var abs = ptr - Pointer.NULL;
		if (abs > u32.max) fatal("runtime call address not in 4GB");
		return ptr;
	}
	def reportOom(w: DataWriter, nlength: int) -> DataWriter {
		fatal("ran out of buffer space");
		return w;
	}
}

// Assembler patching support for out-of-line LEBs and other code refs.
def ABS_MARKER = 0x55443322;
def REL_MARKER = 0x44332211;
class OutOfLineLEB(dest: X86_64Gpr) extends X86_64Addr {
	var retOffset: int; // where OOB code should "return"
	var pos: int = -1;
	var delta: int;

	new() super(null, null, 1, REL_MARKER) { }
}
class IcCodeRef(var offset: int) extends X86_64Addr {
	new() super(null, null, 1, REL_MARKER) { }
}
class Patcher(w: DataWriter) extends X86_64AddrPatcher {
	new() super(ABS_MARKER, REL_MARKER) { }
	def recordRel32(pos: int, delta: int, addr: X86_64Addr) {
		match (addr) {
			x: OutOfLineLEB => {
				x.pos = pos;
				x.delta = delta;
			}
			x: IcCodeRef => {
				if (x.offset < 0) System.error("InterpreterGen", "unbound forward code ref");
				w.at(pos).put_b32(x.offset - (pos + delta));
				w.atEnd();
			}
		}
	}
}
