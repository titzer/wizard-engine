// Copyright 2021 Wizard authors. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def RT: X86_64Runtime;
def PAGE_SIZE = 4096u;
def PAGE_SIZE_i: int = 4096;
def MMAP_RETRIES = 1;
def REDZONE_RETRIES = 1;
// Contains target-specific factory functions.
component Target {
	def V3_PARAM_GPRS = X86_64MasmRegs.PARAM_GPRS; // System-V
	def V3_RET_GPRS = X86_64MasmRegs.RET_GPRS; // System-V + 2

	def limit_memory_size = 0x1_0000uL * 0x1_0000uL;
	def limit_memory_size_64 = 0x4_0000uL * 0x1_0000uL;

	def newMemory = NativeWasmMemory.new;
	def forceGC = RiGc.forceGC;
	def newWasmStack = X86_64StackManager.getFreshStack;
	def recycleWasmStack = X86_64StackManager.recycleStack;
	def tagging = Tagging.new(!FeatureDisable.valueTags, !FeatureDisable.simd);

	private var offsets: V3Offsets;

	new() {
		if (!SpcTuning.disable) {
			ExecuteOptions.registerMode("spc", X86_64SpcAotStrategy.new(false), "pre-compile modules with SPC, no fallback");
			ExecuteOptions.registerMode("jit", X86_64SpcAotStrategy.new(true), "pre-compile modules with SPC, fallback to interpreter");
			ExecuteOptions.registerMode("lazy", X86_64SpcLazyStrategy.new(), "lazy-compile functions with SPC on first execution");
			ExecuteOptions.registerMode("dyn", X86_64DynamicStrategy.new(), "use fast interpreter first, compile hot functions with SPC");
		}
		ExecuteOptions.registerDefaultMode("int", X86_64InterpreterOnlyStrategy.new(),
			"use fast interpreter only");
		Instrumentation.probes.onEnable = X86_64Interpreter.onProbeEnable;
		Instrumentation.probes.onDisable = X86_64Interpreter.onProbeDisable;
	}

	def getOffsets() -> V3Offsets {
		if (offsets == null) offsets = V3Offsets.new();
		return offsets;
	}

	def getTestTiers() -> List<(string, ExecutionStrategy)> {
		var spc_mode = ("jit:", X86_64SpcAotStrategy.new(true));
		var int_mode = ("int:", X86_64InterpreterOnlyStrategy.new());
		var dyn_mode = ("dyn:", X86_64DynamicStrategy.new());
		var lazy_mode = ("lazy:", X86_64SpcLazyStrategy.new());
		return List<(string, ExecutionStrategy)>.new(int_mode, Lists.cons3(spc_mode, dyn_mode, lazy_mode));
	}

	def setTargetCode(f: FuncDecl, addr: Pointer, end: Pointer) {
		if (Trace.compiler) {
			Trace.OUT.put2("func[%d].target_code: break *0x%x", f.func_index, addr - Pointer.NULL)
				.put2(" disass 0x%x, 0x%x", addr - Pointer.NULL, end - Pointer.NULL).ln();
			if (Trace.asm) {
				var cur_byte = addr;
				Trace.OUT.puts("JIT code: ");
				while (cur_byte < end) {
					Trace.OUT.put1("%x ", cur_byte.load<u8>());
					cur_byte++;
				}
				Trace.OUT.ln();
			}
		}
		f.target_code = TargetCode(addr);
		Debug.afterCompile(f, u64.view(addr - Pointer.NULL));
	}
	def pregenIntoFile(filename: string) -> ErrorBuilder {
		var data = System.fileLoad(filename);
		var err = ErrorBuilder.new().puts("interpreter generator: ");
		if (data == null) return err.put1("could not load executable %s\n", filename);
		var ok = X86_64PreGenStubs.genAndWriteIntoExecutable(data);
		if (ok == false) return err.put1("could not patch executable %s\n", filename);
		var fd = System.fileOpen(filename, false);
		if (fd < 0) return err.put1("could not write executable: %s\n", filename);
		System.fileWriteK(fd, data, 0, data.length);
		System.fileClose(fd);
		return null;
	}
	def mapCode(asm: X86_64Assembler, prepare: (X86_64Assembler, u64) -> void) -> Mapping {
		var w = asm.w;
		var length = w.atEnd().pos;
		var mapping = Mmap.reserve(u64.!(length), Mmap.PROT_WRITE), range = mapping.range;
		if (prepare != null) prepare(asm, u64.view(range.start - Pointer.NULL));
		fastFwCopy(range.range(0, length), w.data[0 ... length]);
		Mmap.protect(range.start, u64.!(range.end - range.start), Mmap.PROT_READ | Mmap.PROT_EXEC);
		return mapping;
	}
	def copyInto(range: MemoryRange, offset: int, w: DataWriter) -> int {
		var length = w.atEnd().pos;
		fastFwCopy(range.range(offset, length), w.data[0 ... length]);
		return offset + length;
	}
	def setTieredEntrypoint(module: Module) { // TODO: remove, move all tests/entrypoints to validation
		// With multi-tier support, the interpreter calls through target code, so every
		// function needs a proper entrypoint.
		if (FeatureDisable.multiTier) return;
		var ic = X86_64PreGenStubs.getInterpreterCode();
		var entry = ic.start + ic.header.intSpcEntryOffset;
		for (i < module.functions.length) {
			var f = module.functions[i];
			f.target_code = TargetCode(entry);
		}
	}
	def setUnconditionalInterpreterEntryIfMultiTier(f: FuncDecl) {
		// With multi-tier support, the interpreter calls through target code, so every
		// function needs a proper entrypoint.
		if (FeatureDisable.multiTier) return;
		var ic = X86_64PreGenStubs.getInterpreterCode();
		var entry = ic.start + ic.header.intSpcEntryOffset;
		f.target_code = TargetCode(entry);
	}
	def ticksNs() -> u64 {
		return u32.view(System.ticksNs());
	}
	def rdtsc() -> u64 {
		return RDTSC_FUNC.get()();
	}
	def getRdtsc() -> (void -> u64) {
		return RDTSC_FUNC.get();
	}
	def fastFwCopy(dst: Range<byte>, src: Range<byte>) { // FAST, danger: uses unsafe Virgil features
		var dptr = Pointer.atContents(dst), sptr = Pointer.atContents(src);
		var i = 0, max = dst.length & ~7;
		while (i < max) {
			(dptr + i).store<u64>((sptr + i).load<u64>());
			i += 8;
		}
		while (i < dst.length) {
			(dptr + i).store<byte>((sptr + i).load<byte>());
			i++;
		}
	}

	// wrappers around syscalls, triggers a GC on failed allocation
	def mmap_reserve(size: u64, prot: int) -> Mapping {
		for (i < MMAP_RETRIES) {
			var mapping = Mmap.reserve(size, prot);
			if (mapping != null) return mapping;
			forceGC();
		}
		return Mmap.reserve(size, prot);
	}
	def redzones_add(mapping: Mapping, offset: u32, size: u32) -> bool {
		for (i < REDZONE_RETRIES) {
			if (RedZones.addRedZone(mapping, offset, size)) return true;
			forceGC();
		}
		return RedZones.addRedZone(mapping, offset, size);
	}
}

type TargetOsrInfo(spc_entry: Pointer, osr_entries: List<(int, int)>) #unboxed { }
type TargetCode(spc_entry: Pointer) #unboxed { }
type TargetModule(spc_code: X86_64SpcModuleCode) #unboxed { }
// TODO: immutable cache for frame accessor
type TargetFrame(sp: Pointer) #unboxed {
	def getFrameAccessor() -> X86_64FrameAccessor {
		return X86_64Stacks.getFrameAccessor(sp);
	}
}
type SpcResultForStub(wf: WasmFunction, entrypoint: Pointer, thrown: Throwable) #unboxed { }
class TargetHandlerDest(is_dummy: bool) {
	var stub_label: MasmLabel;
	var dest_label: MasmLabel;

	// Full signature of the stack at dest.
	var dest_stack: Array<ValueType>;
}

class X86_64ExecutionStrategy extends ExecutionStrategy {
	// Call a function with arguments and return a result.
	def call(func: Function, args: Range<Value>) -> Result {
		return X86_64StackManager.runOnFreshStack(func, args); // XXX: specialize for different strategies?
	}
	// Compilation methods called directly by stubs.
	def lazyCompile(wf: WasmFunction) -> SpcResultForStub;
	def tierupCompile(wf: WasmFunction) -> SpcResultForStub;
	// Tiering may require setting up the whole module.
	def onTestModule(module: Module) {
		Target.setTieredEntrypoint(module);
	}
	def disableLazyNameDecodingDuringGC(module: Module) {
		if (module.names != null) module.names.lazyDecodeDisabled = RiGc.inGC;
	}
}

// One tier: fast-int, modules require no pre-processing.
class X86_64InterpreterOnlyStrategy extends X86_64ExecutionStrategy {
	def call(func: Function, args: Range<Value>) -> Result {
		return X86_64StackManager.runOnFreshStack(func, args);
	}

	def onModuleFinish(module: Module, size: u32, err: ErrorGen) {
		disableLazyNameDecodingDuringGC(module);
	}
	def onFuncValidationFinish(module: Module, func: FuncDecl, err: ErrorGen) {
		if (err != null && !err.ok()) return;
		Target.setUnconditionalInterpreterEntryIfMultiTier(func);
	}
	def onNewFunction(wf: WasmFunction, err: ErrorGen) {
		Target.setUnconditionalInterpreterEntryIfMultiTier(wf.decl);
	}

	def onFuncProbeInsert1(module: Module, func: FuncDecl, offset: int, p: Probe) {
		if (FastIntTuning.enableWhammProbeTrampoline && WhammProbe.?(p))
			X86_64WhammTrampoline.makeTrampoline(WhammProbe.!(p), X86_64PreGenStubs.getInterpreterCode());
	}
}

// Base class of all strategies that use SPC.
class X86_64SpcStrategy extends X86_64ExecutionStrategy {
	def onFuncProbeInsert1(module: Module, func: FuncDecl, offset: int, p: Probe) {
		X86_64Spc.setLazyCompileFor(module, func);
	}
	def onFuncProbeInsert2(module: Module, func: FuncDecl, offset: int, p: Probe) {
		X86_64Spc.setLazyCompileFor(module, func);
	}

	def tierupCompile(wf: WasmFunction) -> SpcResultForStub {
		return compileFunction(wf, "tierup");
	}
	def lazyCompile(wf: WasmFunction) -> SpcResultForStub {
		return compileFunction(wf, "lazy");
	}
	private def compileFunction(wf: WasmFunction, how: string) -> SpcResultForStub {
		// Check the JIT filter, if there is one
		if (!applyJitFilter(wf.instance.module, wf.decl, how)) return SpcResultForStub(wf, X86_64Spc.setInterpreterFallback(wf.decl), null);

		var module = wf.instance.module;
		var code = module.target_module.spc_code;
		var compiler = newCompiler(module.filename); // XXX: cache per-thread
		var masm = X86_64MacroAssembler.!(compiler.masm), w = masm.asm.w;

		// generate code for the function
		var success = compiler.gen(module, wf.decl, null);

		// Check for remaining code space
		var regionSize = code.mapping.range.size();
		var remaining = regionSize - u64.!(code.codeEnd);
		var codeSize = w.atEnd().pos;
		if (codeSize > remaining) {
			if (Trace.compiler) Trace.OUT.put3("exhausted code space for module (%d of %d bytes remaining, need %d)",
				remaining, regionSize, codeSize).ln();
			success = false;
		}

		var entrypoint: Pointer;
		if (success) {
			// Copy code into end of region
			entrypoint = code.appendCode(masm);
			Target.setTargetCode(wf.decl, entrypoint, entrypoint + codeSize);
		} else {
			// Failed, enter interpreter
			var f = wf.decl;
			if (Trace.compiler) Trace.OUT.put2("func[%d] %s compile failed", f.func_index, how).ln();
			entrypoint = X86_64Spc.setInterpreterFallback(f);
		}
		return SpcResultForStub(wf, entrypoint, null);
	}
	def installStubForModule(module: Module, set: (Module, FuncDecl) -> void) {
		// ensure entrypoint and lazy compile stubs are generated
		X86_64PreGenStubs.gen();
		// Set all functions to refer to the tier-up compile stub.
		var codeSize = MINIMUM_CODE_SIZE;
		for (i < module.functions.length) {
			var f = module.functions[i];
			if (f.imported()) continue;
			set(module, f);
			codeSize += X86_64Spc.estimateCodeSizeFor(f);
		}
		allocateCodeForModule(module, codeSize);
	}
}

// One tier: SPC, modules are eagerly compiled.
class X86_64SpcAotStrategy(interpreter_fallback: bool) extends X86_64SpcStrategy {
	var hasMonitors = false;

	// Called if monitors will be attached to the (forthcoming) module.
	def onMonitorsStart() {
		hasMonitors = true;
	}
	// Called after a module is parsed.
	def onModuleFinish(module: Module, size: u32, err: ErrorGen) {
		// defer compilation for AOT mode until after monitors have been installed
		if (!hasMonitors) compileEntireModule(module, size, interpreter_fallback, err, 1024);
		disableLazyNameDecodingDuringGC(module);
	}
	// Called after monitors have processed a module.
	def onMonitorsFinish(module: Module, err: ErrorGen) {
		compileEntireModule(module, 0, interpreter_fallback, err, 1024);
	}
	// Called before a test function is run.
	def onTestRun(wf: WasmFunction, err: ErrorGen) {
		var module = wf.instance.module;
		if (module.target_module.spc_code == null ||
			wf.decl.target_code.spc_entry == Pointer.NULL) {
			compileEntireModule(module, 0, interpreter_fallback, err, PAGE_SIZE); // XXX: compile individual functions in test
		}
	}
	def onNewFunction(wf: WasmFunction, err: ErrorGen) {
		Target.setUnconditionalInterpreterEntryIfMultiTier(wf.decl); // XXX: use lazy compile
	}
	def compileEntireModule(module: Module, size: u32, interpreter_fallback: bool, err: ErrorGen, ballast: u32) {
		// ensure entrypoint and lazy compile stubs are generated
		X86_64PreGenStubs.gen();

		var compiler = newCompiler(module.filename);
		var w = compiler.w;

		// generate code for all functions
		var bounds = Array<(int, int)>.new(module.functions.length);
		var suberr = if(!interpreter_fallback, err);
		for (i = 0; err.ok() && i < module.functions.length; i++) {
			var f = module.functions[i];
			if (f.imported()) continue;
			var start = w.atEnd().pos;
			var compiled = applyJitFilter(module, f, "aot") && compiler.gen(module, f, suberr);
			if (compiled) bounds[i] = (start, w.end());
			else bounds[i] = (-1, -1);
		}

		// copy and map code
		var length = u64.view(w.atEnd().pos) + ballast;
		var mapping = Mmap.reserve(length, Mmap.PROT_WRITE), range = mapping.range; // TODO: handle failure
		var masm = X86_64MacroAssembler.!(compiler.masm);
		masm.setTargetAddress(u64.view(range.start - Pointer.NULL));
		Target.copyInto(mapping.range, 0, w);
		// TODO: for security, move embedded references out of the code region and make it non-writable
		Mmap.protect(range.start, u64.!(range.end - range.start), Mmap.PROT_WRITE | Mmap.PROT_READ | Mmap.PROT_EXEC);
		for (i < bounds.length) {
			var b = bounds[i];
			if (b.0 >= 0) {
				var addr = mapping.range.start;
				var f = module.functions[i];
				Target.setTargetCode(f, addr + b.0, addr + b.1);
			} else {
				var f = module.functions[i];
				if (Trace.compiler) Trace.OUT.put1("func[%d] initial compile failed", f.func_index).ln();
				X86_64Spc.setInterpreterFallback(f);
			}
		}
		// XXX: reduce duplication with {X86_64SpcModuleCode.appendCode}.
		var code = X86_64SpcModuleCode.new(mapping);
		if (masm.source_locs != null) {
			code.sourcePcs = Vector.new();
			code.sourcePcs.putv(masm.source_locs);
		}
		if (masm.embeddedRefOffsets != null) {
			if (code.embeddedRefOffsets == null) code.embeddedRefOffsets = Vector.new();
			code.embeddedRefOffsets.putv(masm.embeddedRefOffsets);
		}

		module.target_module = TargetModule(code);
		RiRuntime.registerUserCode(code);
		module.target_module.spc_code.keepAlive();
		Debug.afterCompileModule(module);
	}
}

// One tier: SPC, functions are lazily compiled.
class X86_64SpcLazyStrategy extends X86_64SpcStrategy {
	// Called after a module is parsed.
	def onModuleFinish(module: Module, size: u32, err: ErrorGen) {
		installStubForModule(module, X86_64Spc.setLazyCompileFor);
		disableLazyNameDecodingDuringGC(module);
	}
	// Called before a test function is run.
	def onTestRun(wf: WasmFunction, err: ErrorGen) { // TODO: move this to onFuncValidationFinish
		installStubForModule(wf.instance.module, X86_64Spc.setLazyCompileFor);
	}
	def onNewFunction(wf: WasmFunction, err: ErrorGen) {
		X86_64Spc.setLazyCompileFor(wf.instance.module, wf.decl);
	}
}
// Two tiers: fast-int, frequently-executed functions are compiled with SPC.
class X86_64DynamicStrategy extends X86_64SpcStrategy {
	// Called after a module is parsed.
	def onModuleFinish(module: Module, size: u32, err: ErrorGen) {
		installStubForModule(module, X86_64Spc.setTierUpFor);
		disableLazyNameDecodingDuringGC(module);
		if (Debug.runtime) {
			for (i < module.functions.length) {
				var func = module.functions[i];
				Trace.OUT.put2(
					"Compiled function #%d: addr(target_code) = 0x%x", i,
					Pointer.atObject(func) + Target.getOffsets().FuncDecl_target_code - Pointer.NULL
				).ln();
			}
		}
	}
	// Called before a test function is run.
	def onTestRun(wf: WasmFunction, err: ErrorGen) {
		installStubForModule(wf.instance.module, X86_64Spc.setTierUpFor);
	}
	def onNewFunction(wf: WasmFunction, err: ErrorGen) {
		X86_64Spc.setTierUpFor(wf.instance.module, wf.decl);
	}
	def onTierUp(wf: WasmFunction, pc: int) -> TargetOsrInfo {
		var module = wf.instance.module;
		var compiler = newCompiler(module.filename);
		if (!applyJitFilter(wf.instance.module, wf.decl, "osr")) {
			// OSR compile suppressed
			wf.decl.tierup_trigger = int.max; // no point in trying for a while
			return TargetOsrInfo(Pointer.NULL, null);
		}
		var label = compiler.genOsr(module, wf.decl, pc, null);
		if (label == null) {
			// OSR compile failed
			wf.decl.tierup_trigger = int.max; // no point in trying for a while
			return TargetOsrInfo(Pointer.NULL, null);
		}
		var code = module.target_module.spc_code;
		var masm = X86_64MacroAssembler.!(compiler.masm);
		var codeSize = masm.w.end();
		var entrypoint = code.appendCode(masm);
		var offset = X86_64MasmLabel.!(label).label.pos;
		if (Trace.compiler) Trace.OUT.put3("func[%d].osr_entry (+%d): break *0x%x", wf.decl.func_index, pc, (entrypoint + offset) - Pointer.NULL).ln();
		if (X86_64Interpreter.inCode(wf.decl.target_code.spc_entry)) {
			// Install code into function if its entrypoint still goes to interpreter
			if (Debug.runtime) Trace.OUT.put2("Installing code [0x%x, 0x%x)", entrypoint - Pointer.NULL, entrypoint + codeSize - Pointer.NULL).ln();
			Target.setTargetCode(wf.decl, entrypoint, entrypoint + codeSize);
		}
		// Reset tierup trigger to recompile right away (e.g. other activations stuck in loop).
		wf.decl.tierup_trigger = SpcTuning.postOsrTierUpThreshold;
		return TargetOsrInfo(entrypoint, List.new((pc, offset), null));
	}
}

def newCompiler(filename: string) -> X86_64SinglePassCompiler {
	var extensions = Extension.set.all; // TODO: all extensions enabled for compilation
	var limits = Limits.new();
	var compiler = X86_64SinglePassCompiler.new(extensions, limits, X86_64MasmRegs.CONFIG);
	return compiler;
}
def MINIMUM_CODE_SIZE = PAGE_SIZE_i;
def allocateCodeForModule(module: Module, codeSize: int) {
	// Round up to the next page size.
	var codeSize = PAGE_SIZE_i * ((codeSize + PAGE_SIZE_i - 1) / PAGE_SIZE_i);
	// Allocate a read/write/execute mapping for code.
	var mapping = Mmap.reserve(u64.!(codeSize), Mmap.PROT_WRITE | Mmap.PROT_READ | Mmap.PROT_EXEC);
	var code = X86_64SpcModuleCode.new(mapping);
	module.target_module = TargetModule(code);
	RiRuntime.registerUserCode(code);
	code.keepAlive();
	if (Trace.compiler) Trace.OUT.put3("%s: reserved 0x%x ... 0x%x for spc-jit code",
		module.filename, (mapping.range.start - Pointer.NULL), (mapping.range.end - Pointer.NULL)).ln();
}

def applyJitFilter(module: Module, func: FuncDecl, how: string) -> bool {
	var filter = CompilerOptions.JIT_FILTER.get();
	if (filter == null || filter.matches(module, func)) return true;
	if (Trace.compiler) Trace.OUT.put2("func[%d] %s compile suppressed by filter", func.func_index, how).ln();
	return false;
}

def RDTSC_FUNC = X86_64PreGenFunc<void, u64>.new("rdtsc", null, genRdtsc);

def genRdtsc(ic: X86_64InterpreterCode, w: DataWriter) {
	def asm = X86_64Assemblers.create64(w);
	asm.rdtsc();
	asm.q.shl_r_i(X86_64Regs.RDX, 32);
	asm.q.or_r_r(X86_64Regs.RAX, X86_64Regs.RDX); // XXX: use V3 return register symbolic constant
	asm.ret();
}
