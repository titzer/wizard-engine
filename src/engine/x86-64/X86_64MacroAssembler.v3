// Copyright 2022 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def G = X86_64MasmRegs.toGpr, X = X86_64MasmRegs.toXmmr;
def R: X86_64Regs;
def C: X86_64Conds;
def A(ma: MasmAddr) -> X86_64Addr {
	return X86_64Addr.new(G(ma.base), null, 1, ma.offset);
}
def RT: X86_64Runtime;

class X86_64MasmLabel extends MasmLabel {
	def label: X86_64Label;

	new(create_pos: int, label) super(create_pos) {	}
}
class X86_64MacroAssembler extends MacroAssembler {
	def w: DataWriter;
	def asm = X86_64Assemblers.create64(w);
	var scratch: X86_64Gpr;
	var jump_tables: Vector<(int, Array<X86_64Label>)>;
	var offsets: V3Offsets;
	var trap_stubs: X86_64SpcTrapsStub;

	new(w, regConfig: RegConfig) super(Target.tagging, regConfig) {
		scratch = G(regConfig.scratch);
	}

	def curCodeBytes() -> u64 {
		return u64.!(w.end());
	}
	def setTargetAddress(addr: u64) {
		if (jump_tables != null) {
			for (i < jump_tables.length) {
				var t = jump_tables[i], offset = t.0, labels = t.1;
				for (j < labels.length) {
					var l = labels[j], target = addr + u64.!(l.pos);
					asm.w.at(offset + j * 8).put_b64(long.view(target));
				}
			}
			jump_tables = null;
		}
	}

	// Label operations
	def newLabel(create_pos: int) -> X86_64MasmLabel {
		return X86_64MasmLabel.new(create_pos, asm.newLabel());
	}
	def bindLabel(l: MasmLabel) {
		if (Trace.compiler) Trace.OUT.put2("    bind label (+%d) -> @%d", l.create_pos, w.end()).outln();
		var label = X86_64MasmLabel.!(l);
		asm.bind(label.label);
		label.offset = label.label.pos;
	}
	def bindLabelTo(l: MasmLabel, offset: int) {
		if (Trace.compiler) Trace.OUT.put2("    bind label (+%d) -> @%d", l.create_pos, offset).outln();
		var label = X86_64MasmLabel.!(l);
		label.offset = offset;
		label.label.pos = offset;
	}
	def recordCurSourceLoc() {
		recordSourceLoc(asm.pos());
	}
	def recordRetSourceLoc() {
		// To distinguish an entry for the return address of a call from an entry for a Wasm instruction
		// following the call, return addresses have a {-1} adjustment.
		recordSourceLoc(asm.pos() - 1);
	}

	def emit_intentional_crash() {
		recordCurSourceLoc();
		asm.invalid();
	}

	def emit_read_v3_array_r_r(kind: ValueKind, dst: Reg, array: Reg, index: Reg) {
		var a = G(array), i = G(index);
		match (kind) {
			I32 => asm.movd_r_m(G(dst), X86_64Addr.new(a, i, 4, getOffsets().Array_contents));
			F32 => asm.movss_s_m(X(dst), X86_64Addr.new(a, i, 4, getOffsets().Array_contents));
			I64, REF => asm.movq_r_m(G(dst), X86_64Addr.new(a, i, 8, getOffsets().Array_contents));
			F64 => asm.movsd_s_m(X(dst), X86_64Addr.new(a, i, 8, getOffsets().Array_contents));
			ABS, V128 => asm.movdqu_s_m(X(dst), X86_64Addr.new(a, i, 16, getOffsets().Array_contents)); // TODO: can't scale by 16
		}
	}
	def emit_bounds_check_v3_array(array: Reg, index: Reg, oob_label: MasmLabel) {
		asm.d.cmp_r_m(G(index), X86_64Addr.new(G(array), null, 1, getOffsets().Array_length));
		asm.jc_rel_far(X86_64Conds.GE, X86_64MasmLabel.!(oob_label).label);
	}
	def emit_read_v3_mem_base(dst: Reg, memobj: Reg) {
		asm.movq_r_m(G(dst), X86_64Addr.new(G(memobj), null, 1, getOffsets().X86_64Memory_start));
	}

	def emit_loadbsx_r_r_r_i(kind: ValueKind, dst: Reg, base: Reg, index: Reg, offset: u32) {
		var t = handle_large_offset(index, offset);
		recordCurSourceLoc();
		var x = if (kind == ValueKind.I64, asm.q, asm.d).movbsx_r_m(G(dst), X86_64Addr.new(G(base), t.0, 1, t.1));
	}
	def emit_loadbzx_r_r_r_i(kind: ValueKind, dst: Reg, base: Reg, index: Reg, offset: u32) {
		var t = handle_large_offset(index, offset);
		recordCurSourceLoc();
		var x = if (kind == ValueKind.I64, asm.q, asm.d).movbzx_r_m(G(dst), X86_64Addr.new(G(base), t.0, 1, t.1));
	}
	def emit_loadwsx_r_r_r_i(kind: ValueKind, dst: Reg, base: Reg, index: Reg, offset: u32) {
		var t = handle_large_offset(index, offset);
		recordCurSourceLoc();
		var x = if (kind == ValueKind.I64, asm.q, asm.d).movwsx_r_m(G(dst), X86_64Addr.new(G(base), t.0, 1, t.1));
	}
	def emit_loadwzx_r_r_r_i(kind: ValueKind, dst: Reg, base: Reg, index: Reg, offset: u32) {
		var t = handle_large_offset(index, offset);
		recordCurSourceLoc();
		var x = if (kind == ValueKind.I64, asm.q, asm.d).movwzx_r_m(G(dst), X86_64Addr.new(G(base), t.0, 1, t.1));
	}
	def emit_loaddsx_r_r_r_i(kind: ValueKind, dst: Reg, base: Reg, index: Reg, offset: u32) {
		var d = G(dst);
		var t = handle_large_offset(index, offset);
		recordCurSourceLoc();
		asm.q.movd_r_m(d, X86_64Addr.new(G(base), t.0, 1, t.1));
		asm.q.shl_r_i(d, 32);
		asm.q.sar_r_i(d, 32);
	}
	def emit_loaddzx_r_r_r_i(kind: ValueKind, dst: Reg, base: Reg, index: Reg, offset: u32) {
		var t = handle_large_offset(index, offset);
		recordCurSourceLoc();
		asm.q.movd_r_m(G(dst), X86_64Addr.new(G(base), t.0, 1, t.1));
	}
	def emit_load_r_r_r_i(kind: ValueKind, dst: Reg, base: Reg, index: Reg, offset: u32) {
		var b = G(base), t = handle_large_offset(index, offset);
		recordCurSourceLoc();
		match (kind) {
			I32 => asm.movd_r_m(G(dst), X86_64Addr.new(b, t.0, 1, t.1));
			REF, I64 => asm.movq_r_m(G(dst), X86_64Addr.new(b, t.0, 1, t.1));
			F32 => asm.movss_s_m(X(dst), X86_64Addr.new(b, t.0, 1, t.1));
			F64 => asm.movsd_s_m(X(dst), X86_64Addr.new(b, t.0, 1, t.1));
			ABS, V128 => asm.movdqu_s_m(X(dst), X86_64Addr.new(b, t.0, 1, t.1));
		}
	}

	def emit_storeb_r_r_r_i(kind: ValueKind, val: Reg, base: Reg, index: Reg, offset: u32) {
		var t = handle_large_offset(index, offset);
		recordCurSourceLoc();
		asm.q.movb_m_r(X86_64Addr.new(G(base), t.0, 1, t.1), G(val));
	}
	def emit_storew_r_r_r_i(kind: ValueKind, val: Reg, base: Reg, index: Reg, offset: u32) {
		var t = handle_large_offset(index, offset);
		recordCurSourceLoc();
		asm.q.movw_m_r(X86_64Addr.new(G(base), t.0, 1, t.1), G(val));
	}
	def emit_store_r_r_r_i(kind: ValueKind, val: Reg, base: Reg, index: Reg, offset: u32) {
		var b = G(base), t = handle_large_offset(index, offset);
		recordCurSourceLoc();
		match (kind) {
			I32 => asm.movd_m_r(X86_64Addr.new(b, t.0, 1, t.1), G(val));
			REF, I64 => asm.movq_m_r(X86_64Addr.new(b, t.0, 1, t.1), G(val));
			F32 => asm.movss_m_s(X86_64Addr.new(b, t.0, 1, t.1), X(val));
			F64 => asm.movsd_m_s(X86_64Addr.new(b, t.0, 1, t.1), X(val));
			ABS, V128 => asm.movdqu_m_s(X86_64Addr.new(b, t.0, 1, t.1), X(val));
		}
	}

	private def handle_large_offset(index: Reg, offset: u32) -> (X86_64Gpr, int) {
		var ioffset = int.view(offset), r = G(index);
		if (ioffset != offset) { // too large for signed encoding, add explicitly
			asm.movd_r_i(scratch, ioffset);
			asm.q.add_r_r(scratch, r);
			return (scratch, 0);
		}
		return (r, int.view(offset));
	}

	def emit_mov_r_r(reg: Reg, reg2: Reg) {
		var rd = G(reg);
		if (rd != null) {
			var rs = G(reg2);
			if (rs != null) asm.movq_r_r(rd, G(reg2));
			else asm.movq_r_s(rd, X(reg2)); // TODO: v128
		} else {
			var xd = X(reg), xs = X(reg2);
			if (xs != null) asm.movsd_s_s(xd, xs);
			else asm.movq_s_r(xd, G(reg2)); // TODO: v128
		}
	}
	def emit_mov_r_m(kind: ValueKind, reg: Reg, ma: MasmAddr) {
		var addr = A(ma);
		match (kind) {
			I32 => asm.movd_r_m(G(reg), addr);
			I64, REF => asm.movq_r_m(G(reg), addr);
			F32 => asm.movss_s_m(X(reg), addr);
			F64 => asm.movsd_s_m(X(reg), addr);
			ABS, V128 => asm.movdqu_s_m(X(reg), addr);
		}
	}
	def emit_mov_r_i(reg: Reg, val: int) {
		asm.movd_r_i(G(reg), val);
	}
	def emit_mov_r_l32(reg: Reg, val: int) {
		asm.movq_r_i(G(reg), val);
	}
	def emit_mov_r_f32(reg: Reg, val: u32) {
		asm.movd_r_i(scratch, int.view(val));
		asm.movd_s_r(X(reg), scratch);
	}
	def emit_mov_r_l(reg: Reg, val: long) {
		asm.movq_r_l(G(reg), long.view(val));
	}
	def emit_mov_r_d64(reg: Reg, val: u64) {
		var x = X(reg);
		if (val == 0) return void(asm.xorpd_s_s(x, x));
		asm.movq_r_l(scratch, long.view(val));
		asm.movq_s_r(X(reg), scratch);
	}
	def emit_mov_r_trap(reg: Reg, reason: TrapReason) {
		var ptr = Pointer.atObject(Execute.trapObjects[reason.tag]);
		asm.movq_r_l(G(reg), ptr - Pointer.NULL);
	}
	def emit_mov_r_abs32(reg: Reg, abs: Pointer) {
		asm.movd_r_i(G(reg), int.view(u32.!(abs - Pointer.NULL)));
	}

	def emit_mov_m_r(kind: ValueKind, ma: MasmAddr, reg: Reg) {
		var addr = A(ma);
		match (kind) {
			I32 => asm.movd_m_r(addr, G(reg));
			I64, REF => asm.movq_m_r(addr, G(reg));
			F32 => asm.movss_m_s(addr, X(reg));
			F64 => asm.movsd_m_s(addr, X(reg));
			ABS, V128 => asm.movdqu_m_s(addr, X(reg));
		}
	}
	def emit_mov_m_i(ma: MasmAddr, val: int) {
		asm.movd_m_i(A(ma), val);
	}
	def emit_mov_m_l(ma: MasmAddr, val: long) {
		var addr = A(ma);
		if (val == int.view(val)) asm.movq_m_i(addr, int.view(val));
		else {  // XXX: use constant pool?
			asm.movd_m_i(addr, int.view(val));
			var p4 = X86_64Addr.new(addr.base, addr.index, addr.scale, addr.disp + 4);
			asm.movd_m_i(p4, int.view(val >> 32));
		}
	}
	def emit_mov_m_f(ma: MasmAddr, bits: u32) {
		asm.movd_m_i(A(ma), int.view(bits));
	}
	def emit_mov_m_d(ma: MasmAddr, bits: u64) {
		emit_mov_m_l(ma, long.view(bits));
	}
	def emit_mov_m_q(ma: MasmAddr, low: u64, high: u64) {  // XXX: use constant pool?
		emit_mov_m_l(ma, long.view(low));
		emit_mov_m_l(MasmAddr(ma.base, ma.offset + 8), long.view(high));
	}
	def emit_mov_m_m(kind: ValueKind, dst: MasmAddr, src: MasmAddr) {
		match (kind) {
			I32, F32 => {
				asm.movd_r_m(scratch, A(src));
				asm.movd_m_r(A(dst), scratch);
			}
			I64, F64, REF => {
				asm.movq_r_m(scratch, A(src));
				asm.movq_m_r(A(dst), scratch);
			}
			ABS, V128 => {
				var scratch = R.XMM15; // TODO
				asm.movdqu_s_m(scratch, A(src));
				asm.movdqu_m_s(A(dst), scratch);
			}
		}
	}

	def emit_addi_r_r(reg: Reg, reg2: Reg) {
		asm.add_r_r(G(reg), G(reg2));
	}
	def emit_addi_r_i(reg: Reg, val: int) {
		asm.add_r_i(G(reg), val);
	}

	def emit_subw_r_i(reg: Reg, val: int) {
		asm.sub_r_i(G(reg), val);
	}
	def emit_subw_r_r(reg: Reg, reg2: Reg) {
		asm.sub_r_r(G(reg), G(reg2));
	}
	def emit_addw_r_i(r1: Reg, val: int) {
		asm.add_r_i(G(r1), val);
	}
	def emit_addw_r_r(r1: Reg, r2: Reg) {
		asm.add_r_r(G(r1), G(r2));
	}
	def emit_shlw_r_i(reg: Reg, imm: u6) {
		asm.shl_r_i(G(reg), imm);
	}
	def emit_shrw_r_i(reg: Reg, imm: u6) {
		asm.shr_r_i(G(reg), imm);
	}

	def emit_movq_32s_r_m(dst: X86_64Gpr, addr: X86_64Addr) -> X86_64Assembler {
		asm.movd_r_m(dst, addr);
		asm.q.shl_r_i(dst, 32);
		asm.q.sar_r_i(dst, 32);
		return asm;
	}

	def emit_i32_div_s(r2: X86_64Gpr) { // note: (r1=EAX, r2) -> EAX, kills EDX
		var r1 = R.RAX;
		var div = X86_64Label.new(), done = X86_64Label.new();
		asm.d.cmp_r_i(r2, -1);
		asm.jc_rel_near(C.NZ, div);
		asm.d.cmp_r_i(r1, 0x80000000);
		asm.jc_rel_far(C.Z, X86_64MasmLabel.!(newTrapLabel(TrapReason.DIV_UNREPRESENTABLE)).label);
		asm.d.neg_r(r1);
		asm.jmp_rel_near(done);
		asm.bind(div);
		asm.d.cdq();
		recordCurSourceLoc();
		asm.d.idiv_r(r2);
		asm.bind(done);
	}
	def emit_i32_div_u(r2: X86_64Gpr) { // note: (r1=EAX, r2) -> EAX, kills EDX
		asm.d.movd_r_i(R.RDX, 0);
		recordCurSourceLoc();
		asm.d.div_r(r2);
	}
	def emit_i32_rem_s(r2: X86_64Gpr) { // note: (r1=EAX, r2) -> EDX, kills RAX
		var r1 = R.RDX;
		var div = X86_64Label.new(), done = X86_64Label.new();
		asm.d.cmp_r_i(r2, -1);
		asm.jc_rel_near(C.NZ, div);
		asm.movd_r_i(r1, 0);
		asm.jmp_rel_near(done);
		asm.bind(div);
		asm.d.cdq();
		recordCurSourceLoc();
		asm.d.idiv_r(r2);
		asm.bind(done);
	}
	def emit_i32_rem_u(r2: X86_64Gpr) { // note: (r1=EAX, r2) -> EDX, kills RAX
		asm.movd_r_i(R.RDX, 0);
		recordCurSourceLoc();
		asm.d.div_r(r2);
	}
	def emit_i64_div_s(r2: X86_64Gpr) { // note: (r1=RAX, r2) -> RAX, kills RDX
		var div = X86_64Label.new(), done = X86_64Label.new();
		var r1 = R.RAX;
		asm.q.cmp_r_i(r2, -1);
		asm.jc_rel_near(C.NZ, div);
		asm.movq_r_i(scratch, 0x80); // XXX: use BTS
		asm.q.shl_r_i(scratch, 56);
		asm.q.cmp_r_r(r1, scratch);
		asm.jc_rel_far(C.Z, X86_64MasmLabel.!(newTrapLabel(TrapReason.DIV_UNREPRESENTABLE)).label);
		asm.q.neg_r(r1);
		asm.jmp_rel_near(done);
		asm.bind(div);
		asm.q.cqo();
		recordCurSourceLoc();
		asm.q.idiv_r(r2);
		asm.bind(done);
	}
	def emit_i64_div_u(r2: X86_64Gpr) { // note: (r1=RAX, r2) -> RAX, kills RDX
		asm.movd_r_i(R.RDX, 0);
		recordCurSourceLoc();
		asm.div_r(r2);
	}
	def emit_i64_rem_s(r2: X86_64Gpr) { // note: (r1=RAX, r2) -> RDX, kills RAX
		var r1 = R.RAX;
		var dst = R.RDX;
		var div = X86_64Label.new(), done = X86_64Label.new();
		asm.cmp_r_i(r2, -1);
		asm.jc_rel_near(C.NZ, div);
		asm.movq_r_i(dst, 0);
		asm.jmp_rel_near(done);
		asm.bind(div);
		asm.cqo();
		recordCurSourceLoc();
		asm.idiv_r(r2);
		asm.bind(done);
	}
	def emit_i64_rem_u(r2: X86_64Gpr) { // note: (r1=RAX, r2) -> RDX, kills RAX
		asm.movd_r_i(R.RDX, 0);
		recordCurSourceLoc();
		asm.div_r(r2);
	}

	def emit_binop_r_r(op: Opcode, reg: Reg, reg2: Reg) {
		unimplemented();
	}
	def emit_binop_r_m(op: Opcode, reg: Reg, ma: MasmAddr) {
		unimplemented();
	}
	def emit_binop_r_i(op: Opcode, reg: Reg, val: int) {
		unimplemented();
	}
	def emit_cmpq_r_i(cond: X86_64Cond, r1: X86_64Gpr, val: int) {
		asm.q.cmp_r_i(r1, val);
		asm.set_r(cond, r1);
		asm.q.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_r_i(cond: X86_64Cond, r1: X86_64Gpr, r2: X86_64Gpr, val: int) {
		asm.q.cmp_r_i(r2, val);
		asm.set_r(cond, r1);
		asm.d.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_r_r(cond: X86_64Cond, r1: X86_64Gpr, r2: X86_64Gpr, r3: X86_64Gpr) {
		asm.q.cmp_r_r(r2, r3);
		asm.set_r(cond, r1);
		asm.d.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_r(cond: X86_64Cond, r1: X86_64Gpr, r2: X86_64Gpr) {
		asm.q.cmp_r_r(r1, r2);
		asm.set_r(cond, r1);
		asm.q.movbzx_r_r(r1, r1);
	}
	def emit_cmpq_r_m(cond: X86_64Cond, r1: X86_64Gpr, addr: X86_64Addr) {
		asm.q.cmp_r_m(r1, addr);
		asm.set_r(cond, r1);
		asm.q.movbzx_r_r(r1, r1);
	}
	def emit_ret() {
		asm.ret();
	}
	def emit_nop() {
		asm.q.or_r_r(R.RAX, R.RAX);
	}

	def emit_i_trunc_f(op: Opcode, dst: X86_64Gpr, x1: X86_64Xmmr, xscratch: X86_64Xmmr) {
		match (op) {
			I32_TRUNC_F32_S => emit_i_trunc_f0(TRUNC_i32_f32_s, false, dst, x1, xscratch);
			I32_TRUNC_F32_U => emit_i_trunc_f0(TRUNC_i32_f32_u, false, dst, x1, xscratch);
			I32_TRUNC_F64_S => emit_i_trunc_f0(TRUNC_i32_f64_s, false, dst, x1, xscratch);
			I32_TRUNC_F64_U => emit_i_trunc_f0(TRUNC_i32_f64_u, false, dst, x1, xscratch);
			I64_TRUNC_F32_S => emit_i_trunc_f0(TRUNC_i64_f32_s, false, dst, x1, xscratch);
			I64_TRUNC_F32_U => emit_i_trunc_f0(TRUNC_i64_f32_u, false, dst, x1, xscratch);
			I64_TRUNC_F64_S => emit_i_trunc_f0(TRUNC_i64_f64_s, false, dst, x1, xscratch);
			I64_TRUNC_F64_U => emit_i_trunc_f0(TRUNC_i64_f64_u, false, dst, x1, xscratch);
			I32_TRUNC_SAT_F32_S => emit_i_trunc_f0(TRUNC_i32_f32_s, true, dst, x1, xscratch);
			I32_TRUNC_SAT_F32_U => emit_i_trunc_f0(TRUNC_i32_f32_u, true, dst, x1, xscratch);
			I32_TRUNC_SAT_F64_S => emit_i_trunc_f0(TRUNC_i32_f64_s, true, dst, x1, xscratch);
			I32_TRUNC_SAT_F64_U => emit_i_trunc_f0(TRUNC_i32_f64_u, true, dst, x1, xscratch);
			I64_TRUNC_SAT_F32_S => emit_i64_trunc_sat_f32_s(dst, x1, xscratch); // custom
			I64_TRUNC_SAT_F32_U => emit_i_trunc_f0(TRUNC_i64_f32_u, true, dst, x1, xscratch);
			I64_TRUNC_SAT_F64_S => emit_i64_trunc_sat_f64_s(dst, x1, xscratch); // custom
			I64_TRUNC_SAT_F64_U => emit_i_trunc_f0(TRUNC_i64_f64_u, true, dst, x1, xscratch);
			_ => unimplemented();
		}
	}
	private def emit_i64_trunc_sat_f32_s(dst: X86_64Gpr, x1: X86_64Xmmr, xscratch: X86_64Xmmr) {
		asm.movd_r_i(dst, int.view(Floats.f_1p63));
		asm.movd_s_r(xscratch, dst);
		asm.ucomiss_s_s(x1, xscratch);
		var is_nan = X86_64Label.new(), ovf_pos = X86_64Label.new(), done = X86_64Label.new();
		asm.jc_rel_near(C.P, is_nan);
		asm.jc_rel_near(C.NC, ovf_pos);
		asm.roundss_s_s(x1, x1, X86_64Rounding.TO_ZERO);
		asm.q.cvtss2si_r_s(dst, x1);
		asm.jmp_rel_near(done);
		asm.bind(is_nan);
		asm.movd_r_i(dst, 0);
		asm.jmp_rel_near(done);
		asm.bind(ovf_pos);
		asm.movq_r_i(dst, 0xFFFFFFFE);  // TODO: tricky constant
		asm.q.ror_r_i(dst, 1); // result = 0x7FFFFFFF_FFFFFFFF
		asm.bind(done);
	}
	private def emit_i64_trunc_sat_f64_s(dst: X86_64Gpr, x1: X86_64Xmmr, xscratch: X86_64Xmmr) {
		asm.movd_r_i(dst, int.view(Floats.d_1p63 >> 32));
		asm.q.shl_r_i(dst, 32);
		asm.movq_s_r(xscratch, dst);
		asm.ucomisd_s_s(x1, xscratch);
		var is_nan = X86_64Label.new(), ovf_pos = X86_64Label.new(), done = X86_64Label.new();
		asm.jc_rel_near(C.P, is_nan);
		asm.jc_rel_near(C.NC, ovf_pos);
		asm.roundsd_s_s(x1, x1, X86_64Rounding.TO_ZERO);
		asm.q.cvtsd2si_r_s(dst, x1);
		asm.jmp_rel_near(done);
		asm.bind(is_nan);
		asm.movd_r_i(dst, 0);
		asm.jmp_rel_near(done);
		asm.bind(ovf_pos);
		asm.movq_r_i(dst, 0xFFFFFFFE); // TODO: tricky constant
		asm.q.ror_r_i(dst, 1); // result = 0x7FFFFFFF_FFFFFFFF
		asm.bind(done);
	}
	private def emit_i_trunc_f0(config: FloatTrunc, saturate: bool, dst: X86_64Gpr, x1: X86_64Xmmr, xscratch: X86_64Xmmr) {
		config.mov_s_i(asm, xscratch, config.maxv, scratch);
		config.ucomi_s_s(asm, x1, xscratch);
		var trap = if(!saturate, X86_64MasmLabel.!(newTrapLabel(TrapReason.FLOAT_UNREPRESENTABLE)).label);
		var above = X86_64Label.new(), is_nan = X86_64Label.new(), below = X86_64Label.new();
		var done = X86_64Label.new();
		if (saturate) asm.jc_rel_near(C.P, is_nan);
		else asm.jc_rel_far(C.P, trap);
		if (saturate) asm.jc_rel_near(C.NC, above);
		else asm.jc_rel_far(C.NC, trap);
		var not_big = X86_64Label.new();

		if (config.isI64 && !config.isSigned) {
			// handle u64 convert of 1p63 < v <= 1p64
			config.mov_s_i(asm, xscratch, if(config.isF64, Floats.d_1p63, Floats.f_1p63), scratch);
			config.ucomi_s_s(asm, x1, xscratch);
			asm.jc_rel_near(C.C, not_big);
			config.sub_s_s(asm, x1, xscratch);
			config.round_s_s(asm, x1, x1, X86_64Rounding.TO_ZERO);
			config.cvt2si_r_s(asm.q, dst, x1);
			asm.movd_r_i(scratch, 1);
			asm.ror_r_i(scratch, 1);
			asm.q.add_r_r(dst, scratch);
			asm.jmp_rel_near(done);
		}
		asm.bind(not_big);

		if (!saturate || config.isI64 || !config.isSigned) {
			config.mov_s_i(asm, xscratch, config.minv, scratch);
			config.ucomi_s_s(asm, x1, xscratch);
			if (saturate) asm.jc_rel_near(C.NA, below); // v <= min
			else asm.jc_rel_far(C.NA, trap); // v <= min
		}

		config.round_s_s(asm, x1, x1, X86_64Rounding.TO_ZERO);
		if (!config.isI64 && config.isSigned) {
			config.cvt2si_r_s(asm.d, dst, x1);
		} else {
			config.cvt2si_r_s(asm.q, dst, x1);
		}
		if (saturate) {
			asm.jmp_rel_near(done);
			asm.bind(above);
			config.mov_r_i(asm, dst, config.ceilv);
			asm.jmp_rel_near(done);
			asm.bind(is_nan);
			asm.bind(below);
			asm.movd_r_i(dst, 0);
		}
		asm.bind(done);
	}
	def emit_f32_convert_i64_u(x1: X86_64Xmmr, r1: X86_64Gpr, xscratch: X86_64Xmmr, scratch: X86_64Gpr) {
		asm.q.cvtsi2ss_s_r(x1, r1);
		asm.q.cmp_r_i(r1, 0);
		var done = X86_64Label.new();
		asm.jc_rel_near(C.NS, done);
		// input < 0, compute 2.0d * cvt((x >> 1) | (x&1))
		asm.movq_r_r(scratch, r1);
		asm.q.and_r_i(scratch, 1);
		asm.q.shr_r_i(r1, 1);
		asm.q.or_r_r(r1, scratch);
		asm.q.cvtsi2ss_s_r(x1, r1);
		asm.movd_r_i(scratch, int.view(Floats.f_1p1)); // XXX: const could be in memory
		asm.movd_s_r(xscratch, scratch);
		asm.mulss_s_s(x1, xscratch);
		// done
		asm.bind(done);
	}
	def emit_f64_convert_i64_u(x1: X86_64Xmmr, r1: X86_64Gpr, xscratch: X86_64Xmmr, scratch: X86_64Gpr) {
		asm.q.cvtsi2sd_s_r(x1, r1);
		asm.q.cmp_r_i(r1, 0);
		var done = X86_64Label.new();
		asm.jc_rel_near(C.NS, done);
		// input < 0, compute 2.0d * cvt((x >> 1) | (x&1))
		asm.movq_r_r(scratch, r1);
		asm.q.and_r_i(scratch, 1);
		asm.q.shr_r_i(r1, 1);
		asm.q.or_r_r(r1, scratch);
		asm.q.cvtsi2sd_s_r(x1, r1);
		asm.movd_r_i(scratch, int.view(Floats.d_1p1 >> 32));
		asm.q.shl_r_i(scratch, 32);
		asm.movq_s_r(xscratch, scratch);
		asm.mulsd_s_s(x1, xscratch);
		// done
		asm.bind(done);
	}
	def emit_f32_copysign(x1: X86_64Xmmr, x2: X86_64Xmmr, scratch1: X86_64Gpr, scratch2: X86_64Gpr) {
		asm.movd_r_s(scratch2, x2);
		asm.d.and_r_i(scratch2, 0x80000000);
		asm.movd_r_s(scratch1, x1);
		asm.d.btr_r_i(scratch1, 31);
		asm.d.or_r_r(scratch1, scratch2);
		asm.movd_s_r(x1, scratch1);
	}
	def emit_f64_copysign(x1: X86_64Xmmr, x2: X86_64Xmmr, scratch1: X86_64Gpr, scratch2: X86_64Gpr) {
		asm.movq_r_s(scratch2, x2);
		asm.q.shr_r_i(scratch2, 63); // XXX: use shl, rcl, rcr?
		asm.q.shl_r_i(scratch2, 63);
		asm.movq_r_s(scratch1, x1);
		asm.q.btr_r_i(scratch1, 63);
		asm.q.or_r_r(scratch1, scratch2);
		asm.movq_s_r(x1, scratch1);
	}

	def emit_br(label: MasmLabel) {
		asm.jmp_rel_far(X86_64MasmLabel.!(label).label);
	}
	def emit_br_r(reg: Reg, cond: MasmBrCond, label: MasmLabel) {
		(if(cond.i32, asm.d, asm.q)).cmp_r_i(G(reg), 0);
		var cond = if(cond.zero, X86_64Conds.Z, X86_64Conds.NZ);
		asm.jc_rel_far(cond, X86_64MasmLabel.!(label).label);
	}
	def emit_br_m(addr: MasmAddr, cond: MasmBrCond, label: MasmLabel) {
		(if(cond.i32, asm.d, asm.q)).cmp_m_i(A(addr), 0);
		var cond = if(cond.zero, X86_64Conds.Z, X86_64Conds.NZ);
		asm.jc_rel_far(cond, X86_64MasmLabel.!(label).label);
	}
	def emit_breq_r_i(r: Reg, val: int, label: MasmLabel) {
		asm.d.cmp_r_i(G(r), val);
		asm.jc_rel_far(X86_64Conds.Z, X86_64MasmLabel.!(label).label);
	}
	def emit_breq_r_l(r: Reg, val: int, label: MasmLabel) {
		asm.q.cmp_r_i(G(r), val);
		asm.jc_rel_far(X86_64Conds.Z, X86_64MasmLabel.!(label).label);
	}
	def emit_brne_r_i(r: Reg, val: int, label: MasmLabel) {
		asm.d.cmp_r_i(G(r), val);
		asm.jc_rel_far(X86_64Conds.NZ, X86_64MasmLabel.!(label).label);
	}
	def emit_br_table_r(reg: Reg, labels: Array<MasmLabel>) {
		// XXX: simplify the label patching logic by improving X86_64Assembler
		var r1 = G(reg);
		asm.d.cmp_r_i(r1, labels.length);
		asm.jc_rel_far(C.NC, X86_64MasmLabel.!(labels[labels.length - 1]).label);
		var patcher = X86_64MasmJumpTablePatcher.new();
		asm.q.patcher = patcher;
		asm.q.lea(scratch, X86_64Addr.new(null, null, 1, REL_MARKER));
		asm.q.patcher = null;
		asm.ijmp_m(X86_64Addr.new(scratch, r1, 8, 0));
		w.align(8);
		var jtpos = w.atEnd().pos;
		if (jump_tables == null) jump_tables = Vector.new();
		jump_tables.put(jtpos, Arrays.map(labels, getLabel));
		w.skipN(labels.length * 8);
		w.at(patcher.pos).put_b32(jtpos - (patcher.pos + patcher.delta));
		w.atEnd();
	}
	def emit_call_r(reg: Reg) {
		asm.icall_r(G(reg));
		recordRetSourceLoc();
	}
	def emit_jump_r(reg: Reg) {
		asm.ijmp_r(G(reg));
	}
	def emit_call_runtime_callHost(func_arg: Reg) {
		emit_call_runtime(RT.runtime_callHost);
	}
	def emit_call_runtime_TRAP() {
		emit_call_runtime(RT.runtime_TRAP);
	}
	def emit_jump_runtime_callHost(func_arg: Reg) {
		emit_jump_runtime(RT.runtime_callHost);
	}
	def emit_jump_to_trap_at(reason: TrapReason) {
		var ip = trap_stubs.getIpForReason(reason);
		asm.movq_r_l(scratch, ip - Pointer.NULL);
		asm.ijmp_r(scratch);
	}
	def emit_call_runtime_op(op: Opcode) {
		match (op) {
			GLOBAL_GET => emit_call_runtime(RT.runtime_GLOBAL_GET);
			GLOBAL_SET => emit_call_runtime(RT.runtime_GLOBAL_SET);
			TABLE_GET => emit_call_runtime(RT.runtime_TABLE_GET);
			TABLE_SET => emit_call_runtime(RT.runtime_TABLE_SET);
			MEMORY_GROW => emit_call_runtime(RT.runtime_MEMORY_GROW);
			MEMORY_INIT => emit_call_runtime(RT.runtime_MEMORY_INIT);
			MEMORY_COPY => emit_call_runtime(RT.runtime_MEMORY_COPY);
			MEMORY_FILL => emit_call_runtime(RT.runtime_MEMORY_FILL);
			TABLE_INIT => emit_call_runtime(RT.runtime_TABLE_INIT);
			TABLE_COPY => emit_call_runtime(RT.runtime_TABLE_COPY);
			TABLE_GROW => emit_call_runtime(RT.runtime_TABLE_GROW);
			TABLE_FILL => emit_call_runtime(RT.runtime_TABLE_FILL);
			_ => unimplemented();
		}
	}
	def emit_call_runtime_Probe_instr() {
		emit_call_runtime(RT.runtime_PROBE_instr);
	}
	private def emit_call_runtime<P, R>(closure: P -> R) {
		var ptr = CiRuntime.unpackClosure<X86_64Interpreter, P, R>(closure).0;
		// Do an absolute call into the runtime
		asm.movd_r_i(scratch, int.view(u32.!(ptr - Pointer.NULL))); // XXX: make direct call to runtime if within 2GB
		asm.icall_r(scratch);
		recordRetSourceLoc();
	}
	private def emit_jump_runtime<P, R>(closure: P -> R) {
		var ptr = CiRuntime.unpackClosure<X86_64Interpreter, P, R>(closure).0;
		// Do an absolute call into the runtime
		asm.movd_r_i(scratch, int.view(u32.!(ptr - Pointer.NULL))); // XXX: make direct jump to runtime if within 2GB
		asm.ijmp_r(scratch);
	}
	def emit_store_runtime_vsp(vsp: Reg) {
		var offsets = getOffsets();
		asm.movq_r_m(scratch, absPointer(offsets.Interpreter_valueStack));
		asm.movq_m_r(scratch.plus(offsets.ValueStack_sp), G(vsp));
	}
	def emit_load_runtime_vsp(vsp: Reg) {
		var offsets = getOffsets();
		asm.movq_r_m(scratch, absPointer(offsets.Interpreter_valueStack));
		asm.movq_r_m(G(vsp), scratch.plus(offsets.ValueStack_sp));
	}
	def emit_load_dispatch_table_reg(reg: Reg) {
		var offsets = getOffsets();
		asm.movq_r_m(G(reg), absPointer(offsets.Interpreter_dispatchTable));
	}
	def emit_i32_clz_r_r(r: X86_64Gpr, s: X86_64Gpr) {
		asm.movd_r_i(scratch, -1);
		asm.d.bsr_r_r(r, s);
		asm.d.cmov_r(C.Z, r, scratch);
		asm.movd_r_i(scratch, 31);
		asm.d.sub_r_r(scratch, r);
		asm.movd_r_r(r, scratch); // XXX: can save an instruction here?
	}
	def emit_i32_ctz_r_r(r: X86_64Gpr, s: X86_64Gpr) {
		asm.d.bsf_r_r(r, s);
		asm.movd_r_i(scratch, 32);
		asm.d.cmov_r(C.Z, r, scratch);
	}
	def emit_i64_clz_r_r(r: X86_64Gpr, s: X86_64Gpr) {
		asm.movq_r_i(scratch, -1);
		asm.q.bsr_r_r(r, s);
		asm.q.cmov_r(C.Z, r, scratch);
		asm.movq_r_i(scratch, 63);
		asm.q.sub_r_r(scratch, r);
		asm.movq_r_r(r, scratch); // XXX: can save an instruction with second output reg
	}
	def emit_i64_ctz_r_r(r: X86_64Gpr, s: X86_64Gpr) {
		asm.q.bsf_r_r(r, s);
		asm.movq_r_i(scratch, 64);
		asm.q.cmov_r(C.Z, r, scratch);
	}
	def emit_i64_extend_i32_s(r: X86_64Gpr) {
		asm.q.shl_r_i(r, 32);
		asm.q.sar_r_i(r, 32);
	}
	def emit_i64_extend_i32_u(r: X86_64Gpr) {
		asm.movd_r_r(r, r);
	}
	// SSE assemblers and helpers
	// Masks for simd instructions
	var mask_i8x16_splat_0x0f: (long, long) = (0x0F0F0F0F0F0F0F0F, 0x0F0F0F0F0F0F0F0F);
	var mask_i8x16_popcnt: (long, long) = (0x0302020102010100, 0x0403030203020201);
	var mask_v128_float_neg_constant: (long, long) = (0x8000000080000000, 0x8000000080000000);
	var mask_v128_double_neg_constant: (long, long) = (0x8000000000000000, 0x8000000000000000);
	var mask_v128_float_absolute_constant: (long, long) = (0x7FFFFFFF7FFFFFFF, 0x7FFFFFFF7FFFFFFF);
	var mask_v128_double_absolute_constant: (long, long) = (0x7FFFFFFFFFFFFFFF, 0x7FFFFFFFFFFFFFFF);
	// Load a mask into an Xmm register s
	def load_v128_mask(dst: X86_64Xmmr, mask: (long, long), tmp: X86_64Gpr, xtmp: X86_64Xmmr) {
		// move 64 bit wide general purpose register into an xmm's lower half
		asm.movq_r_l(tmp, mask.0);
		asm.movq_s_r(dst, tmp);
		asm.movq_r_l(tmp, mask.1);
		asm.movq_s_r(xtmp, tmp);
		// interleave low-order quadword from xmm1 and xmm2 into xmm1 register
		asm.punpcklqdq_s_s(dst, xtmp);
	}
	def emit_v128_orps(dst: X86_64Xmmr, src: X86_64Xmmr) {
		asm.orps_s_s(dst, src);
	}
	def emit_v128_xorps(dst: X86_64Xmmr, src: X86_64Xmmr) {
		asm.xorps_s_s(dst, src);
	}
	def emit_v128_andps(dst: X86_64Xmmr, src: X86_64Xmmr) {
		asm.andps_s_s(dst, src);
	}
	def emit_v128_negps(dst: X86_64Xmmr, tmp: X86_64Gpr, xtmp: X86_64Xmmr, mask: X86_64Xmmr) {
		load_v128_mask(mask, mask_v128_float_neg_constant, tmp, xtmp);
		emit_v128_xorps(dst, mask);
	}
	def emit_v128_negpd(dst: X86_64Xmmr, tmp: X86_64Gpr, xtmp: X86_64Xmmr, mask: X86_64Xmmr) {
		load_v128_mask(mask, mask_v128_double_neg_constant, tmp, xtmp);
		emit_v128_xorps(dst, mask);
	}
	def emit_v128_absps(dst: X86_64Xmmr, tmp: X86_64Gpr, xtmp: X86_64Xmmr, mask: X86_64Xmmr) {
		load_v128_mask(mask, mask_v128_float_absolute_constant, tmp, xtmp);
		emit_v128_andps(dst, mask);
	}
	def emit_v128_abspd(dst: X86_64Xmmr, tmp: X86_64Gpr, xtmp: X86_64Xmmr, mask: X86_64Xmmr) {
		load_v128_mask(mask, mask_v128_double_absolute_constant, tmp, xtmp);
		emit_v128_andps(dst, mask);
	}
	def emit_v128_zero(s: X86_64Xmmr) {
		emit_v128_xorps(s, s);
	}
	def emit_i64x2_ge_s(s0: X86_64Xmmr, s1: X86_64Xmmr, scratch: X86_64Xmmr) {
		asm.pcmpgtq_s_s(s1, s0);
		asm.pcmpeqd_s_s(scratch, scratch);
		emit_v128_xorps(s1, scratch); // The result is stored in s1
	}
	def emit_i16x8_extmul_low(dst: X86_64Xmmr, src: X86_64Xmmr, scratch: X86_64Xmmr, is_signed: bool) {
		if (is_signed) {
			asm.pmovsxbw_s_s(scratch, src);
			asm.pmovsxbw_s_s(dst, dst);
		} else {
			asm.pmovzxbw_s_s(scratch, src);
			asm.pmovzxbw_s_s(dst, dst);
		}
		asm.pmullw_s_s(dst, scratch);
	}
	def emit_i16x8_extmul_high_s(dst: X86_64Xmmr, src: X86_64Xmmr, scratch: X86_64Xmmr) {
		asm.movaps_s_s(scratch, src);
		asm.punpckhbw_s_s(dst, dst);
		asm.psraw_i(dst, 8);
		asm.punpckhbw_s_s(scratch, scratch);
		asm.psraw_i(scratch, 8);
		asm.pmullw_s_s(dst, scratch);
	}
	def emit_i16x8_extmul_high_u(dst: X86_64Xmmr, src: X86_64Xmmr, scratch: X86_64Xmmr) {
		asm.xorps_s_s(scratch, scratch);
		asm.punpckhbw_s_s(dst, scratch);
		asm.punpckhbw_s_s(scratch, src);
		asm.psrlw_i(scratch, 8);
		asm.pmullw_s_s(dst, scratch);
	}
	def emit_i32x4_extmul(dst: X86_64Xmmr, src: X86_64Xmmr, scratch: X86_64Xmmr, low: bool, is_signed: bool) {
		asm.movaps_s_s(scratch, dst);
		asm.pmullw_s_s(dst, src);
		if (is_signed) {
			asm.pmulhw_s_s(scratch, src);
		} else {
			asm.pmulhuw_s_s(scratch, src);
		}
		if (low) {
			asm.punpcklwd_s_s(dst, scratch);
		} else {
			asm.punpckhwd_s_s(dst, scratch);
		}
	}
	def emit_i64x2_extmul(dst: X86_64Xmmr, src: X86_64Xmmr, scratch: X86_64Xmmr, low: bool, is_signed: bool) {
		var mask: u8 = if(low, 0x50, 0xFA);
		asm.pshufd_s_s_imm(scratch, src, mask);
		asm.pshufd_s_s_imm(dst, dst, mask);
		if (is_signed) {
			asm.pmuldq_s_s(dst, scratch);
		} else {
			asm.pmuludq_s_s(dst, scratch);
		}
	}
	def emit_i8x16_popcnt(dst: X86_64Xmmr, tmp1: X86_64Gpr, xtmp1: X86_64Xmmr, xtmp2: X86_64Xmmr, mask: X86_64Xmmr) {
		// load masks
		load_v128_mask(xtmp1, mask_i8x16_splat_0x0f, tmp1, xtmp2);
		load_v128_mask(mask, mask_i8x16_popcnt, tmp1, xtmp2);
		asm.movaps_s_s(xtmp2, xtmp1);
		// prepare temp variables
		asm.andps_s_s(xtmp1, dst);
		asm.andnps_s_s(xtmp2, dst);
		asm.psrlw_i(xtmp2, 4);
		// copy popcnt_mask to dst
		asm.movaps_s_s(dst, mask);
		asm.pshufb_s_s(dst, xtmp1);
		asm.movaps_s_s(xtmp1, mask);
		asm.pshufb_s_s(xtmp1, xtmp2);
		asm.paddb_s_s(dst, xtmp1);
	}
	def emit_f32x4_min(dst: X86_64Xmmr, src: X86_64Xmmr, scratch: X86_64Xmmr) {
		// The minps instruction doesn't propagate NaNs and +0's in its first
  		// operand. Perform minps in both orders, merge the results, and adjust.
		asm.movaps_s_s(scratch, src);
		asm.minps_s_s(scratch, dst);
		asm.minps_s_s(dst, src);
		// Propagate -0's and NaNs, which may be non-canonical.
		asm.orps_s_s(scratch, dst);
		// Canonicalize NaNs by quieting and clearing the payload.
		asm.cmpunordps_s_s(dst, scratch);
		asm.orps_s_s(scratch, dst);
		asm.psrld_i(dst, 10);
		asm.andnps_s_s(dst, scratch);
	}
	def emit_f32x4_max(dst: X86_64Xmmr, src: X86_64Xmmr, scratch: X86_64Xmmr) {
		// The maxps instruction doesn't propagate NaNs and +0's in its first
		// operand. Perform maxps in both orders, merge the results, and adjust.
		asm.movaps_s_s(scratch, src);
		asm.maxps_s_s(scratch, dst);
		asm.maxps_s_s(dst, src);
		// Find discrepancies.
		asm.xorps_s_s(dst, scratch);
		// Propagate -0's and NaNs, which may be non-canonical.
		asm.orps_s_s(scratch, dst);
		// Propagate sign discrepancy and (subtle) quiet NaNs.
		asm.subps_s_s(scratch, dst);
		// Canonicalize NaNs by quieting and clearing the payload.
		asm.cmpunordps_s_s(dst, scratch);
		asm.psrld_i(dst, 10);
		asm.andnps_s_s(dst, scratch);
	}
	def emit_f64x2_min(dst: X86_64Xmmr, src: X86_64Xmmr, scratch: X86_64Xmmr) {
		asm.movaps_s_s(scratch, src);
		asm.minpd_s_s(scratch, dst);
		asm.minpd_s_s(dst, src);
		asm.orpd_s_s(scratch, dst);
		asm.cmpunordpd_s_s(dst, scratch);
		asm.orpd_s_s(scratch, dst);
		asm.psrlq_i(dst, 13);
		asm.andnpd_s_s(dst, scratch);
	}
	def emit_f64x2_max(dst: X86_64Xmmr, src: X86_64Xmmr, scratch: X86_64Xmmr) {
		asm.movaps_s_s(scratch, src);
		asm.maxpd_s_s(scratch, dst);
		asm.maxpd_s_s(dst, src);
		asm.xorpd_s_s(dst, scratch);
		asm.orpd_s_s(scratch, dst);
		asm.subpd_s_s(scratch, dst);
		asm.cmpunordpd_s_s(dst, scratch);
		asm.psrlq_i(dst, 13);
		asm.andnpd_s_s(dst, scratch);
	}
	def getLabel(m: MasmLabel) -> X86_64Label {
		return X86_64MasmLabel.!(m).label;
	}
	def absPointer(ptr: Pointer) -> X86_64Addr {
		return X86_64Addr.new(null, null, 1, int.view(u32.!(ptr - Pointer.NULL)));
	}
	def getOffsets() -> V3Offsets {
		if (offsets == null) offsets = V3Offsets.new();
		return offsets;
	}
}

// XXX: Simplify relative loads for jump table by improving X86_64Assembler
def ABS_MARKER = 0x77665544;
def REL_MARKER = 0x99887766;
class X86_64MasmJumpTablePatcher extends X86_64AddrPatcher {
	var pos: int;
	var delta: int;
	new() super(ABS_MARKER, REL_MARKER) { }
	def recordRel32(pos: int, delta: int, addr: X86_64Addr) {
		this.pos = pos;
		this.delta = delta;
	}
}

// A utility that generates constants and picks appropriate instructions for rounding, data movement,
// and conversion in dealing with floating point truncations.
class FloatTrunc(isI64: bool, isF64: bool, isSigned: bool) {
	def round_s_s = if(isF64, X86_64Assembler.roundsd_s_s, X86_64Assembler.roundss_s_s);
	def sub_s_s = if(isF64, X86_64Assembler.subsd_s_s, X86_64Assembler.subss_s_s);
	def ucomi_s_s = if(isF64, X86_64Assembler.ucomisd_s_s, X86_64Assembler.ucomiss_s_s);
	def mov_s_r = if(isF64, X86_64Assembler.movq_s_r, X86_64Assembler.movd_s_r);
	def mov_m_s = if(isF64, X86_64Assembler.movsd_m_s, X86_64Assembler.movss_m_s);
	def maxv: u64 = if(isI64,
				if(isSigned,
					if(isF64, Floats.d_1p63, Floats.f_1p63),
					if(isF64, Floats.d_1p64, Floats.f_1p64)),
				if(isSigned,
					if(isF64, Floats.d_1p31, Floats.f_1p31),
					if(isF64, Floats.d_1p32, Floats.f_1p32)));
	def minv: u64 = if(isI64, // XXX: share these constants with V3 interpreter
				if(isSigned,
					if(isF64, u64.view(-9.223372036854778E18d), u32.view(-9.223373e18f)),
					if(isF64, u64.view(-1d), u32.view(-1f))),
				if(isSigned,
					if(isF64, u64.view(-2147483649d), u32.view(-2.1474839E9f)),
					if(isF64, u64.view(-1d), u32.view(-1f))));

	def minus1: u64 = if(isF64, Floats.d_minus1, Floats.f_minus1);

	def ceilv: u64 = if(isI64,
				if(isSigned, u63.max, u64.max),
				if(isSigned, u31.max, u32.max));
	def floorv: u64 = if(isSigned,
				if(isI64, u64.view(i63.min), u64.view(i31.min)));

	def tag = if(isI64, BpTypeCode.I64, BpTypeCode.I32).code;

	def mov_s_i(asm: X86_64Assembler, s: X86_64Xmmr, v: u64, scratch: X86_64Gpr) {
		if (isF64) {
			asm.movq_r_l(scratch, i64.view(v));
			asm.movq_s_r(s, scratch);
		} else {
			asm.movd_r_i(scratch, int.view(v));
			asm.movd_s_r(s, scratch);
		}
	}
	def mov_r_i(asm: X86_64Assembler, r: X86_64Gpr, v: u64) {
		if (isI64) {
			if (i32.view(v) == i64.view(v)) {
				asm.movq_r_i(r, int.view(v));
			} else if (u32.view(v) == u64.view(v)) {
				asm.movd_r_i(r, int.view(v));
			} else {
				System.error("FloatTrunc", "tricky 64-bit constant unimplemented");
			}
		} else {
			asm.movd_r_i(r, int.view(v));
		}
	}
	def cvt2si_r_s = if(isF64, X86_64Assembler.cvtsd2si_r_s, X86_64Assembler.cvtss2si_r_s);
}
def TRUNC_i32_f32_s = FloatTrunc.new(false, false, true);
def TRUNC_i32_f32_u = FloatTrunc.new(false, false, false);
def TRUNC_i32_f64_s = FloatTrunc.new(false, true, true);
def TRUNC_i32_f64_u = FloatTrunc.new(false, true, false);
def TRUNC_i64_f32_s = FloatTrunc.new(true, false, true);
def TRUNC_i64_f32_u = FloatTrunc.new(true, false, false);
def TRUNC_i64_f64_s = FloatTrunc.new(true, true, true);
def TRUNC_i64_f64_u = FloatTrunc.new(true, true, false);
