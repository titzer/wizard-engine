// Copyright 2022 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def R: X86_64Regs2;
def G = R.toGpr, X = R.toXmmr;
def A(ma: MasmAddr) -> X86_64Addr {
	return X86_64Addr.new(G(ma.base), null, 1, ma.offset);
}

class X86_64MacroAssembler extends MacroAssembler {
	def w: DataWriter;
	def asm = X86_64Assemblers.create64(w);

	new(config: RegConfig, w) super(Target.tagging, X86_64Regs2.SET, config) { }

	def emit_mov_r_r(reg: Reg, reg2: Reg) {
		var rd = G(reg);
		if (rd != null) asm.movq_r_r(rd, G(reg2));
		else asm.movsd_s_s(X(reg), X(reg2)); // TODO: v128
	}
	def emit_mov_r_m(reg: Reg, regClass: RegClass, ma: MasmAddr) {
		var addr = A(ma);
		match (regClass) {
			I32 => asm.movd_r_m(G(reg), addr);
			I64, REF => asm.movq_r_m(G(reg), addr);
			F32 => asm.movss_s_m(X(reg), addr);
			F64 => asm.movsd_s_m(X(reg), addr);
			V128 => asm.movdqu_s_m(X(reg), addr);
		}
	}
	def emit_mov_r_i(reg: Reg, val: int) {
		asm.movd_r_i(R.toGpr(reg), val);
	}

	def emit_mov_m_r(ma: MasmAddr, reg: Reg, regClass: RegClass) {
		var addr = A(ma);
		match (regClass) {
			I32 => asm.movd_m_r(addr, G(reg));
			I64, REF => asm.movq_m_r(addr, G(reg));
			F32 => asm.movss_m_s(addr, X(reg));
			F64 => asm.movsd_m_s(addr, X(reg));
			V128 => asm.movdqu_m_s(addr, X(reg));
		}
	}
	def emit_mov_m_i(ma: MasmAddr, val: int) {
		asm.movd_m_i(A(ma), val);
	}
	def emit_mov_m_l(ma: MasmAddr, val: long) {
		var addr = A(ma);
		if (val == int.view(val)) asm.movq_m_i(addr, int.view(val));
		else {  // XXX: use constant pool?
			asm.movd_m_i(addr, int.view(val));
			var p4 = X86_64Addr.new(addr.base, addr.index, addr.scale, addr.disp + 4);
			asm.movd_m_i(p4, int.view(val >> 32));
		}
	}
	def emit_mov_m_f(ma: MasmAddr, bits: u32) {
		asm.movd_m_i(A(ma), int.view(bits));
	}
	def emit_mov_m_d(ma: MasmAddr, bits: u64) {
		emit_mov_m_l(ma, long.view(bits));
	}
	def emit_mov_m_q(ma: MasmAddr, low: u64, high: u64) {  // XXX: use constant pool?
		emit_mov_m_l(ma, long.view(low));
		emit_mov_m_l(MasmAddr(ma.base, ma.offset + 8), long.view(high));
	}

	def emit_addi_r_r(reg: Reg, reg2: Reg) {
		asm.add_r_r(R.toGpr(reg), X86_64Regs2.toGpr(reg2));
	}
	def emit_addi_r_i(reg: Reg, val: int) {
		asm.add_r_i(R.toGpr(reg), val);
	}

	def emit_binop_r_r(op: Opcode, reg: Reg, reg2: Reg) {
		match (op) {
			// i32 r_m binops
			I32_ADD => asm.d.add_r_r(G(reg), G(reg2));
			I32_SUB => asm.d.sub_r_r(G(reg), G(reg2));
			I32_MUL => asm.d.imul_r_r(G(reg), G(reg2));
			I32_DIV_S,
			I32_DIV_U,
			I32_REM_S,
			I32_REM_U => unimplemented();
			I32_AND => asm.d.and_r_r(G(reg), G(reg2));
			I32_OR  => asm.d.or_r_r(G(reg), G(reg2));
			I32_XOR => asm.d.xor_r_r(G(reg), G(reg2));
			I32_SHL,
			I32_SHR_S,
			I32_SHR_U,
			I32_ROTL,
			I32_ROTR => unimplemented();
			// i64 r_m binops
			I64_ADD => asm.q.add_r_r(G(reg), G(reg2));
			I64_SUB => asm.q.sub_r_r(G(reg), G(reg2));
			I64_MUL => asm.q.imul_r_r(G(reg), G(reg2));
			I64_DIV_S,
			I64_DIV_U,
			I64_REM_S,
			I64_REM_U => unimplemented();
			I64_AND => asm.q.and_r_r(G(reg), G(reg2));
			I64_OR  => asm.q.or_r_r(G(reg), G(reg2));
			I64_XOR => asm.q.xor_r_r(G(reg), G(reg2));
			I64_SHL,
			I64_SHR_S,
			I64_SHR_U,
			I64_ROTL,
			I64_ROTR => unimplemented();
			_ => unimplemented();
		}
	}
	def emit_binop_r_m(op: Opcode, reg: Reg, ma: MasmAddr) {
		var addr = A(ma);
		match (op) {
			// i32 r_m binops
			I32_ADD => asm.d.add_r_m(G(reg), addr);
			I32_SUB => asm.d.sub_r_m(G(reg), addr);
			I32_MUL => asm.d.imul_r_m(G(reg), addr);
			I32_DIV_S,
			I32_DIV_U,
			I32_REM_S,
			I32_REM_U => unimplemented();
			I32_AND => asm.d.and_r_m(G(reg), addr);
			I32_OR  => asm.d.or_r_m(G(reg), addr);
			I32_XOR => asm.d.xor_r_m(G(reg), addr);
			I32_SHL,
			I32_SHR_S,
			I32_SHR_U,
			I32_ROTL,
			I32_ROTR => unimplemented();
			// i64 r_m binops
			I64_ADD => asm.q.add_r_m(G(reg), addr);
			I64_SUB => asm.q.sub_r_m(G(reg), addr);
			I64_MUL => asm.q.imul_r_m(G(reg), addr);
			I64_DIV_S,
			I64_DIV_U,
			I64_REM_S,
			I64_REM_U => unimplemented();
			I64_AND => asm.q.and_r_m(G(reg), addr);
			I64_OR  => asm.q.or_r_m(G(reg), addr);
			I64_XOR => asm.q.xor_r_m(G(reg), addr);
			I64_SHL,
			I64_SHR_S,
			I64_SHR_U,
			I64_ROTL,
			I64_ROTR => unimplemented();
			_ => unimplemented();
		}
	}
	def emit_binop_r_i(op: Opcode, reg: Reg, val: int) {
		match (op) {
			// i32 r_m binops
			I32_ADD => asm.d.add_r_i(G(reg), val);
			I32_SUB => asm.d.sub_r_i(G(reg), val);
			I32_MUL => asm.d.imul_r_i(G(reg), val);
			I32_DIV_S,
			I32_DIV_U,
			I32_REM_S,
			I32_REM_U => unimplemented();
			I32_AND => asm.d.and_r_i(G(reg), val);
			I32_OR  => asm.d.or_r_i(G(reg), val);
			I32_XOR => asm.d.xor_r_i(G(reg), val);
			I32_SHL => asm.d.shl_r_i(G(reg), u5.view(val));
			I32_SHR_S => asm.d.sar_r_i(G(reg), u5.view(val));
			I32_SHR_U => asm.d.shl_r_i(G(reg), u5.view(val));
			I32_ROTL => asm.d.rol_r_i(G(reg), u5.view(val));
			I32_ROTR => asm.d.ror_r_i(G(reg), u5.view(val));
			// i64 r_m binops
			I64_ADD => asm.q.add_r_i(G(reg), val);
			I64_SUB => asm.q.sub_r_i(G(reg), val);
			I64_MUL => asm.q.imul_r_i(G(reg), val);
			I64_DIV_S,
			I64_DIV_U,
			I64_REM_S,
			I64_REM_U => unimplemented();
			I64_AND => asm.q.and_r_i(G(reg), val);
			I64_OR  => asm.q.or_r_i(G(reg), val);
			I64_XOR => asm.q.xor_r_i(G(reg), val);
			I64_SHL => asm.q.shl_r_i(G(reg), u6.view(val));
			I64_SHR_S => asm.q.sar_r_i(G(reg), u6.view(val));
			I64_SHR_U => asm.q.shl_r_i(G(reg), u6.view(val));
			I64_ROTL => asm.q.rol_r_i(G(reg), u6.view(val));
			I64_ROTR => asm.q.rol_r_i(G(reg), u6.view(val));

			_ => unimplemented();
		}
	}
	def emit_ret() {
		asm.ret();
	}
	def emit_nop() {
		asm.q.or_r_r(X86_64Regs.RAX, X86_64Regs.RAX);
	}
}
