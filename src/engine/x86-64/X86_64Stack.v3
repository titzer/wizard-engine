// Copyright 2021 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def G = X86_64MasmRegs.toGpr, X = X86_64MasmRegs.toXmmr;

// TODO: clean this up
def NOP_ENTRY = 1;
def RETADDR_SIZE = Pointer.SIZE;
def V3_STACK_RESUME_FUNC = X86_64PreGenFunc<(X86_64Stack, X86_64Stack), Throwable>.new("v3-stack-resume", X86_64SimpleStub.new("v3-stack-resume", 0), genV3StackResumeStub);
def STACK_RETURN_PARENT_STUB = X86_64PreGenStub.new("stack-return-parent", X86_64ReturnParentStub.new(), genStackReturnParentStub);
def STACK_ENTER_FUNC_STUB = X86_64PreGenStub.new("stack-enter-func", X86_64SimpleStub.new("stack-enter-func", 0), genStackEnterFuncStub);
def STACK_UNWIND_STUB = X86_64PreGenStub.new("stack-unwind", X86_64UnwindStub.new("stack-unwind"), genStackUnwind(_, _, null));
def STACK_OVERFLOW_STUB = X86_64PreGenStub.new("stack-overflow", X86_64UnwindStub.new("stack-overflow"), genStackUnwind(_, _, Execute.trapObjects[TrapReason.STACK_OVERFLOW.tag]));

// Called by V3 code to resume a stack.
// Sets up {X86_64Runtime.curStack}, VSP, and switches the machine stack pointer,
// then pops the resume address off the stack and jumps to it.
def genV3StackResumeStub(ic: X86_64InterpreterCode, w: DataWriter) {
	var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	var asm = X86_64Assembler.!(masm.asm);
	var r_stack = X86_64MasmRegs.PARAM_GPRS[1];	// this stack
	var r_bottom = X86_64MasmRegs.PARAM_GPRS[2];	// bottom of this stack
	var xenv = X86_64MasmRegs.INT_EXEC_ENV;
	var m_curStack = MasmAddr(Reg(0), int.!(masm.getOffsets().X86_64Runtime_curStack - Pointer.NULL));
	// mov [cur_stack], %stack
	masm.emit_mov_m_r(ValueKind.REF, m_curStack, r_stack);
	// mov %tmp, [%bottom.parent_rsp_ptr]
	masm.emit_mov_r_m(ValueKind.REF, xenv.scratch, MasmAddr(r_bottom, masm.offsets.X86_64Stack_parent_rsp_ptr));
	// mov [%tmp == [%bottom.parent_rsp_ptr]], %rsp
	masm.emit_mov_m_r(ValueKind.REF, MasmAddr(xenv.scratch, 0), xenv.sp);
	// mov rsp, [%stack.rsp]  ; switch machine stack pointer
	masm.emit_mov_r_m(ValueKind.REF, xenv.sp, MasmAddr(r_stack, masm.offsets.X86_64Stack_rsp));
	// mov vsp, [%stack.vsp]  ; load VSP
	masm.emit_mov_r_m(ValueKind.REF, xenv.vsp, MasmAddr(r_stack, masm.offsets.X86_64Stack_vsp));
	// pop %tmp  ; pop resume address on top of the stack and jump there.
	masm.emit_pop_r(ValueKind.REF, xenv.scratch);
	// jump *%tmp
	masm.emit_jump_r(xenv.scratch);
}

// The return parent stub requires special handling for Virgil stackwalking to continue
// the stackwalk on the parent stack.
class X86_64ReturnParentStub extends RiUserCode {
	new() super(Pointer.NULL, Pointer.NULL) { }

	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: Range<byte> -> void) {
		if (Debug.stack) X86_64Stacks.traceIpAndSp(ip, sp, out);
		var buf = X86_64Runtime.globalFrameDescriptionBuf;
		buf.puts("\tin [return-parent-stub]");
		if (Debug.stack) {
			var parent_rsp = sp.load<Pointer>();
			buf.put1(" parent_rsp=0x%x", parent_rsp - Pointer.NULL);
		}
		buf.ln().send(out);
		buf.reset();
	}
	// Called from V3 runtime to get to the next frame.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp = sp.load<Pointer>(); // parent_rsp on stack
		if (sp == Pointer.NULL) return (Pointer.NULL, Pointer.NULL); // not initialized
		ip = sp.load<Pointer>();
		return (ip + -1, sp + RETADDR_SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}
}

// The unwind stub is used to remove frames that have been logically unwound but still remain
// on the stack because, e.g. host code has not yet returned to Wasm.
//  [     host frame*    ]                 [     host frame*    ]
//  [   host call stub   ]  == unwind ==>  [     unwind stub    ]
//  [     wasm frame*    ]                 [        ...         ]
//  [ return parent stub ]                 [ return parent stub ]
class X86_64UnwindStub extends RiUserCode {
	def name: string;
	new(name) super(Pointer.NULL, Pointer.NULL) { }

	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: Range<byte> -> void) {
		if (Debug.stack) X86_64Stacks.traceIpAndSp(ip, sp, out);
		var buf = X86_64Runtime.globalFrameDescriptionBuf;
		buf.puts("\tin [").puts(name).puts("-stub]");
		if (Debug.stack) {
			var next_rsp = X86_64Runtime.curStack.rsp;
			buf.put1(" next_rsp=0x%x", next_rsp - Pointer.NULL);
		}
		buf.ln().send(out);
		buf.reset();
	}
	// Called from V3 runtime to get to the next frame.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp = X86_64Runtime.curStack.rsp;
		if (sp == Pointer.NULL) return (Pointer.NULL, Pointer.NULL); // not initialized
		ip = sp.load<Pointer>();
		return (ip + -1, sp + RETADDR_SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}
}

// Returned to by Wasm code when it pops its last frame.
// Inspects the parent stack and switches to it, which might be the host (V3) stack.
def genStackReturnParentStub(ic: X86_64InterpreterCode, w: DataWriter) {
	var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	var asm = X86_64Assembler.!(masm.asm);
	var xenv = X86_64MasmRegs.INT_EXEC_ENV;
	var m_curStack = MasmAddr(Reg(0), int.!(masm.getOffsets().X86_64Runtime_curStack - Pointer.NULL));
	var r_stack = xenv.tmp0;
	var r_parent = xenv.tmp1;
	var r_return_arity = xenv.tmp3;
	var r_parent_vsp = xenv.tmp4;

	// Nothing calls this stub; it is only returned to. A nop serves as a placeholder for a non-existent
	// call instruction so that stackwalking logic works properly.
	asm.nop1();
	// mov %stack, [cur_stack]
	masm.emit_mov_r_m(ValueKind.REF, r_stack, m_curStack);
	// mov [%stack.vsp], %vsp
	masm.emit_mov_m_r(ValueKind.REF, MasmAddr(r_stack, masm.offsets.X86_64Stack_vsp), xenv.vsp);
	// mov %parent, [%stack.parent]
	masm.emit_mov_r_m(ValueKind.REF, r_parent, MasmAddr(r_stack, masm.offsets.X86_64Stack_parent));
	// if %parent == 0 goto l_return ; don't copy values onto parent stack
	var l_return = masm.newLabel(-1);
	masm.emit_br_r(r_parent, MasmBrCond.REF_NULL, l_return);
	// if %ret_throw != 0 goto l_return ; don't copy values if this stack errors
	masm.emit_br_r(xenv.ret_throw, MasmBrCond.REF_NONNULL, l_return);

	// set %return_arity
	masm.emit_mov_r_m(ValueKind.REF, xenv.scratch, MasmAddr(r_stack, masm.offsets.X86_64Stack_return_results));
	masm.emit_mov_r_m(ValueKind.I32, r_return_arity, MasmAddr(xenv.scratch, masm.offsets.Array_length));

	// use %stack as temp reg
	var r_count = xenv.tmp0;
	masm.emit_mov_r_r(ValueKind.I32, r_count, r_return_arity);
	// mov %parent_vsp, [%parent.vsp]
	masm.emit_mov_r_m(ValueKind.REF, r_parent_vsp, MasmAddr(r_parent, masm.offsets.X86_64Stack_vsp));
	// %stack.vsp -= slot_size * %return_arity
	masm.emit_mov_r_r(ValueKind.I32, xenv.scratch, r_return_arity);
	asm.q.imul_r_i(G(xenv.scratch), Target.tagging.slot_size);
	asm.q.sub_r_r(G(xenv.vsp), G(xenv.scratch));
	// emit_value_copy(%parent.vsp, %stack.vsp, %count, r_xmm0);
	masm.emit_value_copy(G(r_parent_vsp), G(xenv.vsp), G(r_count), X(xenv.xmm0));
	// parent_vsp += slot_size * %return_arity
	masm.emit_mov_r_r(ValueKind.I32, xenv.scratch, r_return_arity);
	asm.q.imul_r_i(G(xenv.scratch), Target.tagging.slot_size);
	asm.q.add_r_r(G(r_parent_vsp), G(xenv.scratch));
	// [%parent.vsp] = %parent_vsp
	masm.emit_mov_m_r(ValueKind.REF, MasmAddr(r_parent, masm.offsets.X86_64Stack_vsp), r_parent_vsp);

	// restore %stack
	// mov %stack, [cur_stack]
	masm.emit_mov_r_m(ValueKind.REF, r_stack, m_curStack);
	// mov [%stack.parent], nullptr
	masm.emit_mov_m_l(MasmAddr(r_stack, masm.offsets.X86_64Stack_parent), 0);
	// mov [%stack.parent_rsp_ptr], nullptr
	masm.emit_mov_m_l(MasmAddr(r_stack, masm.offsets.X86_64Stack_parent_rsp_ptr), 0);
	// l_return:
	masm.bindLabel(l_return);
	// mov [cur_stack], %parent
	masm.emit_mov_m_r(ValueKind.REF, m_curStack, r_parent);
	// pop %rsp
	masm.emit_pop_r(ValueKind.REF, xenv.sp);
	// ret
	masm.emit_ret();
}

// Called when resuming a stack that has a function entry on the top of the stack.
def genStackEnterFuncStub(ic: X86_64InterpreterCode, w: DataWriter) {
	var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	var asm = X86_64Assembler.!(masm.asm);
	var xenv = X86_64MasmRegs.INT_EXEC_ENV;
	var m_curStack = MasmAddr(Reg(0), int.!(masm.getOffsets().X86_64Runtime_curStack - Pointer.NULL));
	var r_stack = xenv.tmp0;
	var r_func = xenv.func_arg;

	// Nothing calls this stub. Instead the stack resume stub "returns" to this stub by popping it off
	// the stack and jumping to it. A nop serves as a placeholder for a non-existent call instruction so
	// that stackwalking logic works properly.
	asm.nop1();
	// mov %stack, [cur_stack]    ; load stack from (thread-local) curStack
	masm.emit_mov_r_m(ValueKind.REF, r_stack, m_curStack);
	// mov %func, [%stack.func]   ; load function from stack object
	masm.emit_mov_r_m(ValueKind.REF, r_func, MasmAddr(r_stack, masm.offsets.X86_64Stack_func));
	var call_host = masm.newLabel(-1);
	var call_wasm = masm.newLabel(-1);
	masm.bindLabel(call_wasm);
	// Check if function is a WasmFunction or a HostFunction.
	masm.emit_br_r(r_func, MasmBrCond.IS_NOT_WASM_FUNC, call_host); // XXX: near jump

	// mov %vsp, [%stack.vsp]    ; load VSP from stack object
	masm.emit_mov_r_m(ValueKind.REF, xenv.vsp, MasmAddr(r_stack, masm.offsets.X86_64Stack_vsp));
	// WasmFunction: call into interpreter reentry or target code. XXX: factor out to MacroAssembler?
	if (FeatureDisable.multiTier) {
		// jump to interpreter directly as if tail-calling from SPC code
		var entry = ic.start + ic.header.intSpcEntryOffset;
		asm.jmp_rel_addr(masm.absPointer(entry));
	} else {
		// Load entrypoint from r_func.decl.target_code
		var do_jump = masm.newLabel(-1);
		masm.emit_mov_r_m(ValueKind.REF, xenv.tmp2, MasmAddr(r_func, masm.offsets.WasmFunction_decl));
		masm.emit_mov_r_m(ValueKind.REF, xenv.tmp2, MasmAddr(xenv.tmp2, masm.offsets.FuncDecl_target_code));
		// Jump to interpreter if entrypoint not set
		masm.emit_br_r(xenv.tmp2, MasmBrCond.I32_NONZERO, do_jump);
		var entry = ic.start + ic.header.intSpcEntryOffset;
		masm.emit_mov_r_k(ValueKind.I32, xenv.tmp2, int.!(entry - Pointer.NULL));
		masm.bindLabel(do_jump);
		masm.emit_jump_r(xenv.tmp2);
	}

	// Handle a call to a host function. XXX: jump to CallHostStub instead.
	masm.bindLabel(call_host);
	// mov [%stack.rsp], %sp - 8
	masm.emit_mov_r_r(ValueKind.REF, xenv.tmp3, xenv.sp);
	masm.emit_subw_r_i(xenv.tmp3, Pointer.SIZE);
	masm.emit_mov_m_r(ValueKind.REF, MasmAddr(r_stack, masm.offsets.X86_64Stack_rsp), xenv.tmp3);
	// Call runtime to do a host call
	masm.emit_mov_r_r(ValueKind.REF, xenv.runtime_arg0, r_func);
	masm.emit_call_runtime_callHost(r_func);
	// mov %stack, [cur_stack]    ; reload stack object from (thread-local) curStack
	masm.emit_mov_r_m(ValueKind.REF, r_stack, m_curStack);
	// mov %vsp, [%stack.vsp]
	masm.emit_mov_r_m(ValueKind.REF, xenv.vsp, MasmAddr(r_stack, masm.offsets.X86_64Stack_vsp));
	// mov %func, %runtime_ret1   ; move WasmFunction return into r_func
	masm.emit_mov_r_r(ValueKind.REF, r_func, xenv.runtime_ret1);
	// Check if WasmFunction != null, which means host tail-called Wasm
	masm.emit_br_r(r_func, MasmBrCond.REF_NONNULL, call_wasm); // XXX: near jump
	// Otherwise return throwable
	masm.emit_ret();
}

// Returned to by host code to unwind frames on the stack, e.g. to exit because of a trap or catch
// an exception.
def genStackUnwind(ic: X86_64InterpreterCode, w: DataWriter, thrown: Throwable) {
	var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	var asm = X86_64Assembler.!(masm.asm);
	var xenv = X86_64MasmRegs.INT_EXEC_ENV;
	var m_curStack = MasmAddr(Reg(0), int.!(masm.getOffsets().X86_64Runtime_curStack - Pointer.NULL));
	var r_stack = xenv.tmp0;

	// Nothing calls this stub; it is only returned to. A nop serves as a placeholder for a non-existent
	// call instruction so that stackwalking logic works properly.
	asm.nop1();
	// mov %stack, [cur_stack]          ; reload stack object from curStack
	masm.emit_mov_r_m(ValueKind.REF, r_stack, m_curStack);
	// mov %vsp, [%stack.vsp]           ; reload VSP from stack object
	masm.emit_mov_r_m(ValueKind.REF, xenv.vsp, MasmAddr(r_stack, masm.offsets.X86_64Stack_vsp));
	if (thrown != null) {
		// TODO: instead of a dedicated stack overflow stub, overwrite %ret_throw in the ucontext.
		// mov %ret_throw, #thrown  ; for stack overflow, load a trap object into a register
		var ptr = Pointer.atObject(thrown);
		asm.movq_r_l(G(xenv.ret_throw), ptr - Pointer.NULL);
	}
	// mov [%stack.rsp], %rsp           ; unwind stack
	masm.emit_mov_r_m(ValueKind.REF, xenv.sp, MasmAddr(r_stack, masm.offsets.X86_64Stack_rsp));
	// and return
	masm.emit_ret();
}

