// Copyright 2021 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// Implements a Wasm stack using native memory in {mapping}. Stores explicitly tagged values
// and execution frames within the mapping, containing a custom GC scan routine.
// See diagram at the end of the file for mapping layout.
class X86_64Stack extends WasmStack {
	def valuerep = Target.tagging;
	def size: u32;
	def mapping = Mmap.reserve(size, Mmap.PROT_READ | Mmap.PROT_WRITE);
	var vsp: Pointer;
	var rsp: Pointer;
	var func: Function;
	
	// For tail-calling purposes (from V3): ALWAYS, ALWAYS use this instead of
	// [%parent.rsp]. In case of linking two stacks (due to one resuming the
	// other, assert that this is {null} on the child stack and manually set it).
	var parent_rsp_ptr: Pointer;
	
	// For {resume_throw}.
	var throw_on_resume: Throwable;

	var params_arity = -1;
	var return_results: Array<ValueType>;
	var state_: StackState;

	new(size) {
		if (mapping == null) fatal("out of memory allocating value stack");
		clear();
		def PAGE_SIZE = 4096u;
		// insert redzone roughly in the middle of the mapping.
		var redzone_start = (size >> 1) & ~(PAGE_SIZE - 1u);
		var ok = RedZones.addRedZone(mapping, redzone_start, PAGE_SIZE);
		if (!ok) fatal("could not protect value stack red zone");
		if (valuerep.tagged) RiGc.registerScanner(this, X86_64Stack.scan);
	}
	// Gets the state of this stack.
	def state() -> StackState {
		return state_;
	}
	// Requires {state == EMPTY}.
	// Resets this stack to be {SUSPENDED}, awaiting arguments for {func}.
	def reset(func: Function) -> this {
		checkState("reset()", StackState.EMPTY);
		this.func = func;
		params_arity = func.sig.params.length;
		return_results = func.sig.results;
		state_ = if(params_arity == 0, StackState.RESUMABLE, StackState.SUSPENDED);
		parent_rsp_ptr = pushRspPointer(Pointer.NULL); // placeholder for parent pointer
		pushRspPointer(STACK_RETURN_PARENT_STUB.getEntry() + NOP_ENTRY);
		pushRspPointer(STACK_ENTER_FUNC_STUB.getEntry() + NOP_ENTRY);
	}
	// Requires {state == SUSPENDED}.
	// Pushes {args} incrementally onto the value stack and transitions to {state == RESUMABLE}
	// when enough arguments are pushed.
	def bind(args: Range<Value>) -> this {
		if (args.length == 0) return;
		checkState("bind()", StackState.SUSPENDED);
		if (params_arity < args.length) fatal(Strings.format2("bind() expected %d arguments, got %d", params_arity, args.length));
		for (v in args) push(v);
		params_arity -= args.length;
		if (params_arity == 0) state_ = StackState.RESUMABLE;
	}
	// Requires {state == RESUMABLE}.
	// Resumes running the Wasm or host code in this stack until that code either returns, throws,
	// or suspends itself.
	def resume() -> Result {
		checkState("resume()", StackState.RESUMABLE);
		if (Trace.stack) traceFrames("stack.resume", rsp + RETADDR_SIZE);
		var bottom = this;
		while (bottom.parent != null) bottom = X86_64Stack.!(bottom.parent); // find bottom of this stack
		state_ = StackState.RUNNING;
		var thrown = V3_STACK_RESUME_FUNC.get()(this, bottom);
		if (thrown != null) {
			if (Trap.?(thrown) && Trap.!(thrown).reason == TrapReason.INVALID_SUSPEND) {
				// TODO[ss]: suspend info
				return Result.StackSwitch(StackSwitchInfo.Suspend(null, null, []));
			}
			return Result.Throw(thrown);
		}
		var r = popResult(return_results);
		clear();
		return r;
	}
	// Gets a {FrameLoc} for the top of the stack.
	def where() -> FrameLoc;
	// Gets the caller of a given {FrameLoc}.
	def caller(loc: FrameLoc) -> FrameLoc;
	// "Throw" a trap on this stack.
	def trap(reason: TrapReason) -> Throwable {
		var thrown = throw(Trap.new(reason, null, null));
		return thrown;
	}
	// Throw a throwable on this stack, unwinding to the handler, if there is one.
	def throw(thrown: Throwable) -> Throwable {
		if (Trace.stack) traceFrames(Strings.format1("stack.throw(%q)", thrown.render), rsp + RETADDR_SIZE);
		// Perform a stackwalk, starting from the RSP stored in this object, gathering frames.
		var prev_sp = this.rsp;
		if (Exception.?(thrown)) {
			var ex = Exception.!(thrown);
			var new_sp = walk(findExHandler, ex, prev_sp);
			unwind(prev_sp + -RETADDR_SIZE, new_sp);
			return ex;
		}

		var gatherTrace = !FeatureDisable.stacktraces;
		var trace = if (gatherTrace, Vector<(WasmFunction, int)>.new());
		var return_parent_sp = walk(if(gatherTrace, addFrameToTrace), trace, prev_sp);
		// Unwind this stack to the return parent stub and overwrite the caller's return IP.
		this.vsp = mapping.range.start; // clear value stack
		unwind(prev_sp + -RETADDR_SIZE, return_parent_sp);

		// Append the (reversed) stacktrace to the throwable.
		if (trace != null) thrown.prependFrames(ArrayUtil.copyReverse(trace));
		return thrown;
	}
	// Walk the stack, applying {f} to every target frame (i.e. no stubs). If {f} returns {false},
	// stop the stackwalk. Otherwise stop at the return-parent stub. In any case, return the last
	// valid stack pointer.
	// (XXX: takes a function {f} with an additional parameter, and the parameter, to avoid a closure).
	private def walk<P>(f: (TargetFrame, P) -> bool, param: P, start_sp: Pointer) -> Pointer {
		var sp = start_sp;
		while (sp < mapping.range.end) {
			var retip = (sp + -RETADDR_SIZE).load<Pointer>();
			var code = RiRuntime.findUserCode(retip);
			var accessor: FrameAccessor;
			if (Trace.stack && Debug.stack) {
				Trace.OUT.put1("\tretip=0x%x", retip - Pointer.NULL);
				if (code != null) code.describeFrame(retip, sp, Trace.OUT.putr_void);
				else Trace.OUT.ln();
			}
			match (code) {
				null => break;
				x: X86_64InterpreterCode => if (f != null && !f(TargetFrame(sp), param)) return sp;
				x: X86_64SpcModuleCode => if (f != null && !f(TargetFrame(sp), param)) return sp;
				x: X86_64SpcTrapsStub => if (f != null && !f(TargetFrame(sp), param)) return sp;
				x: X86_64ReturnParentStub => return sp;
			}
			var t = code.nextFrame(retip, sp);
			sp = t.1;
		}
		return Pointer.NULL;
	}
	def traceFrames(when: string, sp: Pointer) {
		Trace.OUT.puts(when);
		if (Debug.stack) Trace.OUT.put1(" rsp=0x%x", rsp - Pointer.NULL);
		Trace.OUT.ln();
		while (sp >= mapping.range.start && sp < mapping.range.end) {
			var retip = (sp + -RETADDR_SIZE).load<Pointer>();
			var code = RiRuntime.findUserCode(retip);
			if (code == null) break;
			code.describeFrame(retip, sp, Trace.STDOUT_void);
			var t = code.nextFrame(retip, sp);
			sp = t.1;
		}
	}
	def setRsp(ptr: Pointer) -> this {
		rsp = ptr;
	}
	// This function returns true if a handler IS NOT found, and otherwise
	// returns {(stp_idx, order)}.
	def findSuspendHandler(frame: TargetFrame, tag: Tag) -> bool {
		var accessor = frame.getFrameAccessor();
		var func = accessor.func();
		var handler = func.decl.findSuspendHandler(func.instance, tag, accessor.pc());

		if (handler.handler_pc >= 0) {
			if (Trace.stack) {
				Trace.OUT.put1("  found suspend handler: pc=%d,", handler.handler_pc);
				Trace.OUT.put1(" stp=%d", handler.sidetable_pos).ln();
			}

			// set pc and stp to handler pos
			accessor.setNewProgramLocation(func.decl, handler.handler_pc, handler.sidetable_pos);
			// perform implicit popping for ctlxfer
			var vfp: Pointer = accessor.vfp();
			var stack_top = handler.val_stack_top + func.decl.num_locals;
			var vsp = vfp + (stack_top * valuerep.slot_size);
			this.vsp = vsp;
			accessor.set_vsp(this.vsp);
			
			return false;
		}

		return true;
	}
	private def findExHandler(frame: TargetFrame, ex: Exception) -> bool {
		var accessor = frame.getFrameAccessor(); // XXX: don't materialize accessor
		var func = accessor.func();
		var handler = func.decl.findExHandler(func.instance, ex.tag, accessor.pc());
		if (handler.handler_pc >= 0) {
			if (Trace.exception) Trace.OUT.put2("  walk found handler: pc=%d, stp=%d", handler.handler_pc, handler.sidetable_pos).ln();
			// XXX: handle exceptions in SPC code too
			accessor.deoptToInterpreter0(func.decl, handler.handler_pc, handler.sidetable_pos);
			var vfp: Pointer = accessor.vfp();
			var slot_num = (handler.val_stack_top + func.decl.num_locals + func.decl.num_ex_slots);
			var vsp = vfp + (slot_num * valuerep.slot_size);
			this.vsp = vsp;
			if (Trace.exception) Trace.OUT.put3("  push %d vfp=0x%x, vsp=0x%x", ex.vals.length, vfp - Pointer.NULL, vsp - Pointer.NULL).ln();
			for (v in ex.vals) this.push(v);
			if (handler.ex_slot >= 0) {
				if (Trace.exception) Trace.OUT.put2("  set_ex_slot vfp=0x%x, ex_slot=%d", vfp - Pointer.NULL, handler.ex_slot).ln();
				this.writeValue(vfp, func.decl.num_locals + handler.ex_slot, Value.Ref(ex));
			}
			if (handler.push_exnref) {
				if (Trace.exception) Trace.OUT.puts("  push_exnref").ln();
				this.push(Value.Ref(ex));
			}
			accessor.set_vsp(this.vsp);
			return false; // terminate stackwalk
		}
		if (Trace.exception) Trace.OUT.puts("  walk found no handler").ln();
		return true;
	}
	private def addFrameToTrace(frame: TargetFrame, trace: Vector<(WasmFunction, int)>) -> bool {
		var accessor = frame.getFrameAccessor(); // XXX: don't materialize accessor
		trace.put(accessor.func(), accessor.pc());
		return true;
	}
	private def unwind(p_retaddr: Pointer, new_sp: Pointer) {
		if (new_sp != this.rsp) {
			this.rsp = (new_sp + -RETADDR_SIZE);
			if (p_retaddr != Pointer.NULL) p_retaddr.store<Pointer>(STACK_UNWIND_STUB.getEntry() + NOP_ENTRY);
		}
	}
	def handleOverflow(ip: Pointer, sp: Pointer) -> Pointer {
		if (Trace.stack) traceFrames("stack.overflow (before)", sp);
		// Unwind the stack, skipping the top frame (which might have triggered the overflow)
		var return_parent_sp = walk<void>(null, (), sp);
		// Unwind without updating any return addresses.
		this.vsp = mapping.range.start; // clear value stack
		unwind(Pointer.NULL, return_parent_sp);
		// Instead, return from the signal to the stack overflow stub, which will unwind with a Trap.STACK_OVERFLOW throwable.
		if (Trace.stack) traceFrames("stack.overflow (after)", this.rsp + RETADDR_SIZE);
		return STACK_OVERFLOW_STUB.getEntry() + NOP_ENTRY;
	}
	// Empties all the frames on this stack and sets it back to the {StackState.EMPTY} state.
	def clear() -> this {
		state_ = StackState.EMPTY;
		vsp = mapping.range.start;
		rsp = mapping.range.end;
		params_arity = -1;
		return_results = null;
		parent =  null;
		parent_rsp_ptr = Pointer.NULL;
		throw_on_resume = null;
		func = null;
	}
	// def pushReenterStub() {
	// 	pushRspPointer(STACK_REENTER_FUNC_STUB.getEntry() + NOP_ENTRY);
	// }
	private def pushRspPointer(p: Pointer) -> Pointer {
		rsp = rsp + -Pointer.SIZE;
		(rsp + 0).store<Pointer>(p);
		return rsp;
	}

	private def checkState(op: string, expected: StackState) {
		if (state_ != expected) fatal(Strings.format3("%s requires state == %s, got %s", op, expected.name, state_.name));
	}

	def push(v: Value) {
		match (v) {
			Ref(obj) => pushPair(BpTypeCode.REF_NULL.code, obj);
			I31(val) => pushPair(BpTypeCode.I31REF.code, (u64.view(val) << 1) | 1);
			I32(val) => pushPair(BpTypeCode.I32.code, u64.view(val));
			I64(val) => pushPair(BpTypeCode.I64.code, u64.view(val));
			F32(bits) => pushPair(BpTypeCode.F32.code, u64.view(bits));
			F64(bits) => pushPair(BpTypeCode.F64.code, u64.view(bits));
			V128(low, high) => {
				if (valuerep.tagged) vsp.store<u8>(BpTypeCode.V128.code); // XXX: factor out
				(vsp + valuerep.tag_size).store(u64.view(low));
				(vsp + valuerep.tag_size + 8).store(u64.view(high));
				vsp += valuerep.slot_size;
			}
		}
	}
	def pushi(v: int) {
		pushPair(BpTypeCode.I32.code, u64.view(v));
	}
	def pushu(v: u32) {
		pushPair(BpTypeCode.I32.code, u64.view(v));
	}
	def pushN(vs: Range<Value>) {
		for (i < vs.length) push(vs[i]);
	}
	def popN(t: Range<ValueType>) -> Array<Value> {
		var r = Array<Value>.new(t.length);
		for (j = t.length - 1; j >= 0; j--) r[j] = pop(t[j]);
		return r;
	}
	def popV(t: ValueType) -> Value {
		return pop(t);
	}
	def pop(t: ValueType) -> Value {
		match (t) {
			I32 => return Value.I32(popb32(BpTypeCode.I32.code));
			I64 => return Value.I64(popb64(BpTypeCode.I64.code));
			F32 => return Value.F32(popb32(BpTypeCode.F32.code));
			F64 => return Value.F64(popb64(BpTypeCode.F64.code));
			V128 => {
				checkTopTag(BpTypeCode.V128.code); // XXX: factor out
				vsp += -(valuerep.slot_size);
				var low = (vsp + valuerep.tag_size).load<u64>();
				var high = (vsp + valuerep.tag_size + 8).load<u64>();
				return Value.V128(low, high);
			}
			Host => return Value.Ref(popObject());
			Ref(nullable, heap) => match(heap) { // TODO: tighter checking of ref value tags
				ANY, EXTERN, EQ, I31 => return popRef();
				Func => return Value.Ref(popFunction());
				_ => return Value.Ref(popObject());
			}
			_ => fatal(Strings.format1("unexpected type: %s", t.name));
		}
		return Value.Ref(null);
	}
	def popRef() -> Value {
		var val = peekRef();
		vsp += -(valuerep.slot_size);
		return val;
	}
	def peekRef() -> Value {
		if (valuerep.tagged) {
			var got = peekTag();
			if (!valuerep.maybeRefTag(got)) fatal(Strings.format1("value stack tag mismatch, expected ref, got %x", got));
		}
		return readI31OrObject(vsp + (valuerep.tag_size - valuerep.slot_size));
	}
	def popi() -> i32 {
		return i32.view(popb32(BpTypeCode.I32.code));
	}
	def popu() -> u32 {
		return popb32(BpTypeCode.I32.code);
	}
	def popl() -> i64 {
		return i64.view(popb64(BpTypeCode.I64.code));
	}
	def popw() -> u64 {
		return popb64(BpTypeCode.I64.code);
	}
	def popb32(tag: byte) -> u32 {
		checkTopTag(tag);
		vsp += -(valuerep.slot_size);
		return (vsp + valuerep.tag_size).load<u32>();
	}
	def popb64(tag: byte) -> u64 {
		checkTopTag(tag);
		vsp += -(valuerep.slot_size);
		return (vsp + valuerep.tag_size).load<u64>();
	}
	def popObject() -> Object {
		if (valuerep.tagged) {
			var got = peekTag();
			if (!valuerep.maybeRefTag(got)) fatal(Strings.format1("value stack tag mismatch, expected ref, got %x", got));
		}
		vsp += -(valuerep.slot_size);
		var val = (vsp + valuerep.tag_size).load<u64>();
		if ((val & 1) == 1) fatal("expected ref, got i31");
		return (vsp + valuerep.tag_size).load<Object>();
	}
	def checkTopTag(tag: byte) -> byte {
		if (!valuerep.tagged) return tag;
		var got = peekTag();
		if (got == tag) return tag;
		fatal(Strings.format2("value stack tag mismatch, expected: %x, got %x", tag, got));
		return tag;
	}
	def peekTag() -> byte {
		return (vsp + -(valuerep.slot_size)).load<u8>() & '\x7F';
	}
	def pushPair<T>(tag: byte, bits: T) {
		if (valuerep.tagged) vsp.store<u8>(tag);
		(vsp + valuerep.tag_size).store(bits);
		vsp += valuerep.slot_size;
	}
	// GC callback to scan (tagged) values on this stack
	def scan() {
		if (Trace.gc) {
			Trace.OUT.put1("scanStack obj=0x%x", u64.view(Pointer.atObject(this) - Pointer.NULL)).ln();
		}
		// Iterate values on the value stack.
		for (p = mapping.range.start; p < vsp; p += valuerep.slot_size) {
			var tag = p.load<byte>();
			if (Trace.gc) {
				Trace.OUT.put2("scanValue @ 0x%x = %x", u64.view(p - Pointer.NULL), tag).ln();
			}
			if (valuerep.maybeRefTag(tag)) {
				var val = (p + valuerep.tag_size).load<u64>();
				if ((val & 1) != 1) RiGc.scanRoot(p + valuerep.tag_size); // low bit == 1 => i31ref
			}
		}
		// Iterate Wasm execution frames. TODO
	}
	def readValue(base: Pointer, offset: int) -> Value {
		if (!valuerep.tagged) fatal("untyped frame access requires value tagging to be enabled");
		var tp = base + offset * valuerep.slot_size;
		if (!mapping.range.contains(tp)) System.error("FrameAccessError", "out of bounds");
		var vp = tp + valuerep.tag_size;
		var tag = tp.load<u8>() & '\x7F';
		match (tag) {
			BpTypeCode.ANYREF.code,
			BpTypeCode.EQREF.code,
			BpTypeCode.REF_NULL.code,
			BpTypeCode.REF.code,
			BpTypeCode.EXTERNREF.code,
			BpTypeCode.NULLEXTERNREF.code,
			BpTypeCode.I31REF.code => return readI31OrObject(vp);

			BpTypeCode.STRUCTREF.code,
			BpTypeCode.NULLREF.code,
			BpTypeCode.ARRAYREF.code,
			BpTypeCode.FUNCREF.code,
			BpTypeCode.EXNREF.code,
			BpTypeCode.NULLFUNCREF.code => return Value.Ref(vp.load<Object>());

			BpTypeCode.I32.code => return Value.I32(vp.load<u32>());
			BpTypeCode.I64.code => return Value.I64(vp.load<u64>());
			BpTypeCode.F32.code => return Value.F32(vp.load<u32>());
			BpTypeCode.F64.code => return Value.F64(vp.load<u64>());
			BpTypeCode.V128.code => return Value.V128(vp.load<u64>(), (vp + 8).load<u64>());
			_ => {
				fatal(Strings.format2("unknown value tag 0x%x @ 0x%x", tag, (tp - Pointer.NULL)));
				return Values.REF_NULL;
			}
		}
	}
	def writeValue(base: Pointer, offset: int, v: Value) {
		if (!valuerep.tagged) fatal("untyped frame access requires value tagging to be enabled");
		var tp = base + offset * valuerep.slot_size;
		var vp = tp + valuerep.tag_size;
		var tag = tp.load<u8>() & '\x7F';
		match (tag) {
			BpTypeCode.I32.code => vp.store<u32>(Values.unbox_u(v));
			BpTypeCode.I64.code => vp.store<u64>(Values.unbox_w(v));
			BpTypeCode.F32.code => vp.store<u32>(Values.unbox_fu32(v));
			BpTypeCode.F64.code => vp.store<u64>(Values.unbox_du64(v));
			BpTypeCode.V128.code => {
				var vv = Value.V128.!(v);
				vp.store<u64>(vv.low);
				(vp + 8).store<u64>(vv.high);
			}
			_ => {
				// TODO: allow reference value writes
				fatal(Strings.format2("unknown or unsupported write of value tag %x @ 0x%x", tag, (tp - Pointer.NULL)));
			}
		}
	}
	def readI31OrObject(vp: Pointer) -> Value {
		var bits = vp.load<u64>();
		if (bits == 0) return Values.REF_NULL;
		if ((bits & 1) == 1) return Value.I31(u31.view(bits >> 1));
		var obj = vp.load<Object>();
		return Value.Ref(obj);
	}
	def popResult(rt: Array<ValueType>) -> Result {
		var r = Array<Value>.new(rt.length);
		for (i = r.length - 1; i >= 0; i--) r[i] = pop(rt[i]);
		return Result.Value(r);
	}
	def fatal(msg: string) {
		System.error("X86_64StackError", msg);
	}
}

def G = X86_64MasmRegs.toGpr, X = X86_64MasmRegs.toXmmr;

component X86_64Stacks {
	var RESUME_STUB_POINTER: Pointer;

	def getFrameAccessor(sp: Pointer) -> X86_64FrameAccessor {
		var retip = (sp + -RETADDR_SIZE).load<Pointer>();
		var code = RiRuntime.findUserCode(retip);
		match (code) {
			x: X86_64InterpreterCode => {
				var prev = (sp + X86_64InterpreterFrame.accessor.offset).load<X86_64FrameAccessor>();
				if (prev != null) return prev;
				// Interpreter frames store the {WasmFunction} _and_ {FuncDecl}.
				var decl = (sp + X86_64InterpreterFrame.func_decl.offset).load<FuncDecl>();
				var n = X86_64FrameAccessor.new(X86_64Runtime.curStack, sp, decl);
				(sp + X86_64InterpreterFrame.accessor.offset).store<X86_64FrameAccessor>(n);
				return n;
			}
			x: X86_64SpcCode => {
				var prev = (sp + X86_64InterpreterFrame.accessor.offset).load<X86_64FrameAccessor>();
				if (prev != null) return prev;
				// SPC frames only store the {WasmFunction}.
				var wf = (sp + X86_64InterpreterFrame.wasm_func.offset).load<WasmFunction>();
				// TODO: assert wf.decl == x.decl()
				var n = X86_64FrameAccessor.new(X86_64Runtime.curStack, sp, wf.decl);
				(sp + X86_64InterpreterFrame.accessor.offset).store<X86_64FrameAccessor>(n);
				return n;
			}
		}
		return null;
	}
	def traceIpAndSp(ip: Pointer, sp: Pointer, out: Range<byte> -> void) {
		var buf = X86_64Runtime.globalFrameDescriptionBuf;
		buf.put2("\t@[ip=0x%x, sp=0x%x] ", ip - Pointer.NULL, sp - Pointer.NULL).send(out);
		buf.reset();
	}
}

// TODO: clean this up
def NOP_ENTRY = 1;
def RETADDR_SIZE = Pointer.SIZE;
def V3_STACK_RESUME_FUNC = X86_64PreGenFunc<(X86_64Stack, X86_64Stack), Throwable>.new("v3-stack-resume", X86_64SimpleStub.new("v3-stack-resume", 0), genV3StackResumeStub);
def STACK_RETURN_PARENT_STUB = X86_64PreGenStub.new("stack-return-parent", X86_64ReturnParentStub.new(), genStackReturnParentStub);
def STACK_ENTER_FUNC_STUB = X86_64PreGenStub.new("stack-enter-func", X86_64SimpleStub.new("stack-enter-func", 0), genStackEnterFuncStub);
def STACK_UNWIND_STUB = X86_64PreGenStub.new("stack-unwind", X86_64UnwindStub.new("stack-unwind"), genStackUnwind(_, _, null));
def STACK_OVERFLOW_STUB = X86_64PreGenStub.new("stack-overflow", X86_64UnwindStub.new("stack-overflow"), genStackUnwind(_, _, Execute.trapObjects[TrapReason.STACK_OVERFLOW.tag]));

// Called by V3 code to resume a stack.
// Sets up {X86_64Runtime.curStack}, VSP, and switches the machine stack pointer,
// then pops the resume address off the stack and jumps to it.
def genV3StackResumeStub(ic: X86_64InterpreterCode, w: DataWriter) {
	var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	var asm = X86_64Assembler.!(masm.asm);
	var r_stack = X86_64MasmRegs.PARAM_GPRS[1];	// this stack
	var r_bottom = X86_64MasmRegs.PARAM_GPRS[2];	// bottom of this stack
	var xenv = X86_64MasmRegs.INT_EXEC_ENV;
	var m_curStack = MasmAddr(Reg(0), int.!(masm.getOffsets().X86_64Runtime_curStack - Pointer.NULL));
	// mov [cur_stack], %stack
	masm.emit_mov_m_r(ValueKind.REF, m_curStack, r_stack);
	// mov %tmp, [%bottom.parent_rsp_ptr]
	masm.emit_mov_r_m(ValueKind.REF, xenv.scratch, MasmAddr(r_bottom, masm.offsets.X86_64Stack_parent_rsp_ptr));
	// mov [%tmp == [%bottom.parent_rsp_ptr]], %rsp
	masm.emit_mov_m_r(ValueKind.REF, MasmAddr(xenv.scratch, 0), xenv.sp);
	// mov rsp, [%stack.rsp]  ; switch machine stack pointer
	masm.emit_mov_r_m(ValueKind.REF, xenv.sp, MasmAddr(r_stack, masm.offsets.X86_64Stack_rsp));
	// mov vsp, [%stack.vsp]  ; load VSP
	masm.emit_mov_r_m(ValueKind.REF, xenv.vsp, MasmAddr(r_stack, masm.offsets.X86_64Stack_vsp));
	// pop %tmp  ; pop resume address on top of the stack and jump there.
	masm.emit_pop_r(ValueKind.REF, xenv.scratch);
	// jump *%tmp
	masm.emit_jump_r(xenv.scratch);
}

// The return parent stub requires special handling for Virgil stackwalking to continue
// the stackwalk on the parent stack.
class X86_64ReturnParentStub extends RiUserCode {
	new() super(Pointer.NULL, Pointer.NULL) { }

	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: Range<byte> -> void) {
		if (Debug.stack) X86_64Stacks.traceIpAndSp(ip, sp, out);
		var buf = X86_64Runtime.globalFrameDescriptionBuf;
		buf.puts("\tin [return-parent-stub]");
		if (Debug.stack) {
			var parent_rsp = sp.load<Pointer>();
			buf.put1(" parent_rsp=0x%x", parent_rsp - Pointer.NULL);
		}
		buf.ln().send(out);
		buf.reset();
	}
	// Called from V3 runtime to get to the next frame.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp = sp.load<Pointer>(); // parent_rsp on stack
		if (sp == Pointer.NULL) return (Pointer.NULL, Pointer.NULL); // not initialized
		ip = sp.load<Pointer>();
		return (ip + -1, sp + RETADDR_SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}
}

// The unwind stub is used to remove frames that have been logically unwound but still remain
// on the stack because, e.g. host code has not yet returned to Wasm.
//  [     host frame*    ]                 [     host frame*    ]
//  [   host call stub   ]  == unwind ==>  [     unwind stub    ]
//  [     wasm frame*    ]                 [        ...         ]
//  [ return parent stub ]                 [ return parent stub ]
class X86_64UnwindStub extends RiUserCode {
	def name: string;
	new(name) super(Pointer.NULL, Pointer.NULL) { }

	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: Range<byte> -> void) {
		if (Debug.stack) X86_64Stacks.traceIpAndSp(ip, sp, out);
		var buf = X86_64Runtime.globalFrameDescriptionBuf;
		buf.puts("\tin [").puts(name).puts("-stub]");
		if (Debug.stack) {
			var next_rsp = X86_64Runtime.curStack.rsp;
			buf.put1(" next_rsp=0x%x", next_rsp - Pointer.NULL);
		}
		buf.ln().send(out);
		buf.reset();
	}
	// Called from V3 runtime to get to the next frame.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp = X86_64Runtime.curStack.rsp;
		if (sp == Pointer.NULL) return (Pointer.NULL, Pointer.NULL); // not initialized
		ip = sp.load<Pointer>();
		return (ip + -1, sp + RETADDR_SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}
}

// Returned to by Wasm code when it pops its last frame.
// Inspects the parent stack and switches to it, which might be the host (V3) stack.
def genStackReturnParentStub(ic: X86_64InterpreterCode, w: DataWriter) {
	var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	var asm = X86_64Assembler.!(masm.asm);
	var xenv = X86_64MasmRegs.INT_EXEC_ENV;
	var m_curStack = MasmAddr(Reg(0), int.!(masm.getOffsets().X86_64Runtime_curStack - Pointer.NULL));
	var r_stack = xenv.tmp0;
	var r_parent = xenv.tmp1;
	var r_return_arity = xenv.tmp3;
	var r_parent_vsp = xenv.tmp4;

	// Nothing calls this stub; it is only returned to. A nop serves as a placeholder for a non-existent
	// call instruction so that stackwalking logic works properly.
	asm.nop1();
	// mov %stack, [cur_stack]
	masm.emit_mov_r_m(ValueKind.REF, r_stack, m_curStack);
	// mov [%stack.vsp], %vsp
	masm.emit_mov_m_r(ValueKind.REF, MasmAddr(r_stack, masm.offsets.X86_64Stack_vsp), xenv.vsp);
	// mov %parent, [%stack.parent]
	masm.emit_mov_r_m(ValueKind.REF, r_parent, MasmAddr(r_stack, masm.offsets.X86_64Stack_parent));
	// if %parent == 0 goto l_return ; don't copy values onto parent stack
	var l_return = masm.newLabel(-1);
	masm.emit_br_r(r_parent, MasmBrCond.REF_NULL, l_return);
	// if %ret_throw != 0 goto l_return ; don't copy values if this stack errors
	masm.emit_br_r(xenv.ret_throw, MasmBrCond.REF_NONNULL, l_return);

	// set %return_arity
	masm.emit_mov_r_m(ValueKind.REF, xenv.scratch, MasmAddr(r_stack, masm.offsets.X86_64Stack_return_results));
	masm.emit_mov_r_m(ValueKind.I32, r_return_arity, MasmAddr(xenv.scratch, masm.offsets.Array_length));

	// use %stack as temp reg
	var r_count = xenv.tmp0;
	masm.emit_mov_r_r(ValueKind.I32, r_count, r_return_arity);
	// mov %parent_vsp, [%parent.vsp]
	masm.emit_mov_r_m(ValueKind.REF, r_parent_vsp, MasmAddr(r_parent, masm.offsets.X86_64Stack_vsp));
	// %stack.vsp -= slot_size * %return_arity
	masm.emit_mov_r_r(ValueKind.I32, xenv.scratch, r_return_arity);
	asm.q.imul_r_i(G(xenv.scratch), Target.tagging.slot_size);
	asm.q.sub_r_r(G(xenv.vsp), G(xenv.scratch));
	// emit_value_copy(%parent.vsp, %stack.vsp, %count, r_xmm0);
	masm.emit_value_copy(G(r_parent_vsp), G(xenv.vsp), G(r_count), X(xenv.xmm0));
	// parent_vsp += slot_size * %return_arity
	masm.emit_mov_r_r(ValueKind.I32, xenv.scratch, r_return_arity);
	asm.q.imul_r_i(G(xenv.scratch), Target.tagging.slot_size);
	asm.q.add_r_r(G(r_parent_vsp), G(xenv.scratch));
	// [%parent.vsp] = %parent_vsp
	masm.emit_mov_m_r(ValueKind.REF, MasmAddr(r_parent, masm.offsets.X86_64Stack_vsp), r_parent_vsp);

	// restore %stack
	// mov %stack, [cur_stack]
	masm.emit_mov_r_m(ValueKind.REF, r_stack, m_curStack);
	// mov [%stack.parent], nullptr
	masm.emit_mov_m_l(MasmAddr(r_stack, masm.offsets.X86_64Stack_parent), 0);
	// mov [%stack.parent_rsp_ptr], nullptr
	masm.emit_mov_m_l(MasmAddr(r_stack, masm.offsets.X86_64Stack_parent_rsp_ptr), 0);
	// l_return:
	masm.bindLabel(l_return);
	// mov [cur_stack], %parent
	masm.emit_mov_m_r(ValueKind.REF, m_curStack, r_parent);
	// pop %rsp
	masm.emit_pop_r(ValueKind.REF, xenv.sp);
	// ret
	masm.emit_ret();
}

// Called when resuming a stack that has a function entry on the top of the stack.
def genStackEnterFuncStub(ic: X86_64InterpreterCode, w: DataWriter) {
	var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	var asm = X86_64Assembler.!(masm.asm);
	var xenv = X86_64MasmRegs.INT_EXEC_ENV;
	var m_curStack = MasmAddr(Reg(0), int.!(masm.getOffsets().X86_64Runtime_curStack - Pointer.NULL));
	var r_stack = xenv.tmp0;
	var r_func = xenv.func_arg;

	// Nothing calls this stub. Instead the stack resume stub "returns" to this stub by popping it off
	// the stack and jumping to it. A nop serves as a placeholder for a non-existent call instruction so
	// that stackwalking logic works properly.
	asm.nop1();
	// mov %stack, [cur_stack]    ; load stack from (thread-local) curStack
	masm.emit_mov_r_m(ValueKind.REF, r_stack, m_curStack);
	// mov %func, [%stack.func]   ; load function from stack object
	masm.emit_mov_r_m(ValueKind.REF, r_func, MasmAddr(r_stack, masm.offsets.X86_64Stack_func));
	var call_host = masm.newLabel(-1);
	var call_wasm = masm.newLabel(-1);
	masm.bindLabel(call_wasm);
	// Check if function is a WasmFunction or a HostFunction.
	masm.emit_br_r(r_func, MasmBrCond.IS_NOT_WASM_FUNC, call_host); // XXX: near jump

	// mov %vsp, [%stack.vsp]    ; load VSP from stack object
	masm.emit_mov_r_m(ValueKind.REF, xenv.vsp, MasmAddr(r_stack, masm.offsets.X86_64Stack_vsp));
	// WasmFunction: call into interpreter reentry or target code. XXX: factor out to MacroAssembler?
	if (FeatureDisable.multiTier) {
		// jump to interpreter directly as if tail-calling from SPC code
		var entry = ic.start + ic.header.intSpcEntryOffset;
		asm.jmp_rel_addr(masm.absPointer(entry));
	} else {
		// Load entrypoint from r_func.decl.target_code
		var do_jump = masm.newLabel(-1);
		masm.emit_mov_r_m(ValueKind.REF, xenv.tmp2, MasmAddr(r_func, masm.offsets.WasmFunction_decl));
		masm.emit_mov_r_m(ValueKind.REF, xenv.tmp2, MasmAddr(xenv.tmp2, masm.offsets.FuncDecl_target_code));
		// Jump to interpreter if entrypoint not set
		masm.emit_br_r(xenv.tmp2, MasmBrCond.I32_NONZERO, do_jump);
		var entry = ic.start + ic.header.intSpcEntryOffset;
		masm.emit_mov_r_k(ValueKind.I32, xenv.tmp2, int.!(entry - Pointer.NULL));
		masm.bindLabel(do_jump);
		masm.emit_jump_r(xenv.tmp2);
	}

	// Handle a call to a host function. XXX: jump to CallHostStub instead.
	masm.bindLabel(call_host);
	// mov [%stack.rsp], %sp - 8
	masm.emit_mov_r_r(ValueKind.REF, xenv.tmp3, xenv.sp);
	masm.emit_subw_r_i(xenv.tmp3, Pointer.SIZE);
	masm.emit_mov_m_r(ValueKind.REF, MasmAddr(r_stack, masm.offsets.X86_64Stack_rsp), xenv.tmp3);
	// Call runtime to do a host call
	masm.emit_mov_r_r(ValueKind.REF, xenv.runtime_arg0, r_func);
	masm.emit_call_runtime_callHost(r_func);
	// mov %stack, [cur_stack]    ; reload stack object from (thread-local) curStack
	masm.emit_mov_r_m(ValueKind.REF, r_stack, m_curStack);
	// mov %vsp, [%stack.vsp]
	masm.emit_mov_r_m(ValueKind.REF, xenv.vsp, MasmAddr(r_stack, masm.offsets.X86_64Stack_vsp));
	// mov %func, %runtime_ret1   ; move WasmFunction return into r_func
	masm.emit_mov_r_r(ValueKind.REF, r_func, xenv.runtime_ret1);
	// Check if WasmFunction != null, which means host tail-called Wasm
	masm.emit_br_r(r_func, MasmBrCond.REF_NONNULL, call_wasm); // XXX: near jump
	// Otherwise return throwable
	masm.emit_ret();
}

// Returned to by host code to unwind frames on the stack, e.g. to exit because of a trap or catch
// an exception.
def genStackUnwind(ic: X86_64InterpreterCode, w: DataWriter, thrown: Throwable) {
	var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	var asm = X86_64Assembler.!(masm.asm);
	var xenv = X86_64MasmRegs.INT_EXEC_ENV;
	var m_curStack = MasmAddr(Reg(0), int.!(masm.getOffsets().X86_64Runtime_curStack - Pointer.NULL));
	var r_stack = xenv.tmp0;

	// Nothing calls this stub; it is only returned to. A nop serves as a placeholder for a non-existent
	// call instruction so that stackwalking logic works properly.
	asm.nop1();
	// mov %stack, [cur_stack]          ; reload stack object from curStack
	masm.emit_mov_r_m(ValueKind.REF, r_stack, m_curStack);
	// mov %vsp, [%stack.vsp]           ; reload VSP from stack object
	masm.emit_mov_r_m(ValueKind.REF, xenv.vsp, MasmAddr(r_stack, masm.offsets.X86_64Stack_vsp));
	if (thrown != null) {
		// TODO: instead of a dedicated stack overflow stub, overwrite %ret_throw in the ucontext.
		// mov %ret_throw, #thrown  ; for stack overflow, load a trap object into a register
		var ptr = Pointer.atObject(thrown);
		asm.movq_r_l(G(xenv.ret_throw), ptr - Pointer.NULL);
	}
	// mov [%stack.rsp], %rsp           ; unwind stack
	masm.emit_mov_r_m(ValueKind.REF, xenv.sp, MasmAddr(r_stack, masm.offsets.X86_64Stack_rsp));
	// and return
	masm.emit_ret();
}

// X86_64Stack object
//================================================================================================
// state =      SUSPENDED                 RESUMABLE                  RUNNING
//
//                stack                      stack             stack <------ X86_64Runtime.curStack
//                  |                          |                  |
//                  |                          |                  +-----+
//                  |                          |                        |
//                  V                          V                        V
// start ->  [     value     ]         [     value     ]         [     value     ]
// vsp ->                              [     value     ]         [     value     ]
//                              vsp -> [     value     ]         [     value     ]
//                                                        vsp -> [     value     ]
//
//             ..............           ...............           ...............
//
//                                                        rsp -> [  wasm frame   ]
//                              rsp -> [ enter/reenter ]         [  wasm frame   ]
// rsp ->    [     entry     ]         [  wasm frame   ]         [  wasm frame   ]
//           [ return parent ]         [ return parent ]         [ return parent ]
// end ->
//
//                                                            stack.parent_rsp
//                                                               |
//                                                         +-----+
//                                                         |
//                                                         |
//                                                         +--> [ virgil frame  ]
//                                                              [ virgil frame  ]
//                                                              [ virgil frame  ]

component X86_64Frames {
	// Convert a machine stack pointer into a reference to the frame.
	def fromSp(sp: Pointer) -> Ref<X86_64InterpreterFrame> {
		return Ref<X86_64InterpreterFrame>.of(CiRuntime.forgeRange<byte>(sp, X86_64InterpreterFrame.size));
	}
}

layout X86_64InterpreterFrame {
	+0	wasm_func	: i64;	// WasmFunction
	+8	mem0_base	: i64;	// Pointer
	+16	vfp		: i64;	// Pointer
	+24	vsp		: i64;	// Pointer
	+32	sidetable	: i64;	// Array<int>
	+40	stp		: i64;	// Pointer
	+48	code		: i64;	// Array<byte>
	+56	ip		: i64;	// Pointer
	+64	eip		: i64;	// Pointer
	+72	func_decl	: i64;	// FuncDecl
	+80	instance	: i64;	// Instance
	+88	curpc		: int;
	+96	accessor	: i64;	// FrameAccessor
	=104;
}

// Native frame states used in the implementation of {FrameStateAccessor}. Since a frame
// can be optimized or deoptimized in place, the frame state accessor has to check the
// state for every call.
enum X86_64FrameState {
	INVALID, INTERPRETER, SPC, SPC_TRAPS_STUB
}
// Implements access to interpreter and SPC frames.
class X86_64FrameAccessor(stack: X86_64Stack, sp: Pointer, decl: FuncDecl) extends FrameAccessor {
	var writer: X86_64FrameWriter; // non-null if any writes have been made
	var cached_depth = -1;
	var cached_pc: int;

	// Returns {true} if this frame has been unwound, either due to returning, a trap, or exception.
	def isUnwound() -> bool {
		if (FeatureDisable.frameAccess) return false; // TODO: proper is-frame-unwound check
		return this != (sp + X86_64InterpreterFrame.accessor.offset).load<X86_64FrameAccessor>();
	}
	// Returns the Wasm function in this frame.
	def func() -> WasmFunction {
		checkNotUnwound();
		return (sp + X86_64InterpreterFrame.wasm_func.offset).load<WasmFunction>();
	}
	// Returns the current program counter.
	def pc() -> int {
		checkNotUnwound();
		var ip = readIp();
		var code = RiRuntime.findUserCode(ip);
		match (code) {
			x: X86_64SpcModuleCode => cached_pc = x.lookupPc(ip, true);
			x: X86_64InterpreterCode => cached_pc = X86_64Interpreter.computePCFromFrame(sp);
			x: X86_64SpcTrapsStub => cached_pc = (sp + X86_64InterpreterFrame.curpc.offset).load<int>();
			_ => cached_pc = -1;
		}
		return cached_pc;
	}
	// Returns {true} if this frame is currently the top executing frame, {false} if the
	// frame has called another function or been unwound.
	def isTop() -> bool {
		return true; // TODO?
	}
	// Returns the call depth of this frame within its segment, with the bottom frame being #0.
	def depth() -> int {
		checkNotUnwound();
		if (cached_depth < 0) cached_depth = computeDepth();
		return cached_depth;
	}
	private def computeDepth() -> int {
		var depth = 0;
		var next_sp = sp;
		while (true) {
			next_sp = next_sp + X86_64InterpreterFrame.size + RETADDR_SIZE;
			var retip = (next_sp + -RETADDR_SIZE).load<Pointer>();
			var code = RiRuntime.findUserCode(retip);
			match (code) {
				x: X86_64InterpreterCode => ;
				x: X86_64SpcCode => ;
				_ => return depth;
			}
			depth++;
		}
		return 0;
	}
	def caller() -> FrameLoc {
		checkNotUnwound();
		var frame = TargetFrame(callerSp());
		var accessor = frame.getFrameAccessor();
		if (accessor == null) return FrameLoc.None;
		return FrameLoc.Wasm(accessor.func(), accessor.pc(), frame);
	}
	def stp() -> int {
		checkNotUnwound();
		var ip = readIp();
		var code = RiRuntime.findUserCode(ip);
		match (code) {
			x: X86_64InterpreterCode => {
				var stp = (sp + X86_64InterpreterFrame.stp.offset).load<Pointer>();
				var stp0 = Pointer.atContents(func().decl.sidetable.entries);
				return int.!((stp - stp0) / 4);
			}
			_ => return -1;
		}
	}
	// Get the number of local variables in this frame.
	def numLocals() -> int {
		checkNotUnwound();
		return decl.num_locals;
	}
	// Get the value of local variable {i}.
	def getLocal(i: int) -> Value {
		checkNotUnwound();
		checkLocalBounds(i);
		var vfp = (sp + X86_64InterpreterFrame.vfp.offset).load<Pointer>();
		return stack.readValue(vfp, i);
	}
	// Get the number of operand stack elements.
	def numOperands() -> int {
		checkNotUnwound();
		var vfp = (sp + X86_64InterpreterFrame.vfp.offset).load<Pointer>();
		var vsp = (sp + X86_64InterpreterFrame.vsp.offset).load<Pointer>();
		var diff = int.!((vsp - vfp) / Target.tagging.slot_size);
		return diff - decl.num_locals;
	}
	// Get operand at depth {i}, with 0 being the top of the stack, -1 being one lower, etc.
	def getOperand(i: int) -> Value {
		checkNotUnwound();
		var vsp = (sp + X86_64InterpreterFrame.vsp.offset).load<Pointer>();
		return stack.readValue(vsp, i - 1);
	}
	// Get the frame writer.
	def getWriter() -> X86_64FrameWriter {
		return if(writer != null, writer, writer = X86_64FrameWriter.new(this));
	}

	// Read the return address from the frame.
	def readRetAddr() -> Pointer {
		return (sp + X86_64InterpreterFrame.size).load<Pointer>();
	}
	// Read the instruction pointer from the frame.
	private def readIp() -> Pointer {
		return (sp + -RETADDR_SIZE).load<Pointer>();
	}
	// Compute the caller frame's stack pointer.
	def callerSp() -> Pointer {
		return sp + X86_64InterpreterFrame.size + RETADDR_SIZE;
	}
	def isSpc() -> bool {
		var ip = readIp();
		var code = RiRuntime.findUserCode(ip);
		return X86_64SpcCode.?(code);
	}

	private def checkNotUnwound() {
		if (isUnwound()) System.error("FrameAccessorError", "frame has been unwound");
	}
	private def checkLocalBounds(i: int) {
		if (u32.view(i) >= decl.num_locals) System.error("FrameAccessorError",
			Strings.format2("local index %d out-of-bounds (%d)", u32.view(i), decl.num_locals));
	}
	private def deoptToInterpreter() {
		var wf = func(), func = wf.decl;
		var pc = this.pc();
		var map = SidetableMap.new(func); // TODO: cache in FuncDecl?
		var stp = map[pc];
		deoptToInterpreter0(func, pc, stp);
	}
	private def deoptToInterpreter0(func: FuncDecl, pc: int, stp: int) {
		var ic = X86_64PreGenStubs.getInterpreterCode();
		setNewProgramLocation(func, pc, stp);
		(sp + -RETADDR_SIZE).store<Pointer>(ic.start + ic.header.deoptReentryOffset);
	}
	private def setNewProgramLocation(func: FuncDecl, pc: int, stp: int) {
		var code = func.cur_bytecode;
		(sp + X86_64InterpreterFrame.func_decl.offset)	.store<FuncDecl>(func);
		(sp + X86_64InterpreterFrame.curpc.offset)	.store<int>(pc);
		(sp + X86_64InterpreterFrame.code.offset)	.store<Array<byte>>(code);
		(sp + X86_64InterpreterFrame.ip.offset)		.store<Pointer>(Pointer.atElement(code, pc));
		(sp + X86_64InterpreterFrame.eip.offset)	.store<Pointer>(Pointer.atContents(code) + code.length);
		var st_entries = func.sidetable.entries;
		var st_ptr = if(stp == st_entries.length, Pointer.atContents(st_entries), Pointer.atElement(func.sidetable.entries, stp));
		(sp + X86_64InterpreterFrame.stp.offset)	.store<Pointer>(st_ptr);
	}
	private def vfp() -> Pointer {
		return (sp + X86_64InterpreterFrame.vfp.offset).load<Pointer>();
	}
	private def set_vsp(p: Pointer) {
		(sp + X86_64InterpreterFrame.vsp.offset).store<Pointer>(p);
	}
}
private class X86_64FrameWriter extends FrameWriter {
	private def accessor: X86_64FrameAccessor;

	new(accessor) { }

	// Set the value of a local variable. (dynamically typechecked).
	def setLocal(i: int, v: Value) {
		accessor.checkNotUnwound();
		accessor.checkLocalBounds(i);
		var vfp = (accessor.sp + X86_64InterpreterFrame.vfp.offset).load<Pointer>();
		accessor.stack.writeValue(vfp, i, v);
		if (accessor.isSpc()) accessor.deoptToInterpreter();
	}
}

// Manages stack space.
component X86_64StackManager {
	var cache: X86_64Stack;

	def getFreshStack() -> X86_64Stack {
		if (cache == null) return X86_64Stack.new(EngineOptions.STACK_SIZE.get());
		var result = cache;
		cache = null;
		return result;
	}
	def recycleStack(stack: X86_64Stack) {
		if (cache == null) cache = stack; // XXX: save the larger/smaller of the stacks?
	}
	def runOnFreshStack(f: Function, args: Range<Value>) -> Result {
		// Always run functions on a separate, fresh stack.
		var prev = X86_64Runtime.curStack; // handle reentrancy
		var stack = X86_64StackManager.getFreshStack();
		var result = stack.reset(f).bind(args).resume();
		X86_64Runtime.curStack = prev;
		X86_64StackManager.recycleStack(stack.clear());
		return result;
	}
}
