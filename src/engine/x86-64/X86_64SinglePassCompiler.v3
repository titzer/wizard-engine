// Copyright 2022 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// XXX: reduce duplication with MacroAssembler
def G = X86_64MasmRegs.toGpr, X = X86_64MasmRegs.toXmmr;
def R: X86_64Regs;
def C: X86_64Conds;
def A(ma: MasmAddr) -> X86_64Addr {
	return X86_64Addr.new(G(ma.base), null, 1, ma.offset);
}

// Shorten constants inside this file.
def NO_REG = SpcConsts.NO_REG;
def IS_STORED = SpcConsts.IS_STORED;
def IS_CONST = SpcConsts.IS_CONST;
def IN_REG = SpcConsts.IN_REG;
def TAG_STORED = SpcConsts.TAG_STORED;
def KIND_MASK = SpcConsts.KIND_MASK;
def KIND_I32 = SpcConsts.KIND_I32;
def KIND_I64 = SpcConsts.KIND_I64;
def KIND_F32 = SpcConsts.KIND_F32;
def KIND_F64 = SpcConsts.KIND_F64;
def KIND_V128 = SpcConsts.KIND_V128;
def KIND_REF = SpcConsts.KIND_REF;

// Implements the target-specific parts of the single-pass compiler for X86-64.
class X86_64SinglePassCompiler extends SinglePassCompiler {
	def w = DataWriter.new();
	def mmasm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	def asm = mmasm.asm;

	new(extensions: Extension.set, limits: Limits, config: RegConfig)
		super(X86_64MasmRegs.SPC_EXEC_ENV, mmasm, X86_64MasmRegs.SPC_ALLOC.copy(), extensions, limits) {
		mmasm.trap_stubs = TRAPS_STUB;
	}

	private def visitCompareI(asm: X86_64Assembler, cond: X86_64Cond) -> bool {
		var b = pop(), a = popReg();
		if (b.isConst()) asm.cmp_r_i(G(a.reg), b.const);
		else if (b.inReg()) asm.cmp_r_r(G(a.reg), G(b.reg));
		else asm.cmp_r_m(G(a.reg), A(masm.slotAddr(state.sp + 1)));
		var d = allocRegTos(ValueKind.I32), r1 = G(d);
		asm.set_r(cond, r1);
		asm.movbzx_r_r(r1, r1);
		state.push(KIND_I32 | IN_REG, d, 0);
		return true;
	}
	def visit_I32_EQZ() {
		state.push(KIND_I32 | IS_CONST, NO_REG, 0);
		visit_I32_EQ();
	}
	def visit_I32_EQ()   { void(tryFold_uu_z(V3Eval.I32_EQ)   || visitCompareI(asm.d, C.Z)); }
	def visit_I32_NE()   { void(tryFold_uu_z(V3Eval.I32_NE)   || visitCompareI(asm.d, C.NZ)); }
	def visit_I32_LT_S() { void(tryFold_ii_z(V3Eval.I32_LT_S) || visitCompareI(asm.d, C.L)); }
	def visit_I32_LT_U() { void(tryFold_uu_z(V3Eval.I32_LT_U) || visitCompareI(asm.d, C.C)); }
	def visit_I32_GT_S() { void(tryFold_ii_z(V3Eval.I32_GT_S) || visitCompareI(asm.d, C.G)); }
	def visit_I32_GT_U() { void(tryFold_uu_z(V3Eval.I32_GT_U) || visitCompareI(asm.d, C.A)); }
	def visit_I32_LE_S() { void(tryFold_ii_z(V3Eval.I32_LE_S) || visitCompareI(asm.d, C.LE)); }
	def visit_I32_LE_U() { void(tryFold_uu_z(V3Eval.I32_LE_U) || visitCompareI(asm.d, C.NA)); }
	def visit_I32_GE_S() { void(tryFold_ii_z(V3Eval.I32_GE_S) || visitCompareI(asm.d, C.GE)); }
	def visit_I32_GE_U() { void(tryFold_uu_z(V3Eval.I32_GE_U) || visitCompareI(asm.d, C.NC)); }

	def visit_I64_EQZ() {
		state.push(KIND_I64 | IS_CONST, NO_REG, 0);
		visit_I64_EQ();
	}
	def visit_I64_EQ()   { void(tryFold_qq_z(V3Eval.I64_EQ)   || visitCompareI(asm.q, C.Z)); }
	def visit_I64_NE()   { void(tryFold_qq_z(V3Eval.I64_NE)   || visitCompareI(asm.q, C.NZ)); }
	def visit_I64_LT_S() { void(tryFold_ll_z(V3Eval.I64_LT_S) || visitCompareI(asm.q, C.L)); }
	def visit_I64_LT_U() { void(tryFold_qq_z(V3Eval.I64_LT_U) || visitCompareI(asm.q, C.C)); }
	def visit_I64_GT_S() { void(tryFold_ll_z(V3Eval.I64_GT_S) || visitCompareI(asm.q, C.G)); }
	def visit_I64_GT_U() { void(tryFold_qq_z(V3Eval.I64_GT_U) || visitCompareI(asm.q, C.A)); }
	def visit_I64_LE_S() { void(tryFold_ll_z(V3Eval.I64_LE_S) || visitCompareI(asm.q, C.LE)); }
	def visit_I64_LE_U() { void(tryFold_qq_z(V3Eval.I64_LE_U) || visitCompareI(asm.q, C.NA)); }
	def visit_I64_GE_S() { void(tryFold_ll_z(V3Eval.I64_GE_S) || visitCompareI(asm.q, C.GE)); }
	def visit_I64_GE_U() { void(tryFold_qq_z(V3Eval.I64_GE_U) || visitCompareI(asm.q, C.NC)); }

	private def visitFloatCmp(emit_cmp: (X86_64Xmmr, X86_64Xmmr) -> X86_64Assembler, unordered_true: bool, cond: X86_64Cond) {
		var b = popReg(), a = popReg();
		var d = allocRegTos(ValueKind.I32);
		var ret_zero = X86_64Label.new(), ret_one = X86_64Label.new(), done = X86_64Label.new();
		emit_cmp(X(a.reg), X(b.reg));
		asm.jc_rel_near(C.P, if(unordered_true, ret_one, ret_zero));
		asm.jc_rel_near(cond, ret_zero);
		asm.bind(ret_one);
		asm.movd_r_i(G(d), 1);
		asm.jmp_rel_near(done);
		asm.bind(ret_zero);
		asm.movd_r_i(G(d), 0);
		asm.bind(done);
		state.push(KIND_I32 | IN_REG, d, 0);
	}
	def visit_F32_EQ() { visitFloatCmp(asm.ucomiss_s_s, false, C.NZ); }
	def visit_F32_NE() { visitFloatCmp(asm.ucomiss_s_s, true, C.Z); }
	def visit_F32_LT() { visitFloatCmp(asm.ucomiss_s_s, false, C.NC); }
	def visit_F32_GT() { visitFloatCmp(asm.ucomiss_s_s, false, C.NA); }
	def visit_F32_LE() { visitFloatCmp(asm.ucomiss_s_s, false, C.A); }
	def visit_F32_GE() { visitFloatCmp(asm.ucomiss_s_s, false, C.C); }

	def visit_F64_EQ() { visitFloatCmp(asm.ucomisd_s_s, false, C.NZ); }
	def visit_F64_NE() { visitFloatCmp(asm.ucomisd_s_s, true, C.Z); }
	def visit_F64_LT() { visitFloatCmp(asm.ucomisd_s_s, false, C.NC); }
	def visit_F64_GT() { visitFloatCmp(asm.ucomisd_s_s, false, C.NA); }
	def visit_F64_LE() { visitFloatCmp(asm.ucomisd_s_s, false, C.A); }
	def visit_F64_GE() { visitFloatCmp(asm.ucomisd_s_s, false, C.C); }

	def visit_REF_IS_NULL() {
		// XXX: recombine with visitCompareI
		var a = pop();
		if (a.isConst()) {
			state.push(KIND_I32 | IS_CONST, NO_REG, if(a.const == 0, 1, 0));
			return;
		}
		if (a.inReg()) asm.cmp_r_i(G(a.reg), 0);
		else asm.q.cmp_m_i(A(masm.slotAddr(state.sp)), 0);
		var d = allocRegTos(ValueKind.I32), r1 = G(d);
		asm.set_r(C.Z, r1);
		asm.movbzx_r_r(r1, r1);
		state.push(KIND_I32 | IN_REG, d, 0);
	}
	def visit_I32_CLZ() {
		void(tryFold_u_u(V3Eval.I32_CLZ)
			|| do_op1_r_r(ValueKind.I32, mmasm.emit_i32_clz_r_r));
	}
	def visit_I32_CTZ() {
		void(tryFold_u_u(V3Eval.I32_CTZ)
			|| do_op1_r_r(ValueKind.I32, mmasm.emit_i32_ctz_r_r));
	}
	def visit_I32_POPCNT() {
		void(tryFold_u_u(V3Eval.I32_POPCNT)
			|| do_op1_r_r(ValueKind.I32, asm.d.popcnt_r_r));
	}

	private def visitSimpleOp2_uu_u(fold: (u32, u32) -> u32,
		emit_r_i: (X86_64Gpr, int) -> X86_64Assembler,
		emit_r_m: (X86_64Gpr, X86_64Addr) -> X86_64Assembler,
		emit_r_r: (X86_64Gpr, X86_64Gpr) -> X86_64Assembler) {
		void(tryFold_uu_u(fold)
			|| do_op2_r_i(ValueKind.I32, emit_r_i)
			|| do_op2_r_m(ValueKind.I32, emit_r_m)
			|| do_op2_r_r(ValueKind.I32, emit_r_r));
	}
	def visit_I32_ADD() { visitSimpleOp2_uu_u(V3Eval.I32_ADD, asm.d.add_r_i, asm.d.add_r_m, asm.d.add_r_r); }
	def visit_I32_SUB() { visitSimpleOp2_uu_u(V3Eval.I32_SUB, asm.d.sub_r_i, asm.d.sub_r_m, asm.d.sub_r_r); }
	def visit_I32_MUL() { visitSimpleOp2_uu_u(V3Eval.I32_MUL, asm.d.imul_r_i, asm.d.imul_r_m, asm.d.imul_r_r); }

	private def visitIDivRem(dst: Reg, emit: X86_64Gpr -> void) {
		var b = popFixedReg(X86_64MasmRegs.RCX); // XXX: use any reg except RAX or RDX
		var a = popFixedReg(X86_64MasmRegs.RAX);
		spillRegAndFree(X86_64MasmRegs.RAX);
		spillRegAndFree(X86_64MasmRegs.RDX);
		emit(G(b.reg));
		regAlloc.assign(dst, int.!(state.sp));
		state.push(a.kindFlagsAndTag(IN_REG), dst, 0);
	}
	def visit_I32_DIV_S() { visitIDivRem(X86_64MasmRegs.RAX, mmasm.emit_i32_div_s); } // XXX: fold potentially-trapping div/rem
	def visit_I32_DIV_U() { visitIDivRem(X86_64MasmRegs.RAX, mmasm.emit_i32_div_u); }
	def visit_I32_REM_S() { visitIDivRem(X86_64MasmRegs.RDX, mmasm.emit_i32_rem_s); }
	def visit_I32_REM_U() { visitIDivRem(X86_64MasmRegs.RDX, mmasm.emit_i32_rem_u); }

	private def visitShift(kind: ValueKind, emit_r_i: (X86_64Gpr, u6) -> X86_64Assembler, emit_r_cl: X86_64Gpr -> X86_64Assembler) -> bool {
		var b = state.peek(), a: SpcVal;
		if (b.isConst()) {
			pop();
			a = popRegToOverwrite();
			emit_r_i(G(a.reg), u6.view(b.const));
		} else {
			b = popFixedReg(X86_64MasmRegs.RCX);
			a = popRegToOverwrite();
			emit_r_cl(G(a.reg));
		}
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, a.reg, 0); // XXX: tag preservation
		return true;
	}
	def visit_I32_SHL()   { void(tryFold_ii_i(V3Eval.I32_SHL) || visitShift(ValueKind.I32, asm.d.shl_r_i, asm.d.shl_r_cl)); }
	def visit_I32_SHR_S() { void(tryFold_ii_i(V3Eval.I32_SHR_S) || visitShift(ValueKind.I32, asm.d.sar_r_i, asm.d.sar_r_cl)); }
	def visit_I32_SHR_U() { void(tryFold_ii_i(V3Eval.I32_SHR_U) || visitShift(ValueKind.I32, asm.d.shr_r_i, asm.d.shr_r_cl)); }
	def visit_I32_ROTL()  { void(tryFold_uu_u(V3Eval.I32_ROTL) || visitShift(ValueKind.I32, asm.d.rol_r_i, asm.d.rol_r_cl)); }
	def visit_I32_ROTR()  { void(tryFold_uu_u(V3Eval.I32_ROTR) || visitShift(ValueKind.I32, asm.d.ror_r_i, asm.d.ror_r_cl)); }
	def visit_I32_AND() { visitSimpleOp2_uu_u(V3Eval.I32_AND, asm.d.and_r_i, asm.d.and_r_m, asm.d.and_r_r); }
	def visit_I32_OR()  { visitSimpleOp2_uu_u(V3Eval.I32_OR, asm.d.or_r_i, asm.d.or_r_m, asm.d.or_r_r); }
	def visit_I32_XOR() { visitSimpleOp2_uu_u(V3Eval.I32_XOR, asm.d.xor_r_i, asm.d.xor_r_m, asm.d.xor_r_r); }

	def visit_I64_CLZ() {
		void(tryFold_w_w(V3Eval.I64_CLZ)
			|| do_op1_r_r(ValueKind.I64, mmasm.emit_i64_clz_r_r));
	}
	def visit_I64_CTZ() {
		void(tryFold_w_w(V3Eval.I64_CTZ)
			|| do_op1_r_r(ValueKind.I64, mmasm.emit_i64_ctz_r_r));
	}
	def visit_I64_POPCNT() {
		void(tryFold_w_w(V3Eval.I64_POPCNT)
			|| do_op1_r_r(ValueKind.I64, asm.q.popcnt_r_r));
	}
	private def visitSimpleOp2_ww_w(fold: (u64, u64) -> u64,
		emit_r_i: (X86_64Gpr, int) -> X86_64Assembler,
		emit_r_m: (X86_64Gpr, X86_64Addr) -> X86_64Assembler,
		emit_r_r: (X86_64Gpr, X86_64Gpr) -> X86_64Assembler) {
		void(tryFold_ww_w(fold)
			|| do_op2_r_i(ValueKind.I64, emit_r_i)
			|| do_op2_r_m(ValueKind.I64, emit_r_m)
			|| do_op2_r_r(ValueKind.I64, emit_r_r));
	}
	def visit_I64_ADD() { visitSimpleOp2_ww_w(V3Eval.I64_ADD, asm.q.add_r_i, asm.q.add_r_m, asm.q.add_r_r); }
	def visit_I64_SUB() { visitSimpleOp2_ww_w(V3Eval.I64_SUB, asm.q.sub_r_i, asm.q.sub_r_m, asm.q.sub_r_r); }
	def visit_I64_MUL() { visitSimpleOp2_ww_w(V3Eval.I64_MUL, asm.q.imul_r_i, asm.q.imul_r_m, asm.q.imul_r_r); }
	def visit_I64_DIV_S() { visitIDivRem(X86_64MasmRegs.RAX, mmasm.emit_i64_div_s); }
	def visit_I64_DIV_U() { visitIDivRem(X86_64MasmRegs.RAX, mmasm.emit_i64_div_u); }
	def visit_I64_REM_S() { visitIDivRem(X86_64MasmRegs.RDX, mmasm.emit_i64_rem_s); }
	def visit_I64_REM_U() { visitIDivRem(X86_64MasmRegs.RDX, mmasm.emit_i64_rem_u); }
	def visit_I64_SHL()   { void(tryFold_ww_w(V3Eval.I64_SHL) || visitShift(ValueKind.I64, asm.q.shl_r_i, asm.q.shl_r_cl)); }
	def visit_I64_SHR_S() { void(tryFold_ll_l(V3Eval.I64_SHR_S) || visitShift(ValueKind.I64, asm.q.sar_r_i, asm.q.sar_r_cl)); }
	def visit_I64_SHR_U() { void(tryFold_ww_w(V3Eval.I64_SHR_U) || visitShift(ValueKind.I64, asm.q.shr_r_i, asm.q.shr_r_cl)); }
	def visit_I64_ROTL()  { void(tryFold_ww_w(V3Eval.I64_ROTL) || visitShift(ValueKind.I64, asm.q.rol_r_i, asm.q.rol_r_cl)); }
	def visit_I64_ROTR()  { void(tryFold_ww_w(V3Eval.I64_ROTR) || visitShift(ValueKind.I64, asm.q.ror_r_i, asm.q.ror_r_cl)); }
	def visit_I64_AND() { visitSimpleOp2_ww_w(V3Eval.I64_AND, asm.q.and_r_i, asm.q.and_r_m, asm.q.and_r_r); }
	def visit_I64_OR()  { visitSimpleOp2_ww_w(V3Eval.I64_OR, asm.q.or_r_i, asm.q.or_r_m, asm.q.or_r_r); }
	def visit_I64_XOR() { visitSimpleOp2_ww_w(V3Eval.I64_XOR, asm.q.xor_r_i, asm.q.xor_r_m, asm.q.xor_r_r); }

	// XXX: try s_m addressing mode for floating point ops
	def visit_F32_ABS() {
		var sv = popReg(), r = X(sv.reg), scratch = mmasm.scratch;
		var d = allocRegTos(ValueKind.F32);
		asm.movd_r_s(scratch, X(sv.reg));
		asm.d.and_r_i(scratch, 0x7FFFFFFF);
		asm.movd_s_r(X(d), scratch);
		state.push(sv.kindFlagsAndTag(IN_REG), d, 0);
	}
	def visit_F32_NEG() {
		var sv = popReg(), r = X(sv.reg), scratch = mmasm.scratch;
		var d = allocRegTos(ValueKind.F32);
		asm.movd_r_s(scratch, X(sv.reg));
		asm.d.xor_r_i(scratch, 0x80000000);
		asm.movd_s_r(X(d), scratch);
		state.push(sv.kindFlagsAndTag(IN_REG), d, 0);
	}
	def visit_F32_CEIL() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_POS_INF)); }
	def visit_F32_FLOOR() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_NEG_INF)); }
	def visit_F32_TRUNC() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_ZERO)); }
	def visit_F32_NEAREST() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_NEAREST)); }
	def visit_F32_SQRT() { do_op1_x_x(ValueKind.F32, asm.sqrtss_s_s); }
	def visit_F32_ADD() { do_op2_x_x(ValueKind.F32, asm.addss_s_s); }
	def visit_F32_SUB() { do_op2_x_x(ValueKind.F32, asm.subss_s_s); }
	def visit_F32_MUL() { do_op2_x_x(ValueKind.F32, asm.mulss_s_s); }
	def visit_F32_DIV() { do_op2_x_x(ValueKind.F32, asm.divss_s_s); }

	def visitFloatMinOrMax(is64: bool, isMin: bool) { // XXX: move to macro assembler?
		var b = popReg(), a = popRegToOverwrite();
		var ret_a = X86_64Label.new(), ret_b = X86_64Label.new(), is_nan = X86_64Label.new();
		var done = X86_64Label.new();
		var xmmA = X(a.reg), xmmB = X(b.reg);

		if (is64) asm.ucomisd_s_s(xmmA, xmmB);
		else asm.ucomiss_s_s(xmmA, xmmB);
		asm.jc_rel_far(C.P, is_nan);
		asm.jc_rel_near(C.C, if(isMin, ret_a, ret_b));
		asm.jc_rel_near(C.A, if(isMin, ret_b, ret_a));
		asm.movq_r_s(mmasm.scratch, xmmB); // XXX: does a 32-bit move save anything?
		if (is64) asm.q.cmp_r_i(mmasm.scratch, 0);
		else asm.d.cmp_r_i(mmasm.scratch, 0);
		asm.jc_rel_near(if(isMin, C.S, C.NS), ret_b); // handle min(-0, 0) == -0
		asm.jmp_rel_near(ret_a);

		asm.bind(is_nan);
		if (is64) {
			masm.emit_mov_r_d64(a.reg, FloatUtils.d_nan);
		} else {
			masm.emit_mov_r_f32(a.reg, FloatUtils.f_nan);
		}
		asm.jmp_rel_near(done);

		asm.bind(ret_b);
		if (is64) asm.movsd_s_s(xmmA, xmmB); // fallthru
		else asm.movss_s_s(xmmA, xmmB); // fallthru
		asm.bind(ret_a); // nop
		asm.bind(done);

		state.push(a.kindFlagsAndTag(IN_REG), a.reg, 0);
	}
	def visit_F32_MIN() { visitFloatMinOrMax(false, true); }
	def visit_F32_MAX() { visitFloatMinOrMax(false, false); }
	def visit_F32_COPYSIGN() {
		var sv = popReg(), d = popRegToOverwrite(); // XXX: constant-fold and strength reduce
		var t1 = allocTmp(ValueKind.I32), t2 = allocTmp(ValueKind.I32);
		mmasm.emit_f32_copysign(X(d.reg), X(sv.reg), G(t1), G(t2));
		state.push(d.kindFlagsAndTag(IN_REG), d.reg, 0);
	}

	// XXX: try s_m addressing mode for floating point ops
	def visit_F64_ABS() {
		var sv = popReg();
		var d = allocRegTos(ValueKind.F64);
		var t = allocTmp(ValueKind.I64), r1 = G(t);
		asm.movq_r_s(r1, X(sv.reg));
		asm.q.rol_r_i(r1, 1);
		asm.q.and_r_i(r1, -2);
		asm.q.ror_r_i(r1, 1);
		asm.movq_s_r(X(d), G(t));
		state.push(sv.kindFlagsAndTag(IN_REG), d, 0);
	}
	def visit_F64_NEG() {
		var sv = popReg();
		var d = allocRegTos(ValueKind.F64);
		var t = allocTmp(ValueKind.I64), r1 = G(t);
		asm.movq_r_s(r1, X(sv.reg));
		asm.q.rol_r_i(r1, 1);
		asm.q.xor_r_i(r1, 1);
		asm.q.ror_r_i(r1, 1);
		asm.movq_s_r(X(d), G(t));
		state.push(sv.kindFlagsAndTag(IN_REG), d, 0);
	}
	def visit_F64_CEIL() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_POS_INF)); }
	def visit_F64_FLOOR() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_NEG_INF)); }
	def visit_F64_TRUNC() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_ZERO)); }
	def visit_F64_NEAREST() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_NEAREST)); }
	def visit_F64_SQRT() { do_op1_x_x(ValueKind.F64, asm.sqrtsd_s_s); }
	def visit_F64_ADD() { do_op2_x_x(ValueKind.F64, asm.addsd_s_s); }
	def visit_F64_SUB() { do_op2_x_x(ValueKind.F64, asm.subsd_s_s); }
	def visit_F64_MUL() { do_op2_x_x(ValueKind.F64, asm.mulsd_s_s); }
	def visit_F64_DIV() { do_op2_x_x(ValueKind.F64, asm.divsd_s_s); }
	def visit_F64_MIN() { visitFloatMinOrMax(true, true); }
	def visit_F64_MAX() { visitFloatMinOrMax(true, false); }
	def visit_F64_COPYSIGN() {
		var sv = popReg(), d = popRegToOverwrite(); // XXX: constant-fold and strength reduce
		var t1 = allocTmp(ValueKind.I64), t2 = allocTmp(ValueKind.I64);
		mmasm.emit_f64_copysign(X(d.reg), X(sv.reg), G(t1), G(t2));
		state.push(d.kindFlagsAndTag(IN_REG), d.reg, 0);
	}

	private def visitITruncF(opcode: Opcode) {
		var xkind = if(opcode.sig.params[0] == ValueType.F32, ValueKind.F32, ValueKind.F64);
		var dkind = if(opcode.sig.results[0] == ValueType.I32, ValueKind.I32, ValueKind.I64);
		var xscratch = allocTmp(xkind);
		var x1 = popReg();
		var d = allocRegTos(dkind);
		spillRegAndFree(x1.reg); // XXX: macro assembler routine overwrites x1
		mmasm.emit_i_trunc_f(opcode, G(d), X(x1.reg), X(xscratch));
		state.push(SpcConsts.kindToFlags(dkind) | IN_REG, d, 0);
	}

	def visit_I32_WRAP_I64() {
		void(tryFold_l_i(i32.view<i64>)
			|| do_op1_r_r(ValueKind.I32, asm.movd_r_r));
	}
	def visit_I32_TRUNC_F32_S() { visitITruncF(Opcode.I32_TRUNC_F32_S); }
	def visit_I32_TRUNC_F32_U() { visitITruncF(Opcode.I32_TRUNC_F32_U); }
	def visit_I32_TRUNC_F64_S() { visitITruncF(Opcode.I32_TRUNC_F64_S); }
	def visit_I32_TRUNC_F64_U() { visitITruncF(Opcode.I32_TRUNC_F64_U); }
	def visit_I64_EXTEND_I32_S() {
		void(tryFold_i_l(i64.view<i32>)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_s));
	}
	def visit_I64_TRUNC_F32_S() { visitITruncF(Opcode.I64_TRUNC_F32_S); }
	def visit_I64_TRUNC_F32_U() { visitITruncF(Opcode.I64_TRUNC_F32_U); }
	def visit_I64_TRUNC_F64_S() { visitITruncF(Opcode.I64_TRUNC_F64_S); }
	def visit_I64_TRUNC_F64_U() { visitITruncF(Opcode.I64_TRUNC_F64_U); }
	def visit_I64_EXTEND_I32_U() {
		void(tryFold_u_l(i64.view<u32>)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_u));
	}
	def visitReinterpret(kind: ValueKind) {
		var sv = pop(), flags = SpcConsts.kindToFlags(kind);
		if (sv.isConst()) {
			state.push(flags | IS_CONST, NO_REG, sv.const);
		} else if (sv.inReg()) {
			var d = allocRegTos(kind);
			match (kind) {
				I32 => asm.movd_r_s(G(d), X(sv.reg));
				I64 => asm.movq_r_s(G(d), X(sv.reg));
				F32 => asm.movd_s_r(X(d), G(sv.reg));
				F64 => asm.movq_s_r(X(d), G(sv.reg));
				_ => bailout("unexpected value kind");
			}
			state.push(flags | IN_REG, d, 0);
		} else {
			state.push(flags | IS_STORED, NO_REG, 0);
		}
	}
	def visit_I32_REINTERPRET_F32() { visitReinterpret(ValueKind.I32); }
	def visit_I64_REINTERPRET_F64() { visitReinterpret(ValueKind.I64); }
	def visit_F32_REINTERPRET_I32() { visitReinterpret(ValueKind.F32); }
	def visit_F64_REINTERPRET_I64() { visitReinterpret(ValueKind.F64); }

	def visit_I32_EXTEND8_S() {
		void(tryFold_i_i(V3Eval.I32_EXTEND8_S)
			|| do_op1_r_r(ValueKind.I32, asm.d.movbsx_r_r));
	}
	def visit_I32_EXTEND16_S() {
		void(tryFold_i_i(V3Eval.I32_EXTEND16_S)
			|| do_op1_r_r(ValueKind.I32, asm.d.movwsx_r_r));
	}
	def visit_I64_EXTEND8_S() {
		void(tryFold_l_l(V3Eval.I64_EXTEND8_S)
			|| do_op1_r_r(ValueKind.I64, asm.q.movbsx_r_r));
	}
	def visit_I64_EXTEND16_S() {
		void(tryFold_l_l(V3Eval.I64_EXTEND16_S)
			|| do_op1_r_r(ValueKind.I64, asm.q.movwsx_r_r));
	}
	def visit_I64_EXTEND32_S() {
		void(tryFold_l_l(V3Eval.I64_EXTEND32_S)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_s));
	}
	def visit_REF_EQ()   {
		// XXX: constant fold and recombine with visitCompareI
		var b = popReg(), a = popReg();
		asm.cmp_r_r(G(a.reg), G(b.reg));
		var d = allocRegTos(ValueKind.I32), r1 = G(d);
		asm.set_r(C.Z, r1);
		asm.movbzx_r_r(r1, r1);
		state.push(KIND_I32 | IN_REG, d, 0);
	}

	def visit_I32_TRUNC_SAT_F32_S() { visitITruncF(Opcode.I32_TRUNC_SAT_F32_S); }
	def visit_I32_TRUNC_SAT_F32_U() { visitITruncF(Opcode.I32_TRUNC_SAT_F32_U); }
	def visit_I32_TRUNC_SAT_F64_S() { visitITruncF(Opcode.I32_TRUNC_SAT_F64_S); }
	def visit_I32_TRUNC_SAT_F64_U() { visitITruncF(Opcode.I32_TRUNC_SAT_F64_U); }
	def visit_I64_TRUNC_SAT_F32_S() { visitITruncF(Opcode.I64_TRUNC_SAT_F32_S); }
	def visit_I64_TRUNC_SAT_F32_U() { visitITruncF(Opcode.I64_TRUNC_SAT_F32_U); }
	def visit_I64_TRUNC_SAT_F64_S() { visitITruncF(Opcode.I64_TRUNC_SAT_F64_S); }
	def visit_I64_TRUNC_SAT_F64_U() { visitITruncF(Opcode.I64_TRUNC_SAT_F64_U); }

	private def visitFConvertI32S(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr) -> X86_64Assembler) {
		var sv = popTmpReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		asm.q.shl_r_i(r1, 32);
		asm.q.sar_r_i(r1, 32); // sign-extend
		emit_cvt(X(d), r1);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	def visit_F32_CONVERT_I32_S() { visitFConvertI32S(ValueKind.F32, asm.cvtsi2ss_s_r); }
	def visit_F64_CONVERT_I32_S() { visitFConvertI32S(ValueKind.F64, asm.cvtsi2sd_s_r); }
	private def visitFConvertI32U(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr) -> X86_64Assembler) {
		var sv = popTmpReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		asm.movd_r_r(r1, r1); // zero-extend
		emit_cvt(X(d), r1);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	def visit_F32_CONVERT_I32_U() { visitFConvertI32U(ValueKind.F32, asm.cvtsi2ss_s_r); }
	def visit_F64_CONVERT_I32_U() { visitFConvertI32U(ValueKind.F64, asm.cvtsi2sd_s_r); }
	private def visitFConvertI64S(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr) -> X86_64Assembler) {
		var sv = popReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		emit_cvt(X(d), r1);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	private def visitFConvertI64U(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr, X86_64Xmmr, X86_64Gpr) -> void) {
		var sv = popReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		var xscratch = allocTmp(kind);
		emit_cvt(X(d), r1, X(xscratch), mmasm.scratch);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	def visit_F32_CONVERT_I64_S() { visitFConvertI64S(ValueKind.F32, asm.cvtsi2ss_s_r); }
	def visit_F32_CONVERT_I64_U() { visitFConvertI64U(ValueKind.F32, mmasm.emit_f32_convert_i64_u); }
	def visit_F64_CONVERT_I64_S() { visitFConvertI64S(ValueKind.F64, asm.cvtsi2sd_s_r); }
	def visit_F64_CONVERT_I64_U() { visitFConvertI64U(ValueKind.F64, mmasm.emit_f64_convert_i64_u); }

	def visit_F32_DEMOTE_F64() { do_op1_x_x(ValueKind.F32, asm.cvtsd2ss_s_s); } // XXX: try s_m addr mode
	def visit_F64_PROMOTE_F32() { do_op1_x_x(ValueKind.F64, asm.cvtss2sd_s_s); } // XXX: try s_m addr mode

	def visit_V128_AND() {do_op2_x_x(ValueKind.V128, asm.andps_s_s); }
	def visit_V128_OR() { do_op2_x_x(ValueKind.V128, asm.orps_s_s); }
	def visit_V128_XOR() { do_op2_x_x(ValueKind.V128, asm.xorps_s_s); }

	def visit_V128_NOT() {
		var sv = popRegToOverwrite(), r = X(sv.reg);
		var tmp = allocTmp(ValueKind.V128), t = X(tmp);
		mmasm.emit_v128_not(r, t);
		state.push(sv.kindFlagsMatching(ValueKind.V128, IN_REG), sv.reg, 0);
	}

	def visit_V128_ANDNOT() { do_c_op2_x_x(ValueKind.V128, asm.andnps_s_s); }

	def visit_V128_ANYTRUE() { do_op1_r_x(ValueKind.I32, mmasm.emit_v128_anytrue); }
	def visit_I8X16_ALLTRUE() { do_op1_r_x(ValueKind.I32, mmasm.emit_i8x16_alltrue(_, _, X(allocTmp(ValueKind.V128)))); }
	def visit_I16X8_ALLTRUE() { do_op1_r_x(ValueKind.I32, mmasm.emit_i16x8_alltrue(_, _, X(allocTmp(ValueKind.V128)))); }
	def visit_I32X4_ALLTRUE() { do_op1_r_x(ValueKind.I32, mmasm.emit_i32x4_alltrue(_, _, X(allocTmp(ValueKind.V128)))); }
	def visit_I64X2_ALLTRUE() { do_op1_r_x(ValueKind.I32, mmasm.emit_i64x2_alltrue(_, _, X(allocTmp(ValueKind.V128)))); }
	def visit_I8X16_BITMASK() { do_op1_r_x(ValueKind.I32, asm.pmovmskb_r_s); }
	def visit_I16X8_BITMASK() { do_op1_r_x(ValueKind.I32, mmasm.emit_i16x8_bitmask); }
	def visit_I32X4_BITMASK() { do_op1_r_x(ValueKind.I32, asm.movmskps_r_s); }
	def visit_I64X2_BITMASK() { do_op1_r_x(ValueKind.I32, asm.movmskpd_r_s); }

	def visit_I8X16_SHL() { visit_V128_SHIFT1(mmasm.emit_i8x16_shl); }
	def visit_I8X16_SHR_S() { visit_V128_SHIFT1(mmasm.emit_i8x16_shr_s); }
	def visit_I8X16_SHR_U() { visit_V128_SHIFT1(mmasm.emit_i8x16_shr_u); }
	def visit_I8X16_ADD() { do_op2_x_x(ValueKind.V128, asm.paddb_s_s); }
	def visit_I8X16_SUB() { do_op2_x_x(ValueKind.V128, asm.psubb_s_s); }
	def visit_I8X16_NEG() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_i8x16_neg); }
	def visit_I8X16_EQ() { do_op2_x_x(ValueKind.V128, asm.pcmpeqb_s_s); }
	def visit_I8X16_NE() { do_op2_x_x(ValueKind.V128, mmasm.emit_i8x16_ne); }
	def visit_I8X16_GT_S() { do_op2_x_x(ValueKind.V128, asm.pcmpgtb_s_s); }
	def visit_I8X16_GT_U() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_i8x16_gt_u); }
	def visit_I8X16_LT_S() { do_c_op2_x_x(ValueKind.V128, asm.pcmpgtb_s_s); }
	def visit_I8X16_LT_U() { do_c_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_i8x16_gt_u); }
	def visit_I8X16_GE_S() { do_op2_x_x(ValueKind.V128, mmasm.emit_i8x16_ge_s); }
	def visit_I8X16_GE_U() { do_op2_x_x(ValueKind.V128, mmasm.emit_i8x16_ge_u); }
	def visit_I8X16_LE_S() { do_c_op2_x_x(ValueKind.V128, mmasm.emit_i8x16_ge_s); }
	def visit_I8X16_LE_U() { do_c_op2_x_x(ValueKind.V128, mmasm.emit_i8x16_ge_u); }
	def visit_I8X16_MIN_S() { do_op2_x_x(ValueKind.V128, asm.pminsb_s_s); }
	def visit_I8X16_MIN_U() { do_op2_x_x(ValueKind.V128, asm.pminub_s_s); }
	def visit_I8X16_MAX_S() { do_op2_x_x(ValueKind.V128, asm.pmaxsb_s_s); }
	def visit_I8X16_MAX_U() { do_op2_x_x(ValueKind.V128, asm.pmaxub_s_s); }
	def visit_I8X16_AVGR_U() { do_op2_x_x(ValueKind.V128, asm.pavgb_s_s); }
	def visit_I8X16_ABS() { do_op1_x_x(ValueKind.V128, asm.pabsb_s_s); }
	def visit_I8X16_POPCNT() { do_op1_x(ValueKind.V128, mmasm.emit_i8x16_popcnt(_, G(allocTmp(ValueKind.I64)), X(allocTmp(ValueKind.V128)), X(allocTmp(ValueKind.V128)), X(allocTmp(ValueKind.V128)))); }
	def visit_I8X16_ADD_SAT_S() { do_op2_x_x(ValueKind.V128, asm.paddsb_s_s); }
	def visit_I8X16_ADD_SAT_U() { do_op2_x_x(ValueKind.V128, asm.paddusb_s_s); }
	def visit_I8X16_SUB_SAT_S() { do_op2_x_x(ValueKind.V128, asm.psubsb_s_s); }
	def visit_I8X16_SUB_SAT_U() { do_op2_x_x(ValueKind.V128, asm.psubusb_s_s); }
	def visit_I8X16_NARROW_I16X8_S() { do_op2_x_x(ValueKind.V128, asm.packsswb_s_s); }
	def visit_I8X16_NARROW_I16X8_U() { do_op2_x_x(ValueKind.V128, asm.packuswb_s_s); }

	def visit_I16X8_SHL() { visit_V128_SHIFT2(4, asm.psllw_s_s); }
	def visit_I16X8_SHR_S() {visit_V128_SHIFT2(4, asm.psraw_s_s); }
	def visit_I16X8_SHR_U() { visit_V128_SHIFT2(4, asm.psrlw_s_s); }
	def visit_I16X8_ADD() { do_op2_x_x(ValueKind.V128, asm.paddw_s_s); }
	def visit_I16X8_SUB() { do_op2_x_x(ValueKind.V128, asm.psubw_s_s); }
	def visit_I16X8_MUL() { do_op2_x_x(ValueKind.V128, asm.pmullw_s_s); }
	def visit_I16X8_NEG() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_i16x8_neg); }
	def visit_I16X8_EQ() { do_op2_x_x(ValueKind.V128, asm.pcmpeqw_s_s); }
	def visit_I16X8_NE() { do_op2_x_x(ValueKind.V128, mmasm.emit_i16x8_ne); }
	def visit_I16X8_GT_S() { do_op2_x_x(ValueKind.V128, asm.pcmpgtw_s_s); }
	def visit_I16X8_GT_U() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_i16x8_gt_u); }
	def visit_I16X8_LT_S() { do_c_op2_x_x(ValueKind.V128, asm.pcmpgtw_s_s); }
	def visit_I16X8_LT_U() { do_c_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_i16x8_gt_u); }
	def visit_I16X8_GE_S() { do_op2_x_x(ValueKind.V128, mmasm.emit_i16x8_ge_s); }
	def visit_I16X8_GE_U() { do_op2_x_x(ValueKind.V128, mmasm.emit_i16x8_ge_u); }
	def visit_I16X8_LE_S() { do_c_op2_x_x(ValueKind.V128, mmasm.emit_i16x8_ge_s); }
	def visit_I16X8_LE_U() { do_c_op2_x_x(ValueKind.V128, mmasm.emit_i16x8_ge_u); }
	def visit_I16X8_MIN_S() { do_op2_x_x(ValueKind.V128, asm.pminsw_s_s); }
	def visit_I16X8_MIN_U() { do_op2_x_x(ValueKind.V128, asm.pminuw_s_s); }
	def visit_I16X8_MAX_S() { do_op2_x_x(ValueKind.V128, asm.pmaxsw_s_s); }
	def visit_I16X8_MAX_U() { do_op2_x_x(ValueKind.V128, asm.pmaxuw_s_s); }
	def visit_I16X8_AVGR_U() { do_op2_x_x(ValueKind.V128, asm.pavgw_s_s); }
	def visit_I16X8_ABS() { do_op1_x_x(ValueKind.V128, asm.pabsw_s_s); }
	def visit_I16X8_ADD_SAT_S() { do_op2_x_x(ValueKind.V128, asm.paddsw_s_s); }
	def visit_I16X8_ADD_SAT_U() { do_op2_x_x(ValueKind.V128, asm.paddusw_s_s); }
	def visit_I16X8_SUB_SAT_S() { do_op2_x_x(ValueKind.V128, asm.psubsw_s_s); }
	def visit_I16X8_SUB_SAT_U() { do_op2_x_x(ValueKind.V128, asm.psubusw_s_s); }
	def visit_I16X8_EXTADDPAIRWISE_I8X16_S() { do_op1_x_gtmp_xtmp(ValueKind.V128, mmasm.emit_i16x8_extadd_pairwise_i8x16_s); }
	def visit_I16X8_EXTADDPAIRWISE_I8X16_U() { do_op1_x_gtmp_xtmp(ValueKind.V128, mmasm.emit_i16x8_extadd_pairwise_i8x16_u); }
	def visit_I16X8_EXTMUL_LOW_I8X16_S() { visit_V128_EXTMUL2(ValueKind.V128, mmasm.emit_i16x8_extmul_low, true); }
	def visit_I16X8_EXTMUL_LOW_I8X16_U() { visit_V128_EXTMUL2(ValueKind.V128, mmasm.emit_i16x8_extmul_low, false); }
	def visit_I16X8_EXTMUL_HIGH_I8X16_S() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_i16x8_extmul_high_s); }
	def visit_I16X8_EXTMUL_HIGH_I8X16_U() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_i16x8_extmul_high_u); }
	def visit_I16X8_Q15MULRSAT_S() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_i16x8_q15mulrsat_s); }
	def visit_I16X8_RELAXED_Q15MULR_S() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_i16x8_q15mulrsat_s); }
	def visit_I16X8_NARROW_I32X4_S() { do_op2_x_x(ValueKind.V128, asm.packssdw_s_s); }
	def visit_I16X8_NARROW_I32X4_U() { do_op2_x_x(ValueKind.V128, asm.packusdw_s_s); }
	def visit_I16X8_EXTEND_LOW_I8X16_S() { do_op1_x_x(ValueKind.V128, asm.pmovsxbw_s_s); }
	def visit_I16X8_EXTEND_LOW_I8X16_U() { do_op1_x_x(ValueKind.V128, asm.pmovzxbw_s_s); }
	def visit_I16X8_EXTEND_HIGH_I8X16_S() { do_op1_x(ValueKind.V128, mmasm.emit_i16x8_s_convert_i8x16_high); }
	def visit_I16X8_EXTEND_HIGH_I8X16_U() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_i16x8_u_convert_i8x16_high); }

	def visit_I32X4_SHL() { visit_V128_SHIFT2(5, asm.pslld_s_s); }
	def visit_I32X4_SHR_S() { visit_V128_SHIFT2(5, asm.psrad_s_s); }
	def visit_I32X4_SHR_U() { visit_V128_SHIFT2(5, asm.psrld_s_s); }
	def visit_I32X4_ADD() { do_op2_x_x(ValueKind.V128, asm.paddd_s_s); }
	def visit_I32X4_SUB() { do_op2_x_x(ValueKind.V128, asm.psubd_s_s); }
	def visit_I32X4_MUL() { do_op2_x_x(ValueKind.V128, asm.pmulld_s_s); }
	def visit_I32X4_NEG() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_i32x4_neg); }
	def visit_I32X4_EQ() { do_op2_x_x(ValueKind.V128, asm.pcmpeqd_s_s); }
	def visit_I32X4_NE() { do_op2_x_x(ValueKind.V128, mmasm.emit_i32x4_ne); }
	def visit_I32X4_GT_S() { do_op2_x_x(ValueKind.V128, asm.pcmpgtd_s_s); }
	def visit_I32X4_GT_U() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_i32x4_gt_u); }
	def visit_I32X4_LT_S() { do_c_op2_x_x(ValueKind.V128, asm.pcmpgtd_s_s); }
	def visit_I32X4_LT_U() { do_c_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_i32x4_gt_u); }
	def visit_I32X4_GE_S() { do_op2_x_x(ValueKind.V128, mmasm.emit_i32x4_ge_s); }
	def visit_I32X4_GE_U() { do_op2_x_x(ValueKind.V128, mmasm.emit_i32x4_ge_u); }
	def visit_I32X4_LE_S() { do_c_op2_x_x(ValueKind.V128, mmasm.emit_i32x4_ge_s); }
	def visit_I32X4_LE_U() { do_c_op2_x_x(ValueKind.V128, mmasm.emit_i32x4_ge_u); }
	def visit_I32X4_MIN_S() { do_op2_x_x(ValueKind.V128, asm.pminsd_s_s); }
	def visit_I32X4_MIN_U() { do_op2_x_x(ValueKind.V128, asm.pminud_s_s); }
	def visit_I32X4_MAX_S() { do_op2_x_x(ValueKind.V128, asm.pmaxsd_s_s); }
	def visit_I32X4_MAX_U() { do_op2_x_x(ValueKind.V128, asm.pmaxud_s_s); }
	def visit_I32X4_ABS() { do_op1_x_x(ValueKind.V128, asm.pabsd_s_s); }
	def visit_I32X4_DOT_I16X8_S() { do_op2_x_x(ValueKind.V128, asm.pmaddwd_s_s); }
	def visit_I32X4_EXTADDPAIRWISE_I16X8_S() { do_op1_x_gtmp_xtmp(ValueKind.V128, mmasm.emit_i32x4_extadd_pairwise_i16x8_s); }
	def visit_I32X4_EXTADDPAIRWISE_I16X8_U() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_i32x4_extadd_pairwise_i16x8_u); }
	def visit_I32X4_EXTMUL_LOW_I16X8_S() { visit_V128_EXTMUL1(ValueKind.V128, mmasm.emit_i32x4_extmul, true, true); }
	def visit_I32X4_EXTMUL_LOW_I16X8_U() { visit_V128_EXTMUL1(ValueKind.V128, mmasm.emit_i32x4_extmul, true, false); }
	def visit_I32X4_EXTMUL_HIGH_I16X8_S() { visit_V128_EXTMUL1(ValueKind.V128, mmasm.emit_i32x4_extmul, false, true); }
	def visit_I32X4_EXTMUL_HIGH_I16X8_U() { visit_V128_EXTMUL1(ValueKind.V128, mmasm.emit_i32x4_extmul, false, false); }
	def visit_I32X4_RELAXED_TRUNC_F32X4_S() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_i32x4_trunc_sat_f32x4_s); }
	def visit_I32X4_TRUNC_SAT_F32X4_S() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_i32x4_trunc_sat_f32x4_s); }
	def visit_I32X4_RELAXED_TRUNC_F32X4_U() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_i32x4_trunc_sat_f32x4_u(_, _, X(allocTmp(ValueKind.V128)))); }
	def visit_I32X4_TRUNC_SAT_F32X4_U() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_i32x4_trunc_sat_f32x4_u(_, _, X(allocTmp(ValueKind.V128)))); }
	def visit_I32X4_RELAXED_TRUNC_F64X2_S_ZERO() { do_op1_x_gtmp_xtmp(ValueKind.V128, mmasm.emit_i32x4_trunc_sat_f64x2_s_zero(_, _, _, X(allocTmp(ValueKind.V128)))); }
	def visit_I32X4_TRUNC_SAT_F64X2_S_ZERO() { do_op1_x_gtmp_xtmp(ValueKind.V128, mmasm.emit_i32x4_trunc_sat_f64x2_s_zero(_, _, _, X(allocTmp(ValueKind.V128)))); }
	def visit_I32X4_RELAXED_TRUNC_F64X2_U_ZERO() { do_op1_x_gtmp_xtmp(ValueKind.V128, mmasm.emit_i32x4_trunc_sat_f64x2_u_zero(_, _, _, X(allocTmp(ValueKind.V128)))); }
	def visit_I32X4_TRUNC_SAT_F64X2_U_ZERO() { do_op1_x_gtmp_xtmp(ValueKind.V128, mmasm.emit_i32x4_trunc_sat_f64x2_u_zero(_, _, _, X(allocTmp(ValueKind.V128)))); }
	def visit_I32X4_EXTEND_LOW_I16X8_S() { do_op1_x_x(ValueKind.V128, asm.pmovsxwd_s_s); }
	def visit_I32X4_EXTEND_LOW_I16X8_U() { do_op1_x_x(ValueKind.V128, asm.pmovzxwd_s_s); }
	def visit_I32X4_EXTEND_HIGH_I16X8_S() { do_op1_x(ValueKind.V128, mmasm.emit_i32x4_s_convert_i16x8_high); }
	def visit_I32X4_EXTEND_HIGH_I16X8_U() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_i32x4_u_convert_i16x8_high); }

	def visit_I64X2_SHL() { visit_V128_SHIFT2(6, asm.psllq_s_s); }
	def visit_I64X2_SHR_S() { visit_V128_SHIFT1(mmasm.emit_i64x2_shr_s); }
	def visit_I64X2_SHR_U() { visit_V128_SHIFT2(6, asm.psrlq_s_s); }
	def visit_I64X2_ADD() { do_op2_x_x(ValueKind.V128, asm.paddq_s_s); }
	def visit_I64X2_SUB() { do_op2_x_x(ValueKind.V128, asm.psubq_s_s); }
	def visit_I64X2_MUL() { do_op2_x_x(ValueKind.V128, mmasm.emit_i64x2_mul(_, _, X(allocTmp(ValueKind.V128)), X(allocTmp(ValueKind.V128)))); }
	def visit_I64X2_NEG() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_i64x2_neg); }
	def visit_I64X2_EQ() { do_op2_x_x(ValueKind.V128, asm.pcmpeqq_s_s); }
	def visit_I64X2_NE() { do_op2_x_x(ValueKind.V128, mmasm.emit_i64x2_ne); }
	def visit_I64X2_GT_S() { do_op2_x_x(ValueKind.V128, asm.pcmpgtq_s_s); }
	def visit_I64X2_LT_S() { do_c_op2_x_x(ValueKind.V128, asm.pcmpgtq_s_s); }
	def visit_I64X2_GE_S() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_i64x2_ge_s); }
	def visit_I64X2_LE_S() { do_c_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_i64x2_ge_s); }
	def visit_I64X2_ABS() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_i64x2_abs); }
	def visit_I64X2_EXTMUL_LOW_I32X4_S() { visit_V128_EXTMUL1(ValueKind.V128, mmasm.emit_i64x2_extmul, true, true); }
	def visit_I64X2_EXTMUL_LOW_I32X4_U() { visit_V128_EXTMUL1(ValueKind.V128, mmasm.emit_i64x2_extmul, true, false); }
	def visit_I64X2_EXTMUL_HIGH_I32X4_S() { visit_V128_EXTMUL1(ValueKind.V128, mmasm.emit_i64x2_extmul, false, true); }
	def visit_I64X2_EXTMUL_HIGH_I32X4_U() { visit_V128_EXTMUL1(ValueKind.V128, mmasm.emit_i64x2_extmul, false, false); }
	def visit_I64X2_EXTEND_LOW_I32X4_S() { do_op1_x_x(ValueKind.V128, asm.pmovsxdq_s_s); }
	def visit_I64X2_EXTEND_LOW_I32X4_U() { do_op1_x_x(ValueKind.V128, asm.pmovzxdq_s_s); }
	def visit_I64X2_EXTEND_HIGH_I32X4_S() { do_op1_x(ValueKind.V128, mmasm.emit_i64x2_s_convert_i32x4_high); }
	def visit_I64X2_EXTEND_HIGH_I32X4_U() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_i64x2_u_convert_i32x4_high); }

	def visit_F32X4_ADD() { do_op2_x_x(ValueKind.V128, asm.addps_s_s); }
	def visit_F32X4_SUB() { do_op2_x_x(ValueKind.V128, asm.subps_s_s); }
	def visit_F32X4_MUL() { do_op2_x_x(ValueKind.V128, asm.mulps_s_s); }
	def visit_F32X4_DIV() { do_op2_x_x(ValueKind.V128, asm.divps_s_s); }
	def visit_F32X4_NEG() { do_op1_x_gtmp_xtmp(ValueKind.V128, mmasm.emit_v128_negps); }
	def visit_F32X4_SQRT() { do_op1_x_x(ValueKind.V128, asm.sqrtps_s_s); }
	def visit_F32X4_EQ() { do_op2_x_x(ValueKind.V128, asm.cmpeqps_s_s); }
	def visit_F32X4_NE() { do_op2_x_x(ValueKind.V128, asm.cmpneqps_s_s); }
	def visit_F32X4_GT() { do_c_op2_x_x(ValueKind.V128, asm.cmpltps_s_s); }
	def visit_F32X4_LT() { do_op2_x_x(ValueKind.V128, asm.cmpltps_s_s); }
	def visit_F32X4_GE() { do_c_op2_x_x(ValueKind.V128, asm.cmpleps_s_s); }
	def visit_F32X4_LE() { do_op2_x_x(ValueKind.V128, asm.cmpleps_s_s); }
	def visit_F32X4_PMIN() { do_c_op2_x_x(ValueKind.V128, asm.minps_s_s); }
	def visit_F32X4_PMAX() { do_c_op2_x_x(ValueKind.V128, asm.maxps_s_s); }
	def visit_F32X4_MIN() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_f32x4_min); }
	def visit_F32X4_RELAXED_MIN() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_f32x4_min); }
	def visit_F32X4_MAX() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_f32x4_max); }
	def visit_F32X4_RELAXED_MAX() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_f32x4_max); }
	def visit_F32X4_ABS() { do_op1_x_gtmp_xtmp(ValueKind.V128, mmasm.emit_v128_absps); }
	def visit_F32X4_CEIL() { do_op1_x_x(ValueKind.V128, asm.roundps_s_s(_, _, X86_64Rounding.TO_POS_INF)); }
	def visit_F32X4_FLOOR() { do_op1_x_x(ValueKind.V128, asm.roundps_s_s(_, _, X86_64Rounding.TO_NEG_INF)); }
	def visit_F32X4_TRUNC() { do_op1_x_x(ValueKind.V128, asm.roundps_s_s(_, _, X86_64Rounding.TO_ZERO)); }
	def visit_F32X4_NEAREST() { do_op1_x_x(ValueKind.V128, asm.roundps_s_s(_, _, X86_64Rounding.TO_NEAREST)); }
	def visit_F32X4_CONVERT_I32X4_S() { do_op1_x_x(ValueKind.V128, asm.cvtdq2ps_s_s); }
	def visit_F32X4_CONVERT_I32X4_U() { do_op1_x_xtmp(ValueKind.V128, mmasm.emit_f32x4_convert_i32x4_u); }
	def visit_F32X4_DEMOTE_F64X2_ZERO() { do_op1_x_x(ValueKind.V128, asm.cvtpd2ps_s_s); }

	def visit_F64X2_ADD() { do_op2_x_x(ValueKind.V128, asm.addpd_s_s); }
	def visit_F64X2_SUB() { do_op2_x_x(ValueKind.V128, asm.subpd_s_s); }
	def visit_F64X2_MUL() { do_op2_x_x(ValueKind.V128, asm.mulpd_s_s); }
	def visit_F64X2_DIV() { do_op2_x_x(ValueKind.V128, asm.divpd_s_s); }
	def visit_F64X2_NEG() { do_op1_x_gtmp_xtmp(ValueKind.V128, mmasm.emit_v128_negpd); }
	def visit_F64X2_SQRT() { do_op1_x_x(ValueKind.V128, asm.sqrtpd_s_s); }
	def visit_F64X2_EQ() { do_op2_x_x(ValueKind.V128, asm.cmpeqpd_s_s); }
	def visit_F64X2_NE() { do_op2_x_x(ValueKind.V128, asm.cmpneqpd_s_s); }
	def visit_F64X2_GT() { do_c_op2_x_x(ValueKind.V128, asm.cmpltpd_s_s); }
	def visit_F64X2_LT() { do_op2_x_x(ValueKind.V128, asm.cmpltpd_s_s); }
	def visit_F64X2_GE() { do_c_op2_x_x(ValueKind.V128, asm.cmplepd_s_s); }
	def visit_F64X2_LE() { do_op2_x_x(ValueKind.V128, asm.cmplepd_s_s); }
	def visit_F64X2_PMIN() { do_c_op2_x_x(ValueKind.V128, asm.minpd_s_s); }
	def visit_F64X2_PMAX() { do_c_op2_x_x(ValueKind.V128, asm.maxpd_s_s); }
	def visit_F64X2_MIN() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_f64x2_min); }
	def visit_F64X2_RELAXED_MIN() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_f64x2_min); }
	def visit_F64X2_MAX() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_f64x2_max); }
	def visit_F64X2_RELAXED_MAX() { do_op2_x_x_xtmp(ValueKind.V128, mmasm.emit_f64x2_max); }
	def visit_F64X2_ABS() { do_op1_x_gtmp_xtmp(ValueKind.V128, mmasm.emit_v128_abspd); }
	def visit_F64X2_CEIL() { do_op1_x_x(ValueKind.V128, asm.roundpd_s_s(_, _, X86_64Rounding.TO_POS_INF)); }
	def visit_F64X2_FLOOR() { do_op1_x_x(ValueKind.V128, asm.roundpd_s_s(_, _, X86_64Rounding.TO_NEG_INF)); }
	def visit_F64X2_TRUNC() { do_op1_x_x(ValueKind.V128, asm.roundpd_s_s(_, _, X86_64Rounding.TO_ZERO)); }
	def visit_F64X2_NEAREST() { do_op1_x_x(ValueKind.V128, asm.roundpd_s_s(_, _, X86_64Rounding.TO_NEAREST)); }
	def visit_F64X2_CONVERT_LOW_I32X4_S() { do_op1_x_x(ValueKind.V128, asm.cvtdq2pd_s_s); }
	def visit_F64X2_CONVERT_LOW_I32X4_U() { do_op1_x_gtmp_xtmp(ValueKind.V128, mmasm.emit_f64x2_convert_low_i32x4_u); }
	def visit_F64X2_PROMOTE_LOW_F32X4() { do_op1_x_x(ValueKind.V128, asm.cvtps2pd_s_s); }

	def doMultiplyAdd(is64: bool, isNeg: bool) {
		var mc = popReg().reg, c = X(mc);
		var mb = popReg().reg, b = X(mb);
		var ma = popReg().reg, a = X(ma);
		// XXX: avoid extra moves by careful shuffling of regs
		var mt = allocTmp(ValueKind.V128), t = X(mt);
		asm.movaps_s_s(t, a);
		var mr = allocRegTos(ValueKind.V128), r = X(mr);
		asm.movaps_s_s(r, c);
		if (is64) {
			asm.mulpd_s_s(t, b);
			if (isNeg) asm.subpd_s_s(r, t);
			else asm.addpd_s_s(r, t);
		} else {
			asm.mulps_s_s(t, b);
			if (isNeg) asm.subps_s_s(r, t);
			else asm.addps_s_s(r, t);
		}
		state.push(KIND_V128 | IN_REG, mr, 0);
	}

	def visit_F32X4_RELAXED_MADD()	{ doMultiplyAdd(false, false); }
	def visit_F32X4_RELAXED_NMADD()	{ doMultiplyAdd(false, true); }
	def visit_F64X2_RELAXED_MADD()	{ doMultiplyAdd(true, false); }
	def visit_F64X2_RELAXED_NMADD()	{ doMultiplyAdd(true, true); }

	def visit_V128_LOAD_8_LANE(imm: MemArg, lane: byte) { visit_V128_LOAD_LANE(imm, lane, loadMemarg_b, asm.pinsrb_s_r_i); }
	def visit_V128_LOAD_16_LANE(imm: MemArg, lane: byte) { visit_V128_LOAD_LANE(imm, lane, loadMemarg_w, asm.pinsrw_s_r_i); }
	def visit_V128_LOAD_32_LANE(imm: MemArg, lane: byte) { visit_V128_LOAD_LANE(imm, lane, loadMemarg_d, asm.pinsrd_s_r_i); }
	def visit_V128_LOAD_64_LANE(imm: MemArg, lane: byte) { visit_V128_LOAD_LANE(imm, lane, loadMemarg_q, asm.pinsrq_s_r_i); }
	def visit_V128_LOAD_8X8_S(imm: MemArg) { visit_V128_LOAD_EXTEND(imm, asm.pmovsxbw_s_m); }
	def visit_V128_LOAD_8X8_U(imm: MemArg) { visit_V128_LOAD_EXTEND(imm, asm.pmovzxbw_s_m); }
	def visit_V128_LOAD_16X4_S(imm: MemArg) { visit_V128_LOAD_EXTEND(imm, asm.pmovsxwd_s_m); }
	def visit_V128_LOAD_16X4_U(imm: MemArg) { visit_V128_LOAD_EXTEND(imm, asm.pmovzxwd_s_m); }
	def visit_V128_LOAD_32X2_S(imm: MemArg) { visit_V128_LOAD_EXTEND(imm, asm.pmovsxdq_s_m); }
	def visit_V128_LOAD_32X2_U(imm: MemArg) { visit_V128_LOAD_EXTEND(imm, asm.pmovzxdq_s_m); }
	def visit_V128_LOAD_32_ZERO(imm: MemArg) { visit_V128_LOAD_ZERO(imm, loadMemarg_d, asm.pinsrd_s_r_i); }
	def visit_V128_LOAD_64_ZERO(imm: MemArg) { visit_V128_LOAD_ZERO(imm, loadMemarg_q, asm.pinsrq_s_r_i); }
	def visit_V128_LOAD_8_SPLAT(imm: MemArg) { visit_V128_LOAD_SPLAT(imm, loadMemarg_b, mmasm.emit_i8x16_splat(_, _, X(allocTmp(ValueKind.V128)))); }
	def visit_V128_LOAD_16_SPLAT(imm: MemArg) { visit_V128_LOAD_SPLAT(imm, loadMemarg_w, mmasm.emit_i16x8_splat); }
	def visit_V128_LOAD_32_SPLAT(imm: MemArg) { visit_V128_LOAD_SPLAT(imm, loadMemarg_d, mmasm.emit_i32x4_splat); }
	def visit_V128_LOAD_64_SPLAT(imm: MemArg) { visit_V128_LOAD_SPLAT(imm, loadMemarg_q, mmasm.emit_i64x2_splat); }

	def visit_I8X16_REPLACE_LANE(lane: byte) { visit_V128_REPLACE_LANE(lane, asm.pinsrb_s_r_i); }
	def visit_I16X8_REPLACE_LANE(lane: byte) { visit_V128_REPLACE_LANE(lane, asm.pinsrw_s_r_i); }
	def visit_I32X4_REPLACE_LANE(lane: byte) { visit_V128_REPLACE_LANE(lane, asm.pinsrd_s_r_i); }
	def visit_I64X2_REPLACE_LANE(lane: byte) { visit_V128_REPLACE_LANE(lane, asm.pinsrq_s_r_i); }
	def visit_F32X4_REPLACE_LANE(lane: byte) {
		var b = popReg();
		var a = popRegToOverwrite();
		asm.insertps_s_s_i(X(a.reg), X(b.reg), byte.view((lane << 4) & 0x30));
		state.push(a.kindFlagsMatching(ValueKind.V128, IN_REG), a.reg, 0);
	}
	def visit_F64X2_REPLACE_LANE(lane: byte) {
		var b = popReg();
		var a = popRegToOverwrite();
		if (lane == 0) {
			asm.movsd_s_s(X(a.reg), X(b.reg));
		} else {
			asm.movlhps_s_s(X(a.reg), X(b.reg));
		}
		state.push(a.kindFlagsMatching(ValueKind.V128, IN_REG), a.reg, 0);
	}

	def visit_I8X16_EXTRACT_LANE_S(lane: byte) { visit_V128_EXTRACT_LANE_S(ValueKind.I32, lane, asm.pextrb_r_s_i, asm.q.movbsx_r_r); }
	def visit_I8X16_EXTRACT_LANE_U(lane: byte) { visit_V128_EXTRACT_LANE(ValueKind.I32, lane, asm.pextrb_r_s_i); }
	def visit_I16X8_EXTRACT_LANE_S(lane: byte) { visit_V128_EXTRACT_LANE_S(ValueKind.I32, lane, asm.pextrw_r_s_i, asm.q.movwsx_r_r); }
	def visit_I16X8_EXTRACT_LANE_U(lane: byte) { visit_V128_EXTRACT_LANE(ValueKind.I32, lane, asm.pextrw_r_s_i); }
	def visit_I32X4_EXTRACT_LANE(lane: byte) { visit_V128_EXTRACT_LANE(ValueKind.I32, lane, asm.pextrd_r_s_i); }
	def visit_I64X2_EXTRACT_LANE(lane: byte) { visit_V128_EXTRACT_LANE(ValueKind.I64, lane, asm.pextrq_r_s_i); }
	def visit_F32X4_EXTRACT_LANE(lane: byte) {
		var a = popReg(), r = X(a.reg);
		var kind = ValueKind.F32;
		var d = allocRegTos(kind);
		if (lane == 0) {
			asm.movss_s_s(X(d), r);
		} else if (lane == 1) {
			asm.movshdup_s_s(X(d), r);
		} else if (lane == 2) {
			asm.movhlps_s_s(X(d), r);
		} else {
			asm.pshufd_s_s_i(X(d), r, lane);
		}
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	def visit_F64X2_EXTRACT_LANE(lane: byte) {
		var a = popReg(), r = X(a.reg);
		var kind = ValueKind.F64;
		var d = allocRegTos(kind);
		if (lane == 0) {
			asm.movsd_s_s(X(d), r);
		} else {
			asm.movhlps_s_s(X(d), r);
		}
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}

	def visit_V128_STORE_8_LANE(imm: MemArg, lane: byte) { visit_V128_STORE_LANE(imm, lane, storeMemarg_b, asm.pextrb_r_s_i); }
	def visit_V128_STORE_16_LANE(imm: MemArg, lane: byte) { visit_V128_STORE_LANE(imm, lane, storeMemarg_w, asm.pextrw_r_s_i); }
	def visit_V128_STORE_32_LANE(imm: MemArg, lane: byte) { visit_V128_STORE_LANE(imm, lane, storeMemarg_d, asm.pextrd_r_s_i); }
	def visit_V128_STORE_64_LANE(imm: MemArg, lane: byte) { visit_V128_STORE_LANE(imm, lane, storeMemarg_q, asm.pextrq_r_s_i); }

	def visit_I8X16_SPLAT() { visit_V128_SPLAT_I(mmasm.emit_i8x16_splat(_, _, X(allocTmp(ValueKind.V128)))); }
	def visit_I16X8_SPLAT() { visit_V128_SPLAT_I(mmasm.emit_i16x8_splat); }
	def visit_I32X4_SPLAT() { visit_V128_SPLAT_I(mmasm.emit_i32x4_splat); }
	def visit_I64X2_SPLAT() { visit_V128_SPLAT_I(mmasm.emit_i64x2_splat); }
	def visit_F32X4_SPLAT() { visit_V128_SPLAT_F(asm.pshufd_s_s_i(_, _, 0)); }
	def visit_F64X2_SPLAT() { visit_V128_SPLAT_F(asm.movddup_s_s); }

	private def visitSwizzle() {
		do_op2_x_x(ValueKind.V128, mmasm.emit_i8x16_swizzle(_, _,
				G(allocTmp(ValueKind.I64)), X(allocTmp(ValueKind.V128))));
	}
	def visit_I8X16_SWIZZLE() { visitSwizzle(); }
	def visit_I8X16_RELAXED_SWIZZLE() { visitSwizzle(); }

	def visit_I8X16_SHUFFLE(imms: Array<byte>) {
		var b = popReg();
		var a = popRegToOverwrite();
		var gtmp = G(allocTmp(ValueKind.I64));
		var xtmp_mask = X(allocTmp(ValueKind.V128));

		def SIMD_128_SIZE: byte = 16;
		var mask1: Array<u64> = [0x0, 0x0];
		var mask2: Array<u64> = [0x0, 0x0];
		for (i = 15; i >= 0; i--) {
			def lane = imms[i];
			def j: int = i >> 3;
			// Construct a mask for a
			mask1[j] <<= 8;
			mask1[j] |= if (lane < SIMD_128_SIZE, lane, 0x80);
			// Construct a mask for b
			mask2[j] <<= 8;
			mask2[j] |= if (lane >= SIMD_128_SIZE, byte.view(lane & 0x0F), 0x80);
		}
		// Extract bytes from a
		mmasm.load_v128_mask(xtmp_mask, (mask1[0], mask1[1]), gtmp);
		asm.pshufb_s_s(X(a.reg), xtmp_mask);
		// Extract bytes from b
		mmasm.load_v128_mask(xtmp_mask, (mask2[0], mask2[1]), gtmp);
		asm.pshufb_s_s(X(b.reg), xtmp_mask);
		// Merge the two results
		asm.orps_s_s(X(a.reg), X(b.reg));
		state.push(a.kindFlagsMatching(ValueKind.V128, IN_REG), a.reg, 0);
	}

	def visit_V128_BITSELECT() {
		var c = popReg();
		var b = popReg();
		var a = popRegToOverwrite();
		var t = allocTmp(ValueKind.V128);
		mmasm.emit_v128_bitselect(X(a.reg), X(b.reg), X(c.reg), X(t));
		state.push(a.kindFlagsMatching(ValueKind.V128, IN_REG), a.reg, 0);
	}

	private def visit_V128_SHIFT1<T>(masm_shift: (X86_64Xmmr, X86_64Gpr, X86_64Gpr, X86_64Xmmr, X86_64Xmmr) -> T) {
		var b = popReg();
		var a = popRegToOverwrite();
		var gtmp = G(allocTmp(ValueKind.I64));
		var xtmp0 = X(allocTmp(ValueKind.V128));
		var xtmp1 = X(allocTmp(ValueKind.V128));
		masm_shift(X(a.reg), G(b.reg), gtmp, xtmp0, xtmp1);
		state.push(a.kindFlagsMatching(ValueKind.V128, IN_REG), a.reg, 0);
	}

	private def visit_V128_SHIFT2<T>(width: byte, asm_shift: (X86_64Xmmr, X86_64Xmmr) -> T) {
		var b = popReg();
		var a = popRegToOverwrite();
		var gtmp = G(allocTmp(ValueKind.I64));
		var xtmp = X(allocTmp(ValueKind.V128));
		mmasm.emit_v128_shift(X(a.reg), G(b.reg), width, gtmp, xtmp, asm_shift);
		state.push(a.kindFlagsMatching(ValueKind.V128, IN_REG), a.reg, 0);
	}

	// Decode memarg and return the mem address and trap reason if any
	private def decodeMemarg(imm: MemArg) -> (X86_64Addr, TrapReason) {
		var base_reg = regs.mem0_base;
		if (imm.memory_index != 0) {
			// XXX: cache the base register for memories > 0
			base_reg = allocTmp(ValueKind.REF);
			emit_load_instance(base_reg);
			mmasm.emit_v3_Instance_memories_r_r(base_reg, base_reg);
			mmasm.emit_v3_Array_elem_r_ri(ValueKind.REF, base_reg, base_reg, imm.memory_index);
			mmasm.emit_v3_Memory_start_r_r(base_reg, base_reg);
		}
		var iv = pop();
		var index_reg: Reg;
		var offset = imm.offset;
		if (iv.isConst()) {
			var sum = u64.view(offset) + u32.view(iv.const); // fold offset calculation
			if (sum > u32.max) return (null, TrapReason.MEMORY_OOB);
			offset = u32.view(sum);
		} else {
			index_reg = ensureReg(iv, state.sp);
		}
		return (mmasm.decode_memarg_addr(base_reg, index_reg, u32.!(offset)), TrapReason.NONE);
	}
	// Utility functions for loading a value from memory into a register.
	private def loadMemarg<T>(dst: Reg, src: MemArg, asm_mov_r_m: (X86_64Gpr, X86_64Addr) -> T) {
		def t = decodeMemarg(src);
		if (t.1 != TrapReason.NONE) return emitTrap(t.1);
		def addr = t.0;
		mmasm.emit_v128_load_lane_r_m(dst, addr, asm_mov_r_m);
	}
	private def loadMemarg_b(dst: Reg, src: MemArg) { loadMemarg(dst, src, asm.q.movb_r_m); }
	private def loadMemarg_w(dst: Reg, src: MemArg) { loadMemarg(dst, src, asm.q.movw_r_m); }
	private def loadMemarg_d(dst: Reg, src: MemArg) { loadMemarg(dst, src, asm.q.movd_r_m); }
	private def loadMemarg_q(dst: Reg, src: MemArg) { loadMemarg(dst, src, asm.q.movq_r_m); }

	// Utility functions for storing a value from a register into memory.
	private def storeMemarg<T>(dst: MemArg, src: Reg, asm_mov_m_r: (X86_64Addr, X86_64Gpr) -> T) {
		def t = decodeMemarg(dst);
		if (t.1 != TrapReason.NONE) return emitTrap(t.1);
		def addr = t.0;
		mmasm.emit_v128_store_lane_m_r(addr, src, asm_mov_m_r);
	}
	private def storeMemarg_b(dst: MemArg, src: Reg) { storeMemarg(dst, src, asm.q.movb_m_r); }
	private def storeMemarg_w(dst: MemArg, src: Reg) { storeMemarg(dst, src, asm.q.movw_m_r); }
	private def storeMemarg_d(dst: MemArg, src: Reg) { storeMemarg(dst, src, asm.q.movd_m_r); }
	private def storeMemarg_q(dst: MemArg, src: Reg) { storeMemarg(dst, src, asm.q.movq_m_r); }

	def visit_V128_STORE_LANE<T>(imm: MemArg, lane: byte, storeMem: (MemArg, Reg) -> void, asm_pext_r_s_i: (X86_64Gpr, X86_64Xmmr, byte) -> T) {
		var sv = popReg();
		var val = allocTmp(ValueKind.I64);
		asm_pext_r_s_i(G(val), X(sv.reg), lane);
		storeMem(imm, val);
	}

	private def visit_V128_LOAD_LANE<T>(imm: MemArg, lane: byte, loadMem: (Reg, MemArg) -> void, asm_pins_s_r_i: (X86_64Xmmr, X86_64Gpr, byte) -> T) {
		var sv = popRegToOverwrite(), r = X(sv.reg);
		var val = allocTmp(ValueKind.I64);
		loadMem(val, imm);
		asm_pins_s_r_i(r, G(val), lane);
		state.push(sv.kindFlagsMatching(ValueKind.V128, IN_REG), sv.reg, 0);
	}

	private def visit_V128_REPLACE_LANE<T>(lane: byte, asm_pins_s_r_i: (X86_64Xmmr, X86_64Gpr, byte) -> T) {
		var b = popReg();
		var a = popRegToOverwrite();
		asm_pins_s_r_i(X(a.reg), G(b.reg), lane);
		state.push(a.kindFlagsMatching(ValueKind.V128, IN_REG), a.reg, 0);
	}

	private def visit_V128_EXTRACT_LANE<T>(kind: ValueKind, lane: byte, asm_pext_r_s_i: (X86_64Gpr, X86_64Xmmr, byte) -> T) {
		var a = popReg();
		var d = allocRegTos(kind);
		asm_pext_r_s_i(G(d), X(a.reg), lane);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}

	private def visit_V128_EXTRACT_LANE_S<T>(kind: ValueKind, lane: byte, asm_pext_r_s_i: (X86_64Gpr, X86_64Xmmr, byte) -> T,  asm_movext_s_s: (X86_64Gpr, X86_64Gpr) -> T) {
		var a = popReg();
		var d = allocRegTos(kind);
		asm_pext_r_s_i(G(d), X(a.reg), lane);
		asm_movext_s_s(G(d), G(d));
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}

	private def visit_V128_LOAD_EXTEND<T>(imm: MemArg, asm_pmov_s_m: (X86_64Xmmr, X86_64Addr) -> T) {
		def t = decodeMemarg(imm);
		if (t.1 != TrapReason.NONE) return emitTrap(t.1);
		var d = allocRegTos(ValueKind.V128);
		var val = allocTmp(ValueKind.I64);
		def addr = t.0;
		asm_pmov_s_m(X(d), addr);
		state.push(KIND_V128 | IN_REG, d, 0);
	}

	private def visit_V128_LOAD_ZERO<T>(imm: MemArg, loadMem: (Reg, MemArg) -> void, asm_pins_s_r_i: (X86_64Xmmr, X86_64Gpr, byte) -> T) {
		var val = allocTmp(ValueKind.I64);
		loadMem(val, imm);
		var d = allocRegTos(ValueKind.V128);
		mmasm.emit_v128_zero(X(d));
		asm_pins_s_r_i(X(d), G(val), 0);
		state.push(KIND_V128 | IN_REG, d, 0);
	}

	private def visit_V128_LOAD_SPLAT<T>(imm: MemArg, loadMem: (Reg, MemArg) -> void, masm_splat: (X86_64Xmmr, X86_64Gpr) -> void) {
		var val = allocTmp(ValueKind.I64);
		loadMem(val, imm);
		var d = allocRegTos(ValueKind.V128);
		masm_splat(X(d), G(val));
		state.push(KIND_V128 | IN_REG, d, 0);
	}

	private def visit_V128_SPLAT_I<T>(masm_splat: (X86_64Xmmr, X86_64Gpr) -> T) {
		var val = popReg();
		var d = allocRegTos(ValueKind.V128);
		masm_splat(X(d), G(val.reg));
		state.push(KIND_V128 | IN_REG, d, 0);
	}
	private def visit_V128_SPLAT_F<T>(asm_meth: (X86_64Xmmr, X86_64Xmmr) -> T) {
		var val = popReg();
		var d = allocRegTos(ValueKind.V128);
		asm_meth(X(d), X(val.reg));
		state.push(KIND_V128 | IN_REG, d, 0);
	}
	private def visit_V128_EXTMUL1<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr, X86_64Xmmr, bool, bool) -> T, is_low: bool, is_signed: bool) {
		var b = popReg();
		var a = popRegToOverwrite();
		var x_tmp = X(allocTmp(kind));
		emit(X(a.reg), X(b.reg), x_tmp, is_low, is_signed);
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
	}
	private def visit_V128_EXTMUL2<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr, X86_64Xmmr, bool) -> T, is_signed: bool) {
		var b = popReg();
		var a = popRegToOverwrite();
		var x_tmp = X(allocTmp(kind));
		emit(X(a.reg), X(b.reg), x_tmp, is_signed);
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
	}

	def visit_I16X8_RELAXED_DOT_I8X16_I7X16_S() { do_c_op2_x_x(ValueKind.V128, asm.pmaddubsw_s_s); }
	def visit_I32X4_RELAXED_DOT_I8X16_I7X16_ADD_S() {
		var c = popTmpReg(), r_xmm2 = X(c.reg);
		var b = popTmpReg(), r_xmm1 = X(b.reg);
		var a = popRegToOverwrite(), r_xmm0 = X(a.reg);
		var t = G(allocTmp(ValueKind.I64));

		asm.pmaddubsw_s_s(r_xmm1, r_xmm0);
		mmasm.load_v128_mask(r_xmm0, mmasm.mask_i16x8_splat_0x0001, t);
		asm.pmaddwd_s_s(r_xmm0, r_xmm1);
		asm.paddd_s_s(r_xmm0, r_xmm2);

		state.push(a.kindFlagsMatching(ValueKind.V128, IN_REG), a.reg, 0);
	}

	def visit_ATOMIC_FENCE(flags: u8) {
		asm.mfence();
	}

	// r1 = op(r1)
	private def do_op1_r<T>(kind: ValueKind, emit: (X86_64Gpr -> T)) -> bool {
		var sv = popRegToOverwrite(), r = G(sv.reg);
		emit(r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), sv.reg, 0);
		return true;
	}
	// r1 = op(r2)
	private def do_op1_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr) -> T) -> bool {
		var sv = popReg(), r = G(sv.reg);
		var d = allocRegTos(kind);
		emit(G(d), r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), d, 0);
		return true;
	}
	// r1 = op(r1, r2)
	private def do_op2_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr) -> T) -> bool {
		var b = popReg();
		var a = popRegToOverwrite();
		emit(G(a.reg), G(b.reg));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// r1 = op(r1, m2)
	private def do_op2_r_m<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Addr) -> T) -> bool {
		var b = state.peek();
		if (b.inReg() || b.isConst()) return false;
		state.pop();
		var addr = masm.slotAddr(state.sp);
		var a = popRegToOverwrite();
		emit(G(a.reg), A(addr));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// r1 = op(r1, imm)
	private def do_op2_r_i<T>(kind: ValueKind, emit: (X86_64Gpr, int) -> T) -> bool {
		var b = state.peek();
		if (!b.isConst()) return false;
		pop();
		var a = popRegToOverwrite();
		emit(G(a.reg), b.const);
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// r1 = op(r2, imm)
	private def do_op2_r_r_i<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, int) -> T) -> bool;
	// r1 = op(r2, m3)
	private def do_op2_r_r_m<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, X86_64Addr) -> T) -> bool;
	// r1 = op(r2, r3)
	private def do_op2_r_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, X86_64Gpr) -> T) -> bool;
	// r1 = op(r2)
	private def do_op1_x_x<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var sv = popReg(), r = X(sv.reg);
		var d = allocRegTos(kind);
		emit(X(d), r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), d, 0);
		return true;
	}
	// r1 = op(r2)
	private def do_op1_r_x<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Xmmr) -> T) -> bool {
		var sv = popReg(), r = X(sv.reg);
		var d = allocRegTos(kind);
		emit(G(d), r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), d, 0);
		return true;
	}
	// r1 = op(r1), SIMD unop
	private def do_op1_x<T>(kind: ValueKind, emit: (X86_64Xmmr) -> T) -> bool {
		var sv = popRegToOverwrite(), r = X(sv.reg);
		emit(r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), sv.reg, 0);
		return true;
	}
	// r1 = op(r1, xtmp)
	private def do_op1_x_xtmp<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var sv = popRegToOverwrite(), r = X(sv.reg);
		var x_tmp = X(allocTmp(kind));
		emit(r, x_tmp);
		state.push(sv.kindFlagsMatching(kind, IN_REG), sv.reg, 0);
		return true;
	}
	// r1 = op(r1, gtmp, xtmp)
	private def do_op1_x_gtmp_xtmp<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Gpr, X86_64Xmmr) -> T) -> bool {
		var sv = popRegToOverwrite(), r = X(sv.reg);
		var g_tmp = G(allocTmp(ValueKind.I64));
		var x_tmp = X(allocTmp(kind));
		emit(r, g_tmp, x_tmp);
		state.push(sv.kindFlagsMatching(kind, IN_REG), sv.reg, 0);
		return true;
	}
	// x1 = op(x1, x2)
	private def do_op2_x_x<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var b = popReg();
		var a = popRegToOverwrite();
		emit(X(a.reg), X(b.reg));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// x1 = op(x1, x2, xtmp)
	private def do_op2_x_x_xtmp<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var b = popReg();
		var a = popRegToOverwrite();
		var x_tmp = X(allocTmp(kind));
		emit(X(a.reg), X(b.reg), x_tmp);
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// x2 = op(x2, x1), commuted version of do_op2_x_x
	private def do_c_op2_x_x<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var b = popRegToReuse(-1);
		var a = popReg();
		emit(X(b.reg), X(a.reg));
		state.push(b.kindFlagsMatching(kind, IN_REG), b.reg, 0);
		return true;
	}
	// x2 = op(x2, x1, xtmp), commuted version of do_op2_x_x_xtmp
	private def do_c_op2_x_x_xtmp<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var b = popRegToReuse(-1);
		var a = popReg();
		var x_tmp = X(allocTmp(kind));
		emit(X(b.reg), X(a.reg), x_tmp);
		state.push(b.kindFlagsMatching(kind, IN_REG), b.reg, 0);
		return true;
	}
}

def ucontext_rip_offset = 168;
def SIGFPE  = 8;
def SIGBUS  = 10;
def SIGSEGV = 11;

// Represents SPC code that uses complete frames (both trapping code and module code).
// Implements the RiUserCode interface to add generated machine code to the V3 runtime.
// Handles stackwalking and signals in JITed code.
class X86_64SpcCode extends RiUserCode {
	def name: string;

	new(name, start: Pointer, end: Pointer) super(start, end) { }

	// V3-runtime callback: upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: Range<byte> -> void) {
		if (Debug.stack) X86_64Stacks.traceIpAndSp(ip, sp, out);
		var msg: string;
		msg = "\tin [";
		out(msg);
		msg = name;
		out(msg);
		msg = "] ";
		out(msg);
		var instance = (sp + X86_64InterpreterFrame.instance.offset).load<Instance>();
		var wf = (sp + X86_64InterpreterFrame.wasm_func.offset).load<WasmFunction>();
		// TODO: lazy parse of names section may allocate; must avoid this in OOM situation
		var buf = X86_64Runtime.globalFrameDescriptionBuf;
		wf.decl.render(instance.module.names, buf);
		buf.ln().send(out);
		buf.reset();
	}
	// V3-runtime callback: to advance over this frame.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp += X86_64InterpreterFrame.size;	 // assume frame is allocated
		ip = sp.load<Pointer>(); // return address on stack
		return (ip + -1, sp + Pointer.SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}
	// V3-runtime callback: when the garbage collector needs to scan a JIT stack frame.
	def scanFrame(ip: Pointer, sp: Pointer) {
		// Handle other roots in the frame
		RiGc.rescanRoot(sp + X86_64InterpreterFrame.wasm_func.offset);
		RiGc.rescanRoot(sp + X86_64InterpreterFrame.instance.offset);
		RiGc.rescanRoot(sp + X86_64InterpreterFrame.accessor.offset);
		if (SpcTuning.inlineSmallFunc && SpcTuning.intrinsifyWhammProbe) {
			RiGc.rescanRoot(sp + X86_64InterpreterFrame.stp.offset); // TODO: define X86_64SpcFrame and use dedicated slot
		}
	}
}

// Represents the JITed code for an entire module.
class X86_64SpcModuleCode extends X86_64SpcCode {
	def mapping: Mapping;
	var codeEnd: int;			// for dynamically adding code to the end
	var sourcePcs: Vector<(int, int)>;
	var embeddedRefOffsets: Vector<int>;

	new(mapping) super("spc-module", mapping.range.start, mapping.range.end) {
		RiGc.registerScanner(this, X86_64SpcModuleCode.scan);
	}

	// XXX: Keeps the whole-program optimizer from deleting fields needed for rooting the range.
	def keepAlive() {
		if (mapping == null) System.error(null, null);
		if (mapping.range == null) System.error(null, null);
	}
	// V3-runtime callback: handle an OS-level signal that occurred while {ip} was in JIT code.
	def handleSignal(signum: int, siginfo: Pointer, ucontext: Pointer, ip: Pointer, sp: Pointer) -> bool {
		if (Debug.runtime) {
			Trace.OUT.put2("  !signal %d in SPC code @ 0x%x", signum, RiOs.getIp(ucontext) - Pointer.NULL).ln();
		}
		match (signum) {
			SIGFPE => {
				// presume divide/modulus by zero
				updateUContextToTrapsStub(ucontext, TrapReason.DIV_BY_ZERO);
				return true;
			}
			SIGBUS, SIGSEGV => {
				var addr = RiOs.getAccessAddress(siginfo, ucontext);
				var reason: TrapReason;
				if (RedZones.isInRedZone(addr)) {
					// Stack overflow is handled specially from other traps.
					def RETADDR_SIZE = Pointer.SIZE;
					var handler_ip = X86_64Runtime.curStack.handleOverflow(ip, sp + X86_64InterpreterFrame.size + RETADDR_SIZE);
					var p_rip = ucontext + ucontext_rip_offset;
					p_rip.store<Pointer>(handler_ip);
					return true;
				} else {
					// XXX: handle null derefs for wasm-gc opcodes
					reason = TrapReason.MEMORY_OOB;
				}
				updateUContextToTrapsStub(ucontext, reason);
				return true;
			}
		}
		return false;
	}
	// Updates the siginfo's {ucontext} to set the handler %rip and to write the PC of the fault location
	// into the stack frame for the handler.
	private def updateUContextToTrapsStub(ucontext: Pointer, reason: TrapReason) {
		var p_rip = ucontext + ucontext_rip_offset;
		var p_rsp = RiOs.getSp(ucontext);
		if (!RiRuntime.inStackRedZone(p_rsp)) {
			// Update the current PC in the JIT frame, if it is accessible.
			var ip = p_rip.load<Pointer>();
			var pc = lookupPc(ip, false);
			(p_rsp + X86_64InterpreterFrame.curpc.offset).store<int>(pc);
		}

		var handler_ip = TRAPS_STUB.getIpForReason(reason);
		(p_rip).store<Pointer>(handler_ip);
	}
	// Look up the source {pc} of a location {i} in this code. Returns {-1} if no exact entry is found.
	// Return addresses are treated differently than other addresses in the code.
	def lookupPc(ip: Pointer, isRetAddr: bool) -> int {
		if (Trace.compiler) Trace.OUT.put2("SpcCode.lookupPc(0x%x, ret=%z)", (ip - Pointer.NULL), isRetAddr).ln();
		if (sourcePcs == null) return -1;
		if (!mapping.range.contains(ip)) return -1;
		var offset = ip - mapping.range.start - if(isRetAddr, 1);
		// XXX: use binary search for looking up source PCs in SPC code
		if (Trace.compiler) Trace.OUT.put1(" looking for offset=%d", offset).ln();
		for (i < sourcePcs.length) {
			var entry = sourcePcs[i];
			if (Trace.compiler) Trace.OUT.put2("  (offset=%d, pc=%d)", entry.0, entry.1).ln();
			if (offset == entry.0) return entry.1;
		}
		return -1;
	}
	// Appends code to the end of this module.
	def appendCode(masm: X86_64MacroAssembler) -> Pointer {
		var range = mapping.range;
		var startOffset = codeEnd;
		var entrypoint = range.start + startOffset;
		masm.setTargetAddress(u64.view(entrypoint - Pointer.NULL));
		codeEnd = Target.copyInto(range, startOffset, masm.w);
		if (masm.source_locs != null) {
			if (sourcePcs == null) sourcePcs = Vector.new();
			for (i < masm.source_locs.length) {
				var entry = masm.source_locs[i];
				sourcePcs.put(entry.0 + startOffset, entry.1);
			}
		}
		if (masm.embeddedRefOffsets != null) {
			if (embeddedRefOffsets == null) embeddedRefOffsets = Vector.new();
			for (i < masm.embeddedRefOffsets.length) {
				embeddedRefOffsets.put(masm.embeddedRefOffsets[i] + startOffset);
			}
		}
		return entrypoint;
	}
	// Callback for the GC to custom-scan roots in the module code, e.g. embedded object
	// constants like probe objects.
	private def scan() {
		if (embeddedRefOffsets == null) return;
		var codeStart = mapping.range.start;
		for (i < embeddedRefOffsets.length) {
			var ref_loc = codeStart + embeddedRefOffsets[i];
			RiGc.scanRoot(ref_loc);
		}
	}
}

// Represents (shared) code that calls the runtime to generate a trap.
// This is jumped to conditionally by SPC code or via the signal handler for the
// {X86_64SpcModuleCode}.
def TRAP_HANDLER_SIZE = 48;
class X86_64SpcTrapsStub extends X86_64SpcCode {

	new() super("spc-trap-stubs", Pointer.NULL, Pointer.NULL) { }

	// Get the instruction pointer for the stub that will call the runtime for the given reason.
	def getIpForReason(reason: TrapReason) -> Pointer {
		return start + reason.tag * TRAP_HANDLER_SIZE;
	}
}

// The lazy-compile stub needs special handling in the Virgil runtime because it has
// a frame that stores the function being compiled.
class X86_64SpcCompileStub extends RiUserCode {
	def stubName: string;
	def frameSize = Pointer.SIZE;

	new(stubName) super(Pointer.NULL, Pointer.NULL) { }

	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: Range<byte> -> void) {
		if (Debug.stack) X86_64Stacks.traceIpAndSp(ip, sp, out);
		var msg = "\tin [spc-";
		out(msg);
		out(stubName);
		msg = "-compile-stub] ";
		out(msg);
		var wf = (sp + 0).load<WasmFunction>();
		// TODO: lazy parse of names section may allocate; must avoid this in OOM situation
		var buf = X86_64Runtime.globalFrameDescriptionBuf;
		wf.decl.render(wf.instance.module.names, buf);
		buf.ln().send(out);
		buf.reset();
	}
	// Called from V3 runtime for a frame where {ip} is in the stub code.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp += frameSize;	 // assume frame is allocated
		ip = sp.load<Pointer>(); // return address on stack
		return (ip + -1, sp + Pointer.SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}
	// Called from V3 runtime when the garbage collector needs to scan a JIT stack frame.
	def scanFrame(ip: Pointer, sp: Pointer) {
		// The WasmFunction is stored in the frame for debugging.
		RiGc.rescanRoot(sp + 0);
	}
}

def V3_SPC_ENTRY_FUNC = X86_64PreGenFunc<(WasmFunction, Pointer, Pointer), Throwable>.new("v3-spc-entry", null, genSpcEntryFunc);
def LAZY_COMPILE_STUB = X86_64PreGenStub.new("spc-lazy-compile", X86_64SpcCompileStub.new("lazy"), genLazyCompileStub);
def TIERUP_COMPILE_STUB = X86_64PreGenStub.new("spc-tierup-compile", X86_64SpcCompileStub.new("tierup"), genTierUpCompileStub);
def TRAPS_STUB = X86_64SpcTrapsStub.new();
def TRAPS_PREGEN = X86_64PreGenStub.new("spc-trap", TRAPS_STUB, genTrapsStub);

def genSpcEntryFunc(ic: X86_64InterpreterCode, w: DataWriter) {
	if (SpcTuning.disable) return;
	var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	var asm = X86_64Assembler.!(masm.asm);
	var regs = X86_64MasmRegs.SPC_EXEC_ENV;
	var func_arg = G(regs.func_arg);
	if (func_arg == G(Target.V3_PARAM_GPRS[2])) {
		// XXX: use macro assembler to order moves
		var scratch = masm.scratch;
		asm.movq_r_r(scratch, func_arg);
		asm.movq_r_r(func_arg, G(Target.V3_PARAM_GPRS[1]));	// function
		asm.movq_r_r(G(regs.vsp), scratch);			// vsp
	} else {
		asm.movq_r_r(func_arg, G(Target.V3_PARAM_GPRS[1]));	// function
		asm.movq_r_r(G(regs.vsp), G(Target.V3_PARAM_GPRS[2]));	// vsp
	}
	asm.ijmp_r(G(Target.V3_PARAM_GPRS[3]));				// tail-call entrypoint
	asm.invalid();
}
def genLazyCompileStub(ic: X86_64InterpreterCode, w: DataWriter) {
	if (SpcTuning.disable) return;
	var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	var asm = X86_64Assembler.!(masm.asm);
	var regs = X86_64MasmRegs.SPC_EXEC_ENV;
	var func_arg = G(regs.func_arg);
	asm.pushq_r(G(regs.func_arg));					// push function onto stack
	masm.emit_store_curstack_vsp(regs.vsp);
	asm.movq_r_r(G(Target.V3_PARAM_GPRS[1]), G(regs.func_arg));	// function
	// Load {null} for the receiver.
	asm.movq_r_i(G(Target.V3_PARAM_GPRS[0]), 0);
	// Call {X86_64Spc.lazyCompile} directly.
	masm.emit_call_abs(codePointer(X86_64Spc.lazyCompile));
	asm.q.add_r_i(R.RSP, Pointer.SIZE);				// pop function off stack
	// Check for non-null abrupt return.
	var unwind = X86_64Label.new();
	asm.q.cmp_r_i(G(Target.V3_RET_GPRS[2]), 0);
	asm.jc_rel_near(C.NZ, unwind);
	// Tail-call the result of the compile.
	var scratch = X86_64Regs.R15;
	asm.movq_r_r(scratch, G(Target.V3_RET_GPRS[1]));			// entrypoint
	asm.movq_r_r(G(regs.func_arg), G(Target.V3_RET_GPRS[0]));		// function
	masm.emit_load_curstack_vsp(regs.vsp);
	asm.ijmp_r(scratch);						// jump to entrypoint
	asm.invalid();
	// Simply return the {Throwable} object.
	asm.bind(unwind);
	asm.movq_r_r(G(Target.V3_RET_GPRS[0]), G(Target.V3_RET_GPRS[2]));
	asm.ret();
}
def genTierUpCompileStub(ic: X86_64InterpreterCode, w: DataWriter) {
	if (SpcTuning.disable) return;
	var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	var asm = X86_64Assembler.!(masm.asm);
	var regs = X86_64MasmRegs.SPC_EXEC_ENV;
	var func_arg = G(regs.func_arg);
	// Decrement execution counter by 1 and compile if <= 0
	var scratch = X86_64MasmRegs.R15; // XXX: regs.func_arg == scratch!
	masm.emit_v3_WasmFunction_decl_r_r(scratch, regs.func_arg);
	asm.q.sub_m_i(G(scratch).plus(masm.getOffsets().FuncDecl_tierup_trigger), FastIntTuning.entryTierUpDecrement);
	var interpreter_entry = X86_64Label.new();
	interpreter_entry.pos = ic.header.intSpcEntryOffset;
	asm.jc_rel_near(C.A, interpreter_entry);			// jump to interpreter if >= 0
	// Call compiler
	asm.pushq_r(G(regs.func_arg));					// push function onto stack
	masm.emit_store_curstack_vsp(regs.vsp);
	asm.movq_r_r(G(Target.V3_PARAM_GPRS[1]), G(regs.func_arg));	// function
	// Load {null} for the receiver.
	asm.movq_r_i(G(Target.V3_PARAM_GPRS[0]), 0);
	// Call {X86_64Spc.tierupCompile} directly.
	masm.emit_call_abs(codePointer(X86_64Spc.tierupCompile));
	asm.q.add_r_i(R.RSP, Pointer.SIZE);				// pop function off stack
	// Tail-call the result of the compile (which could be interpreter entry).
	asm.movq_r_r(G(scratch), G(Target.V3_RET_GPRS[1]));		// entrypoint
	asm.movq_r_r(G(regs.func_arg), G(Target.V3_RET_GPRS[0]));		// function
	masm.emit_load_curstack_vsp(regs.vsp);
	asm.ijmp_r(G(scratch));						// jump to entrypoint
	asm.invalid();
}
def genTrapsStub(ic: X86_64InterpreterCode, w: DataWriter) {
	var masm = X86_64MacroAssembler.new(w, X86_64MasmRegs.CONFIG);
	var asm = X86_64Assembler.!(masm.asm);
	var regs = X86_64MasmRegs.SPC_EXEC_ENV;
	for (reason in TrapReason) {
		var start = w.atEnd().pos;
		if (reason == TrapReason.STACK_OVERFLOW) {
			masm.emit_mov_r_trap(X86_64MasmRegs.SPC_EXEC_ENV.ret_throw, TrapReason.STACK_OVERFLOW);
		} else {
			var m_wasm_func = X86_64Regs.RSP.plus(X86_64InterpreterFrame.wasm_func.offset);
			masm.emit_get_curstack(regs.runtime_arg0);	// stack
			masm.emit_v3_set_X86_64Stack_rsp_r_r(regs.runtime_arg0, regs.sp);
			masm.emit_push_X86_64Stack_rsp_r_r(regs.runtime_arg0);
			asm.movq_r_m(G(Target.V3_PARAM_GPRS[2]), m_wasm_func);	// wasm_func
			asm.movq_r_i(G(Target.V3_PARAM_GPRS[3]), -1);		// pc
			asm.movd_r_i(G(Target.V3_PARAM_GPRS[4]), reason.tag);	// reason
			masm.emit_call_runtime_TRAP();
		}
		asm.q.add_r_i(R.RSP, X86_64InterpreterFrame.size); // pop caller frame
		asm.ret();
		var skip = TRAP_HANDLER_SIZE - (w.atEnd().pos - start);
		if (skip < 0) System.error("TrapsStubError", "handler size too big");
		w.skipN(skip);
	}
}
def codePointer<P, R>(f: P -> R) -> Pointer {
	return CiRuntime.unpackClosure<X86_64Spc, P, R>(f).0;
}

// Global functionality associated with the single-pass compiler for X86-64.
component X86_64Spc {
	// A handy chokepoint for entering JIT code from V3.
	def invoke(wf: WasmFunction, sp: Pointer) -> Throwable {
		return V3_SPC_ENTRY_FUNC.get()(wf, sp, wf.decl.target_code.spc_entry);
	}
	def setLazyCompileFor(module: Module, decl: FuncDecl) {
		if (Debug.runtime) Trace.OUT.put1("setLazyCompile %q", decl.render(module.names, _)).ln();
		decl.target_code = TargetCode(LAZY_COMPILE_STUB.getEntry());
	}
	def setTierUpFor(module: Module, decl: FuncDecl) {
		if (Debug.runtime) Trace.OUT.put1("setTierUp %q", decl.render(module.names, _)).ln();
		decl.target_code = TargetCode(TIERUP_COMPILE_STUB.getEntry());
		decl.tierup_trigger = SpcTuning.fastIntTierUpThreshold;
	}
	def setInterpreterFallback(decl: FuncDecl) -> Pointer {
		var addr = X86_64PreGenStubs.getSpcIntEntry();
		decl.target_code = TargetCode(addr);
		decl.tierup_trigger = int.max;
		return addr;
	}
	def estimateCodeSizeFor(decl: FuncDecl) -> int {
		return 60 + decl.orig_bytecode.length * 20; // TODO: huge overestimate
	}
	private def lazyCompile(wf: WasmFunction) -> (WasmFunction, Pointer, Throwable) {
		// The global stub simply consults the execution strategy.
		var result = X86_64SpcStrategy.!(Execute.tiering).lazyCompile(wf);
		return (result.wf, result.entrypoint, result.thrown);
	}
	private def tierupCompile(wf: WasmFunction) -> (WasmFunction, Pointer, Throwable) {
		// The global stub simply consults the execution strategy.
		var result = X86_64SpcStrategy.!(Execute.tiering).tierupCompile(wf);
		return (result.wf, result.entrypoint, result.thrown);
	}
}
