// Copyright 2022 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// XXX: reduce duplication with MacroAssembler
def G = X86_64Regs2.toGpr, X = X86_64Regs2.toXmmr;
def R: X86_64Regs;
def C: X86_64Conds;
def A(ma: MasmAddr) -> X86_64Addr {
	return X86_64Addr.new(G(ma.base), null, 1, ma.offset);
}

// Shorten constants inside this file.
def NO_REG = SpcConsts.NO_REG;
def IS_STORED = SpcConsts.IS_STORED;
def IS_CONST = SpcConsts.IS_CONST;
def IN_REG = SpcConsts.IN_REG;
def TAG_STORED = SpcConsts.TAG_STORED;
def KIND_MASK = SpcConsts.KIND_MASK;
def KIND_I32 = SpcConsts.KIND_I32;
def KIND_I64 = SpcConsts.KIND_I64;
def KIND_F32 = SpcConsts.KIND_F32;
def KIND_F64 = SpcConsts.KIND_F64;
def KIND_V128 = SpcConsts.KIND_V128;
def KIND_REF = SpcConsts.KIND_REF;
def KIND_ABS = SpcConsts.KIND_ABS;

class X86_64SinglePassCompiler extends SinglePassCompiler {
	def w = DataWriter.new();
	def mmasm = X86_64MacroAssembler.new(w, X86_64Regs2.SPC_ALLOC.copy());
	def asm = mmasm.asm;

	new(extensions: Extension.set, limits: Limits, config: RegConfig, module: Module)
		super(mmasm, extensions, limits, module) {
	}

	private def visitCompareI(asm: X86_64Assembler, cond: X86_64Cond) -> bool {
		var b = state.pop(), a = popReg();
		if (b.isConst()) asm.cmp_r_i(G(a.reg), b.const);
		else if (b.inReg()) asm.cmp_r_r(G(a.reg), G(b.reg));
		else asm.cmp_r_m(G(a.reg), A(masm.slotAddr(state.sp + 1)));
		freeVal(b);
		freeVal(a);
		var reg = allocRegTos(ValueKind.I32), r1 = G(reg);
		asm.set_r(cond, r1);
		asm.movbzx_r_r(r1, r1);
		state.push(KIND_I32 | IN_REG, reg, 0);
		return true;
	}
	def visit_I32_EQZ() {
		state.push(KIND_I32 | IS_CONST, NO_REG, 0);
		visit_I32_EQ();
	}
	def visit_I32_EQ()   { var x = tryFold_uu_z(V3Eval.I32_EQ)   || visitCompareI(asm.d, C.Z); }
	def visit_I32_NE()   { var x = tryFold_uu_z(V3Eval.I32_NE)   || visitCompareI(asm.d, C.NZ); }
	def visit_I32_LT_S() { var x = tryFold_ii_z(V3Eval.I32_LT_S) || visitCompareI(asm.d, C.L); }
	def visit_I32_LT_U() { var x = tryFold_uu_z(V3Eval.I32_LT_U) || visitCompareI(asm.d, C.C); }
	def visit_I32_GT_S() { var x = tryFold_ii_z(V3Eval.I32_GT_S) || visitCompareI(asm.d, C.G); }
	def visit_I32_GT_U() { var x = tryFold_uu_z(V3Eval.I32_GT_U) || visitCompareI(asm.d, C.A); }
	def visit_I32_LE_S() { var x = tryFold_ii_z(V3Eval.I32_LE_S) || visitCompareI(asm.d, C.LE); }
	def visit_I32_LE_U() { var x = tryFold_uu_z(V3Eval.I32_LE_U) || visitCompareI(asm.d, C.NA); }
	def visit_I32_GE_S() { var x = tryFold_ii_z(V3Eval.I32_GE_S) || visitCompareI(asm.d, C.GE); }
	def visit_I32_GE_U() { var x = tryFold_uu_z(V3Eval.I32_GE_U) || visitCompareI(asm.d, C.NC); }

	def visit_I64_EQZ() {
		state.push(KIND_I64 | IS_CONST, NO_REG, 0);
		visit_I64_EQ();
	}
	def visit_I64_EQ()   { var x = tryFold_qq_z(V3Eval.I64_EQ)   || visitCompareI(asm.q, C.Z); }
	def visit_I64_NE()   { var x = tryFold_qq_z(V3Eval.I64_NE)   || visitCompareI(asm.q, C.NZ); }
	def visit_I64_LT_S() { var x = tryFold_ll_z(V3Eval.I64_LT_S) || visitCompareI(asm.q, C.L); }
	def visit_I64_LT_U() { var x = tryFold_qq_z(V3Eval.I64_LT_U) || visitCompareI(asm.q, C.C); }
	def visit_I64_GT_S() { var x = tryFold_ll_z(V3Eval.I64_GT_S) || visitCompareI(asm.q, C.G); }
	def visit_I64_GT_U() { var x = tryFold_qq_z(V3Eval.I64_GT_U) || visitCompareI(asm.q, C.A); }
	def visit_I64_LE_S() { var x = tryFold_ll_z(V3Eval.I64_LE_S) || visitCompareI(asm.q, C.LE); }
	def visit_I64_LE_U() { var x = tryFold_qq_z(V3Eval.I64_LE_U) || visitCompareI(asm.q, C.NA); }
	def visit_I64_GE_S() { var x = tryFold_ll_z(V3Eval.I64_GE_S) || visitCompareI(asm.q, C.GE); }
	def visit_I64_GE_U() { var x = tryFold_qq_z(V3Eval.I64_GE_U) || visitCompareI(asm.q, C.NC); }

	private def visitFloatCmp(emit_cmp: (X86_64Xmmr, X86_64Xmmr) -> X86_64Assembler, unordered_true: bool, cond: X86_64Cond) {
		var b = popReg(), a = popReg();
		var ret_zero = X86_64Label.new(), ret_one = X86_64Label.new(), done = X86_64Label.new();
		emit_cmp(X(a.reg), X(b.reg));
		asm.jc_rel_near(C.P, if(unordered_true, ret_one, ret_zero));
		asm.jc_rel_near(cond, ret_zero);
		var d = allocRegTos(ValueKind.I32);
		asm.bind(ret_one);
		asm.movd_r_i(G(d), 1);
		asm.jmp_rel_near(done);
		asm.bind(ret_zero);
		asm.movd_r_i(G(d), 0);
		asm.bind(done);
		freeVal(a);
		freeVal(b);
		state.push(KIND_I32 | IN_REG, d, 0);
	}
	def visit_F32_EQ() { visitFloatCmp(asm.ucomiss_s_s, false, C.NZ); }
	def visit_F32_NE() { visitFloatCmp(asm.ucomiss_s_s, true, C.Z); }
	def visit_F32_LT() { visitFloatCmp(asm.ucomiss_s_s, false, C.NC); }
	def visit_F32_GT() { visitFloatCmp(asm.ucomiss_s_s, false, C.NA); }
	def visit_F32_LE() { visitFloatCmp(asm.ucomiss_s_s, false, C.A); }
	def visit_F32_GE() { visitFloatCmp(asm.ucomiss_s_s, false, C.C); }

	def visit_F64_EQ() { visitFloatCmp(asm.ucomisd_s_s, false, C.NZ); }
	def visit_F64_NE() { visitFloatCmp(asm.ucomisd_s_s, true, C.Z); }
	def visit_F64_LT() { visitFloatCmp(asm.ucomisd_s_s, false, C.NC); }
	def visit_F64_GT() { visitFloatCmp(asm.ucomisd_s_s, false, C.NA); }
	def visit_F64_LE() { visitFloatCmp(asm.ucomisd_s_s, false, C.A); }
	def visit_F64_GE() { visitFloatCmp(asm.ucomisd_s_s, false, C.C); }

	def visit_REF_IS_NULL() {
		state.push(KIND_REF | IS_CONST, NO_REG, 0);
		visit_REF_EQ();
	}
	def visit_I32_CLZ() {
		var x = tryFold_u_u(V3Eval.I32_CLZ)
		|| do_op1_r_r(ValueKind.I32, mmasm.emit_i32_clz_r_r);
	}
	def visit_I32_CTZ() {
		var x = tryFold_u_u(V3Eval.I32_CTZ)
			|| do_op1_r_r(ValueKind.I32, mmasm.emit_i32_ctz_r_r);
	}
	def visit_I32_POPCNT() {
		var x = tryFold_u_u(V3Eval.I32_POPCNT)
			|| do_op1_r_r(ValueKind.I32, asm.d.popcnt_r_r);
	}

	private def visitSimpleOp2_uu_u(fold: (u32, u32) -> u32,
		emit_r_i: (X86_64Gpr, int) -> X86_64Assembler,
		emit_r_m: (X86_64Gpr, X86_64Addr) -> X86_64Assembler,
		emit_r_r: (X86_64Gpr, X86_64Gpr) -> X86_64Assembler) {
		var x = tryFold_uu_u(fold)
			|| do_op2_r_i(ValueKind.I32, emit_r_i)
			|| do_op2_r_m(ValueKind.I32, emit_r_m)
			|| do_op2_r_r(ValueKind.I32, emit_r_r);
	}
	def visit_I32_ADD() { visitSimpleOp2_uu_u(V3Eval.I32_ADD, asm.d.add_r_i, asm.d.add_r_m, asm.d.add_r_r); }
	def visit_I32_SUB() { visitSimpleOp2_uu_u(V3Eval.I32_SUB, asm.d.sub_r_i, asm.d.sub_r_m, asm.d.sub_r_r); }
	def visit_I32_MUL() { visitSimpleOp2_uu_u(V3Eval.I32_MUL, asm.d.imul_r_i, asm.d.imul_r_m, asm.d.imul_r_r); }

	private def visitIDivRem(dst: Reg, emit: X86_64Gpr -> void) {
		var b = popFixedReg(X86_64Regs2.RCX); // XXX: use any reg except RAX or RDX
		var a = popFixedReg(X86_64Regs2.RAX);
		spillReg(X86_64Regs2.RDX);
		emit(G(b.reg));
		freeVal(b);
		freeVal(a);
		masm.regAlloc.unfree(dst, int.!(state.sp));
		state.push(a.kindFlagsAndTag(IN_REG), dst, 0);
	}
	def visit_I32_DIV_S() { visitIDivRem(X86_64Regs2.RAX, mmasm.emit_i32_div_s); }
	def visit_I32_DIV_U() { visitIDivRem(X86_64Regs2.RAX, mmasm.emit_i32_div_u); }
	def visit_I32_REM_S() { visitIDivRem(X86_64Regs2.RDX, mmasm.emit_i32_rem_s); }
	def visit_I32_REM_U() { visitIDivRem(X86_64Regs2.RDX, mmasm.emit_i32_rem_u); }

	private def visitShift(kind: ValueKind, emit_r_i: (X86_64Gpr, u6) -> X86_64Assembler, emit_r_cl: X86_64Gpr -> X86_64Assembler) -> bool {
		var sv = state.peek(), a: SpcVal;
		if (sv.isConst()) {
			state.pop();
			a = popRegToOverwrite();
			emit_r_i(G(a.reg), u6.view(sv.const));
		} else {
			sv = popFixedReg(X86_64Regs2.RCX);
			a = popRegToOverwrite();
			emit_r_cl(G(a.reg));
		}
		freeVal(sv);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, a.reg, 0); // XXX: tag preservation
		return true;
	}
	def visit_I32_SHL() { var x = tryFold_ii_i(V3Eval.I32_SHL) || visitShift(ValueKind.I32, asm.d.shl_r_i, asm.d.shl_r_cl); }
	def visit_I32_SHR_S() { var x = tryFold_ii_i(V3Eval.I32_SHR_S) || visitShift(ValueKind.I32, asm.d.sar_r_i, asm.d.sar_r_cl); }
	def visit_I32_SHR_U() { var x = tryFold_ii_i(V3Eval.I32_SHR_U) || visitShift(ValueKind.I32, asm.d.shr_r_i, asm.d.shr_r_cl); }
	def visit_I32_ROTL() { var x = tryFold_uu_u(V3Eval.I32_ROTL) || visitShift(ValueKind.I32, asm.d.rol_r_i, asm.d.rol_r_cl); }
	def visit_I32_ROTR() { var x = tryFold_uu_u(V3Eval.I32_ROTR) || visitShift(ValueKind.I32, asm.d.ror_r_i, asm.d.ror_r_cl); }
	def visit_I32_AND() { visitSimpleOp2_uu_u(V3Eval.I32_AND, asm.d.and_r_i, asm.d.and_r_m, asm.d.and_r_r); }
	def visit_I32_OR()  { visitSimpleOp2_uu_u(V3Eval.I32_OR, asm.d.or_r_i, asm.d.or_r_m, asm.d.or_r_r); }
	def visit_I32_XOR() { visitSimpleOp2_uu_u(V3Eval.I32_XOR, asm.d.xor_r_i, asm.d.xor_r_m, asm.d.xor_r_r); }

	def visit_I64_CLZ() {
		var x = tryFold_q_q(V3Eval.I64_CLZ)
		|| do_op1_r_r(ValueKind.I64, mmasm.emit_i64_clz_r_r);
	}
	def visit_I64_CTZ() {
		var x = tryFold_q_q(V3Eval.I64_CTZ)
			|| do_op1_r_r(ValueKind.I64, mmasm.emit_i64_ctz_r_r);
	}
	def visit_I64_POPCNT() {
		var x = tryFold_q_q(V3Eval.I64_POPCNT)
			|| do_op1_r_r(ValueKind.I64, asm.q.popcnt_r_r);
	}
	private def visitSimpleOp2_qq_q(fold: (u64, u64) -> u64,
		emit_r_i: (X86_64Gpr, int) -> X86_64Assembler,
		emit_r_m: (X86_64Gpr, X86_64Addr) -> X86_64Assembler,
		emit_r_r: (X86_64Gpr, X86_64Gpr) -> X86_64Assembler) {
		var x = tryFold_qq_q(fold)
			|| do_op2_r_i(ValueKind.I64, emit_r_i)
			|| do_op2_r_m(ValueKind.I64, emit_r_m)
			|| do_op2_r_r(ValueKind.I64, emit_r_r);
	}
	def visit_I64_ADD() { visitSimpleOp2_qq_q(V3Eval.I64_ADD, asm.q.add_r_i, asm.q.add_r_m, asm.q.add_r_r); }
	def visit_I64_SUB() { visitSimpleOp2_qq_q(V3Eval.I64_SUB, asm.q.sub_r_i, asm.q.sub_r_m, asm.q.sub_r_r); }
	def visit_I64_MUL() { visitSimpleOp2_qq_q(V3Eval.I64_MUL, asm.q.imul_r_i, asm.q.imul_r_m, asm.q.imul_r_r); }
	def visit_I64_DIV_S() { visitIDivRem(X86_64Regs2.RAX, mmasm.emit_i64_div_s); }
	def visit_I64_DIV_U() { visitIDivRem(X86_64Regs2.RAX, mmasm.emit_i64_div_u); }
	def visit_I64_REM_S() { visitIDivRem(X86_64Regs2.RDX, mmasm.emit_i64_rem_s); }
	def visit_I64_REM_U() { visitIDivRem(X86_64Regs2.RDX, mmasm.emit_i64_rem_u); }
	def visit_I64_SHL() { var x = tryFold_qq_q(V3Eval.I64_SHL) || visitShift(ValueKind.I64, asm.q.shl_r_i, asm.q.shl_r_cl); }
	def visit_I64_SHR_S() { var x = tryFold_ll_l(V3Eval.I64_SHR_S) || visitShift(ValueKind.I64, asm.q.sar_r_i, asm.q.sar_r_cl); }
	def visit_I64_SHR_U() { var x = tryFold_qq_q(V3Eval.I64_SHR_U) || visitShift(ValueKind.I64, asm.q.shr_r_i, asm.q.shr_r_cl); }
	def visit_I64_ROTL() { var x = tryFold_qq_q(V3Eval.I64_ROTL) || visitShift(ValueKind.I64, asm.q.rol_r_i, asm.q.rol_r_cl); }
	def visit_I64_ROTR() { var x = tryFold_qq_q(V3Eval.I64_ROTR) || visitShift(ValueKind.I64, asm.q.ror_r_i, asm.q.ror_r_cl); }
	def visit_I64_AND() { visitSimpleOp2_qq_q(V3Eval.I64_AND, asm.q.and_r_i, asm.q.and_r_m, asm.q.and_r_r); }
	def visit_I64_OR()  { visitSimpleOp2_qq_q(V3Eval.I64_OR, asm.q.or_r_i, asm.q.or_r_m, asm.q.or_r_r); }
	def visit_I64_XOR() { visitSimpleOp2_qq_q(V3Eval.I64_XOR, asm.q.xor_r_i, asm.q.xor_r_m, asm.q.xor_r_r); }

	// XXX: try s_m addressing mode for floating point ops
	def visit_F32_ABS() {
		var sv = popReg(), r = X(sv.reg), scratch = mmasm.scratch;
		var d = allocRegTos(ValueKind.F32);
		asm.movd_r_s(scratch, X(sv.reg));
		asm.d.and_r_i(scratch, 0x7FFFFFFF);
		asm.movd_s_r(X(d), scratch);
		state.push(sv.kindFlagsAndTag(IN_REG), d, 0);
		freeVal(sv);
	}
	def visit_F32_NEG() {
		var sv = popReg(), r = X(sv.reg), scratch = mmasm.scratch;
		var d = allocRegTos(ValueKind.F32);
		asm.movd_r_s(scratch, X(sv.reg));
		asm.d.xor_r_i(scratch, 0x80000000);
		asm.movd_s_r(X(d), scratch);
		state.push(sv.kindFlagsAndTag(IN_REG), d, 0);
		freeVal(sv);
	}
	def visit_F32_CEIL() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_POS_INF)); }
	def visit_F32_FLOOR() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_NEG_INF)); }
	def visit_F32_TRUNC() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_ZERO)); }
	def visit_F32_NEAREST() { do_op1_x_x(ValueKind.F32, asm.roundss_s_s(_, _, X86_64Rounding.TO_NEAREST)); }
	def visit_F32_SQRT() { do_op1_x_x(ValueKind.F32, asm.sqrtss_s_s); }
	def visit_F32_ADD() { do_op2_x_x(ValueKind.F32, asm.addss_s_s); }
	def visit_F32_SUB() { do_op2_x_x(ValueKind.F32, asm.subss_s_s); }
	def visit_F32_MUL() { do_op2_x_x(ValueKind.F32, asm.mulss_s_s); }
	def visit_F32_DIV() { do_op2_x_x(ValueKind.F32, asm.divss_s_s); }

	def visitFloatMinOrMax(is64: bool, isMin: bool) { // XXX: move to macro assembler?
		var b = popReg(), a = popRegToOverwrite();
		var ret_a = X86_64Label.new(), ret_b = X86_64Label.new(), is_nan = X86_64Label.new();
		var done = X86_64Label.new();
		var xmmA = X(a.reg), xmmB = X(b.reg);

		if (is64) asm.ucomisd_s_s(xmmA, xmmB);
		else asm.ucomiss_s_s(xmmA, xmmB);
		asm.jc_rel_far(C.P, is_nan);
		asm.jc_rel_near(C.C, if(isMin, ret_a, ret_b));
		asm.jc_rel_near(C.A, if(isMin, ret_b, ret_a));
		asm.movq_r_s(mmasm.scratch, xmmB); // XXX: does a 32-bit move save anything?
		if (is64) asm.q.cmp_r_i(mmasm.scratch, 0);
		else asm.d.cmp_r_i(mmasm.scratch, 0);
		asm.jc_rel_near(if(isMin, C.S, C.NS), ret_b); // handle min(-0, 0) == -0
		asm.jmp_rel_near(ret_a);

		asm.bind(is_nan);
		if (is64) {
			asm.movd_r_i(mmasm.scratch, int.view(Floats.d_nan >> 32));
			asm.q.shl_r_i(mmasm.scratch, 32);
			asm.movq_s_r(xmmA, mmasm.scratch);
		} else {
			masm.emit_mov_r_f32(a.reg, int.view(Floats.f_nan));
		}
		asm.jmp_rel_near(done);
		
		asm.bind(ret_b);
		if (is64) asm.movsd_s_s(xmmA, xmmB); // fallthru
		else asm.movss_s_s(xmmA, xmmB); // fallthru
		asm.bind(ret_a); // nop
		asm.bind(done);
		
		state.push(a.kindFlagsAndTag(IN_REG), a.reg, 0);
		freeVal(b);
	}
	def visit_F32_MIN() { visitFloatMinOrMax(false, true); }
	def visit_F32_MAX() { visitFloatMinOrMax(false, false); }
	def visit_F32_COPYSIGN() {
		var sv = popReg(), d = popRegToOverwrite(); // XXX: constant-fold and strength reduce
		var t1 = allocTmp(ValueKind.I32), t2 = allocTmp(ValueKind.I32);
		mmasm.emit_f32_copysign(X(d.reg), X(sv.reg), G(t1), G(t2));
		freeVal(sv);
		state.push(d.kindFlagsAndTag(IN_REG), d.reg, 0);
	}

	// XXX: try s_m addressing mode for floating point ops
	def visit_F64_ABS() {
		var sv = popRegToOverwrite();
		var t = allocTmp(ValueKind.I64), r1 = G(t);
		asm.movq_r_s(r1, X(sv.reg));
		asm.q.rol_r_i(r1, 1);
		asm.q.and_r_i(r1, -2);
		asm.q.ror_r_i(r1, 1);
		asm.movq_s_r(X(sv.reg), G(t));
		state.push(sv.kindFlagsAndTag(IN_REG), sv.reg, 0);
	}
	def visit_F64_NEG() {
		var sv = popRegToOverwrite();
		var t = allocTmp(ValueKind.I64), r1 = G(t);
		asm.movq_r_s(r1, X(sv.reg));
		asm.q.rol_r_i(r1, 1);
		asm.q.xor_r_i(r1, 1);
		asm.q.ror_r_i(r1, 1);
		asm.movq_s_r(X(sv.reg), G(t));
		state.push(sv.kindFlagsAndTag(IN_REG), sv.reg, 0);
	}
	def visit_F64_CEIL() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_POS_INF)); }
	def visit_F64_FLOOR() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_NEG_INF)); }
	def visit_F64_TRUNC() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_ZERO)); }
	def visit_F64_NEAREST() { do_op1_x_x(ValueKind.F64, asm.roundsd_s_s(_, _, X86_64Rounding.TO_NEAREST)); }
	def visit_F64_SQRT() { do_op1_x_x(ValueKind.F64, asm.sqrtsd_s_s); }
	def visit_F64_ADD() { do_op2_x_x(ValueKind.F64, asm.addsd_s_s); }
	def visit_F64_SUB() { do_op2_x_x(ValueKind.F64, asm.subsd_s_s); }
	def visit_F64_MUL() { do_op2_x_x(ValueKind.F64, asm.mulsd_s_s); }
	def visit_F64_DIV() { do_op2_x_x(ValueKind.F64, asm.divsd_s_s); }
	def visit_F64_MIN() { visitFloatMinOrMax(true, true); }
	def visit_F64_MAX() { visitFloatMinOrMax(true, false); }
	def visit_F64_COPYSIGN() {
		var sv = popReg(), d = popRegToOverwrite(); // XXX: constant-fold and strength reduce
		var t1 = allocTmp(ValueKind.I64), t2 = allocTmp(ValueKind.I64);
		mmasm.emit_f64_copysign(X(d.reg), X(sv.reg), G(t1), G(t2));
		freeVal(sv);
		state.push(d.kindFlagsAndTag(IN_REG), d.reg, 0);
	}

	private def visitITruncF(opcode: Opcode) {
		var x1 = popReg();
		var dkind = if(opcode.sig.results[0] == ValueType.I32, ValueKind.I32, ValueKind.I64);
		var xscratch = allocTmp(if(opcode.sig.params[0] == ValueType.F32, ValueKind.F32, ValueKind.F64));
		var dst = allocRegTos(dkind);
		mmasm.emit_i_trunc_f(opcode, G(dst), X(x1.reg), X(xscratch));
		freeVal(x1);
		state.push(SpcConsts.kindToFlags(dkind) | IN_REG, dst, 0);
	}

	def visit_I32_WRAP_I64() {
		var x = tryFold_l_i(i32.view<i64>)
			|| do_op1_r_r(ValueKind.I32, asm.movd_r_r);
	}
	def visit_I32_TRUNC_F32_S() { visitITruncF(Opcode.I32_TRUNC_F32_S); }
	def visit_I32_TRUNC_F32_U() { visitITruncF(Opcode.I32_TRUNC_F32_U); }
	def visit_I32_TRUNC_F64_S() { visitITruncF(Opcode.I32_TRUNC_F64_S); }
	def visit_I32_TRUNC_F64_U() { visitITruncF(Opcode.I32_TRUNC_F64_U); }
	def visit_I64_EXTEND_I32_S() {
		var x = tryFold_i_l(i64.view<i32>)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_s);
	}
	def visit_I64_TRUNC_F32_S() { visitITruncF(Opcode.I64_TRUNC_F32_S); }
	def visit_I64_TRUNC_F32_U() { visitITruncF(Opcode.I64_TRUNC_F32_U); }
	def visit_I64_TRUNC_F64_S() { visitITruncF(Opcode.I64_TRUNC_F64_S); }
	def visit_I64_TRUNC_F64_U() { visitITruncF(Opcode.I64_TRUNC_F64_U); }
	def visit_I64_EXTEND_I32_U() {
		var x = tryFold_u_l(i64.view<u32>)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_u);
	}
	def visitReinterpret(kind: ValueKind) {
		var sv = state.pop(), flags = SpcConsts.kindToFlags(kind);
		if (sv.isConst()) {
			state.push(flags | IS_CONST, NO_REG, sv.const);
		} else if (sv.inReg()) {
			var d = allocRegTos(kind);
			match (kind) {
				I32 => asm.movd_r_s(G(d), X(sv.reg));
				I64 => asm.movq_r_s(G(d), X(sv.reg));
				F32 => asm.movd_s_r(X(d), G(sv.reg));
				F64 => asm.movq_s_r(X(d), G(sv.reg));
				_ => bailout("unexpected value kind");
			}
			state.push(flags | IN_REG, d, 0);
			freeVal(sv);
		} else {
			state.push(flags | IS_STORED, NO_REG, 0);
		}
	}
	def visit_I32_REINTERPRET_F32() { visitReinterpret(ValueKind.I32); }
	def visit_I64_REINTERPRET_F64() { visitReinterpret(ValueKind.I64); }
	def visit_F32_REINTERPRET_I32() { visitReinterpret(ValueKind.F32); }
	def visit_F64_REINTERPRET_I64() { visitReinterpret(ValueKind.F64); }

	def visit_I32_EXTEND8_S() {
		var x = tryFold_i_i(V3Eval.I32_EXTEND8_S)
			|| do_op1_r_r(ValueKind.I32, asm.d.movbsx_r_r);
	}
	def visit_I32_EXTEND16_S() {
		var x = tryFold_i_i(V3Eval.I32_EXTEND16_S)
			|| do_op1_r_r(ValueKind.I32, asm.d.movwsx_r_r);
	}
	def visit_I64_EXTEND8_S() {
		var x = tryFold_l_l(V3Eval.I64_EXTEND8_S)
			|| do_op1_r_r(ValueKind.I64, asm.q.movbsx_r_r);
	}
	def visit_I64_EXTEND16_S() {
		var x = tryFold_l_l(V3Eval.I64_EXTEND16_S)
			|| do_op1_r_r(ValueKind.I64, asm.q.movwsx_r_r);
	}
	def visit_I64_EXTEND32_S() {
		var x = tryFold_l_l(V3Eval.I64_EXTEND32_S)
			|| do_op1_r(ValueKind.I64, mmasm.emit_i64_extend_i32_s);
	}
	def visit_REF_EQ()   { visitCompareI(asm.q, C.Z); }

	def visit_I32_TRUNC_SAT_F32_S() { visitITruncF(Opcode.I32_TRUNC_SAT_F32_S); }
	def visit_I32_TRUNC_SAT_F32_U() { visitITruncF(Opcode.I32_TRUNC_SAT_F32_U); }
	def visit_I32_TRUNC_SAT_F64_S() { visitITruncF(Opcode.I32_TRUNC_SAT_F64_S); }
	def visit_I32_TRUNC_SAT_F64_U() { visitITruncF(Opcode.I32_TRUNC_SAT_F64_U); }
	def visit_I64_TRUNC_SAT_F32_S() { visitITruncF(Opcode.I64_TRUNC_SAT_F32_S); }
	def visit_I64_TRUNC_SAT_F32_U() { visitITruncF(Opcode.I64_TRUNC_SAT_F32_U); }
	def visit_I64_TRUNC_SAT_F64_S() { visitITruncF(Opcode.I64_TRUNC_SAT_F64_S); }
	def visit_I64_TRUNC_SAT_F64_U() { visitITruncF(Opcode.I64_TRUNC_SAT_F64_U); }

	private def visitFConvertI32S(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr) -> X86_64Assembler) {
		var sv = popReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		asm.q.shl_r_i(r1, 32);
		asm.q.sar_r_i(r1, 32); // sign-extend
		emit_cvt(X(d), r1);
		freeVal(sv);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	def visit_F32_CONVERT_I32_S() { visitFConvertI32S(ValueKind.F32, asm.cvtsi2ss_s_r); }
	def visit_F64_CONVERT_I32_S() { visitFConvertI32S(ValueKind.F64, asm.cvtsi2sd_s_r); }
	private def visitFConvertI32U(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr) -> X86_64Assembler) {
		var sv = popReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		asm.movd_r_r(r1, r1); // zero-extend
		emit_cvt(X(d), r1);
		freeVal(sv);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	def visit_F32_CONVERT_I32_U() { visitFConvertI32U(ValueKind.F32, asm.cvtsi2ss_s_r); }
	def visit_F64_CONVERT_I32_U() { visitFConvertI32U(ValueKind.F64, asm.cvtsi2sd_s_r); }
	private def visitFConvertI64S(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr) -> X86_64Assembler) {
		var sv = popReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		emit_cvt(X(d), r1);
		freeVal(sv);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	private def visitFConvertI64U(kind: ValueKind, emit_cvt: (X86_64Xmmr, X86_64Gpr, X86_64Xmmr, X86_64Gpr) -> void) {
		var sv = popReg(), r1 = G(sv.reg);
		var d = allocRegTos(kind);
		var xscratch = allocTmp(kind);
		emit_cvt(X(d), r1, X(xscratch), mmasm.scratch);
		freeVal(sv);
		state.push(SpcConsts.kindToFlags(kind) | IN_REG, d, 0);
	}
	def visit_F32_CONVERT_I64_S() { visitFConvertI64S(ValueKind.F32, asm.cvtsi2ss_s_r); }
	def visit_F32_CONVERT_I64_U() { visitFConvertI64U(ValueKind.F32, mmasm.emit_f32_convert_i64_u); }
	def visit_F64_CONVERT_I64_S() { visitFConvertI64S(ValueKind.F64, asm.cvtsi2sd_s_r); }
	def visit_F64_CONVERT_I64_U() { visitFConvertI64U(ValueKind.F64, mmasm.emit_f64_convert_i64_u); }

	def visit_F32_DEMOTE_F64() { do_op1_x_x(ValueKind.F32, asm.cvtsd2ss_s_s); } // XXX: try s_m addr mode
	def visit_F64_PROMOTE_F32() { do_op1_x_x(ValueKind.F64, asm.cvtss2sd_s_s); } // XXX: try s_m addr mode

	// r1 = op(r1)
	private def do_op1_r<T>(kind: ValueKind, emit: (X86_64Gpr -> T)) -> bool {
		var sv = popRegToOverwrite(), r = G(sv.reg);
		emit(r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), sv.reg, 0);
		return true;
	}
	// r1 = op(r2)
	private def do_op1_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr) -> T) -> bool {
		var sv = popReg(), r = G(sv.reg);
		var d = allocRegTos(kind);
		emit(G(d), r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), d, 0);
		freeVal(sv);
		return true;
	}
	// r1 = op(r1, r2)
	private def do_op2_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr) -> T) -> bool {
		var b = popReg();
		var a = popRegToOverwrite();
		emit(G(a.reg), G(b.reg));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		freeVal(b);
		return true;
	}
	// r1 = op(r1, m2)
	private def do_op2_r_m<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Addr) -> T) -> bool {
		var b = state.peek();
		if (b.inReg() || b.isConst()) return false;
		state.pop();
		var addr = masm.slotAddr(state.sp);
		var a = popRegToOverwrite();
		emit(G(a.reg), A(addr));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// r1 = op(r1, imm)
	private def do_op2_r_i<T>(kind: ValueKind, emit: (X86_64Gpr, int) -> T) -> bool {
		var b = state.peek();
		if (!b.isConst()) return false;
		state.pop();
		var a = popRegToOverwrite();
		emit(G(a.reg), b.const);
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		return true;
	}
	// r1 = op(r2, imm)
	private def do_op2_r_r_i<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, int) -> T) -> bool;
	// r1 = op(r2, m3)
	private def do_op2_r_r_m<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, X86_64Addr) -> T) -> bool;
	// r1 = op(r2, r3)
	private def do_op2_r_r_r<T>(kind: ValueKind, emit: (X86_64Gpr, X86_64Gpr, X86_64Gpr) -> T) -> bool;
	// r1 = op(r2)
	private def do_op1_x_x<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var sv = popReg(), r = X(sv.reg);
		var d = allocRegTos(kind);
		emit(X(d), r);
		state.push(sv.kindFlagsMatching(kind, IN_REG), d, 0);
		freeVal(sv);
		return true;
	}
	// x1 = op(x1, x2)
	private def do_op2_x_x<T>(kind: ValueKind, emit: (X86_64Xmmr, X86_64Xmmr) -> T) -> bool {
		var b = popReg();
		var a = popRegToOverwrite();
		emit(X(a.reg), X(b.reg));
		state.push(a.kindFlagsMatching(kind, IN_REG), a.reg, 0);
		freeVal(b);
		return true;
	}
}

def ucontext_rip_offset = 168;
def ucontext_rsp_offset = 160;
def SIGFPE  = 8;
def SIGBUS  = 10;
def SIGSEGV = 11;

// Implements the RiUserCode interface to add generated machine code to the V3 runtime.
// Handles stackwalking and signals in JITed code.
class X86_64SpcCode extends RiUserCode {
	def mapping: Mapping;
	def frameSize = IVarConfig.frameSize;
	var oobMemoryHandlerOffset: int;	// handler for signals caused by OOB memory access
	var divZeroHandlerOffset: int;		// handler for signals caused by divide by zero
	var stackOverflowHandlerOffset: int;	// handler for signals caused by (value- or call-) stack overflow
	var buf = StringBuilder.new().grow(128);  // avoid allocations when describing frames

	new(mapping) super(mapping.range.start, mapping.range.end) { }

	// Called from V3 runtime upon fatal errors to describe a frame for a stacktrace.
	def describeFrame(ip: Pointer, sp: Pointer, out: (Array<byte>, int, int) -> ()) {
		var msg = "\tin [spc-jit] ";
		out(msg, 0, msg.length);
		var instance = (sp + IVarConfig.frame.INSTANCE.disp).load<Instance>();
		var wf = (sp + IVarConfig.frame.WASM_FUNC.disp).load<WasmFunction>();
		// TODO: lazy parse of names section may allocate; must avoid this in OOM situation
		wf.decl.render(instance.module.names, buf);
		buf.ln().out(out);
		buf.reset();
	}

	// Called from V3 runtime for a frame where {ip} is in JIT code.
	def nextFrame(ip: Pointer, sp: Pointer) -> (Pointer, Pointer) {
		sp += frameSize;	 // assume frame is allocated
		ip = sp.load<Pointer>(); // return address on stack
		return (ip + -1, sp + Pointer.SIZE); // XXX: V3 quirk with -1 (use RiOs?)
	}

	// Called from V3 runtime when the garbage collector needs to scan a JIT stack frame.
	def scanFrame(ip: Pointer, sp: Pointer) {
		// Handle other roots in the frame
		RiGc.scanRoot(sp + IVarConfig.frame.WASM_FUNC.disp);
		RiGc.scanRoot(sp + IVarConfig.frame.INSTANCE.disp);
		RiGc.scanRoot(sp + IVarConfig.frame.ACCESSOR.disp);
	}

	// Called from V3 runtime to handle an OS-level signal that occurred while {ip} was in JIT code.
	def handleSignal(signum: int, siginfo: Pointer, ucontext: Pointer, ip: Pointer, sp: Pointer) -> bool {
		var pip = ucontext + ucontext_rip_offset;
		var ip = pip.load<Pointer>();
		if (Trace.interpreter) {
			Trace.OUT.put2("  !signal %d in JIT code @ 0x%x", signum, ip - Pointer.NULL).outln();
		}
		match (signum) {
			SIGFPE => {
				// presume divide/modulus by zero
				pip.store<Pointer>(start + divZeroHandlerOffset);
				return true;
			}
			SIGBUS, SIGSEGV => {
				var addr = RiOs.getAccessAddress(siginfo, ucontext);
				if (RedZones.isInRedZone(addr)) {
					pip.store<Pointer>(start + stackOverflowHandlerOffset);
					return true;
				}
				pip.store<Pointer>(start + oobMemoryHandlerOffset);
				return true;
			}
		}
		return true;
	}
	def keepAlive() { // XXX: need to trick V3 whole-program optimizer to not delete these fields
		if (mapping == null) System.error(null, null);
		if (mapping.range == null) System.error(null, null);
	}
	// Get a frame accessor for the probe API.
	def getFrameAccessor(sp: Pointer) -> X86_64SpcFrameAccessor {
		var retip = (sp + -Pointer.SIZE).load<Pointer>();
		if (!mapping.range.contains(sp)) return null;
		if (true) { // XXX: tuning for caching frame accessor object
			var prev = (sp + IVarConfig.frame.ACCESSOR.disp).load<X86_64SpcFrameAccessor>();
			if (prev != null) return prev;
			var n = X86_64SpcFrameAccessor.new(sp);
			(sp + IVarConfig.frame.ACCESSOR.disp).store<X86_64SpcFrameAccessor>(n);
			return n;
		}
		return X86_64SpcFrameAccessor.new(sp);
	}
}

// Implements frame accessor for JIT compiled method frames. For SPC, the frame layout is almost
// identical to interpreter frames, so it inherits from the base implementation.
class X86_64SpcFrameAccessor extends X86_64BaseFrameAccessor {
	new(sp: Pointer) super(sp) {
		var wf = (sp + IVarConfig.frame.WASM_FUNC.disp).load<WasmFunction>();
		decl = wf.decl;
	}

	// Returns {true} if this frame has been unwound, either due to returning, a trap, or exception.
	def isUnwound() -> bool {
		return this != (sp + IVarConfig.frame.ACCESSOR.disp).load<FrameAccessor>();
	}
	// Returns the current program counter.
	def pc() -> int {
		return 0; // TODO: compute PC from return IP of callee
	}
	// Set the value of a local variable. (dynamically typechecked).
	def setLocal(i: int, v: Value); // TODO: translate to interpreter frame
	// Set operand at depth {i}, with 0 being the top of the stack, -1 being one lower, etc. (dynamically typechecked).
	def setOperand(i: int, v: Value); // TODO: translate to interpreter frame
}