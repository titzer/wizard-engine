// Copyright 2022 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// The {V3Eval} component contains canonical Virgil-level implementations of many Wasm operations.
// These operations work on Virgil-level values like {u32} and {float}, not boxed {Value}s so they
// can be reused in multiple different ways.
// For example, the {V3Interpreter} handles value stacks and boxes but ultimate calls these
// routines, and the compiler(s) use these routines during constant-folding, thus guaranteeing
// the same semantics.
component V3Eval {
	// ---- i32 comparisons ------------------------------------------------
	def I32_EQZ	= u32.==(0, _);
	def I32_EQ	= u32.==;
	def I32_NE	= u32.!=;
	def I32_LT_S	= i32.<;
	def I32_LT_U	= u32.<;
	def I32_GT_S	= i32.>;
	def I32_GT_U	= u32.>;
	def I32_LE_S	= i32.<=;
	def I32_LE_U	= u32.<=;
	def I32_GE_S	= i32.>=;
	def I32_GE_U	= u32.>=;

	// ---- i64 comparisons ------------------------------------------------
	def I64_EQZ	= u64.==(0, _);
	def I64_EQ	= u64.==;
	def I64_NE	= u64.!=;
	def I64_LT_S	= i64.<;
	def I64_LT_U	= u64.<;
	def I64_GT_S	= i64.>;
	def I64_GT_U	= u64.>;
	def I64_LE_S	= i64.<=;
	def I64_LE_U	= u64.<=;
	def I64_GE_S	= i64.>=;
	def I64_GE_U	= u64.>=;

	// ---- f32 comparisons ------------------------------------------------
	def F32_EQ	= float.==;
	def F32_NE	= float.!=;
	def F32_LT	= float.<;
	def F32_GT	= float.>;
	def F32_LE	= float.<=;
	def F32_GE	= float.>=;

	// ---- f64 comparisons ------------------------------------------------
	def F64_EQ	= double.==;
	def F64_NE	= double.!=;
	def F64_LT	= double.<;
	def F64_GT	= double.>;
	def F64_LE	= double.<=;
	def F64_GE	= double.>=;

	// ---- i32 arithmetic -------------------------------------------------
	def I32_CLZ(x: u32) -> u32 {
		var count = 0u;
		if (x == 0) return 32;
		while ((x & 0x80000000u) == 0) { count++; x <<= 1; }
		return count;
	}
	def I32_CTZ(x: u32) -> u32 {
		var count = 0u;
		if (x == 0) return 32;
		while ((x & 1u) == 0) { count++; x >>= 1; }
		return count;
	}
	def I32_POPCNT(x: u32) -> u32 {
		var count = 0u;
		for (i < 32) {
			if ((x & 1) == 1) count++;
			x >>= 1;
		}
		return count;
	}
	def I32_ADD	= u32.+;
	def I32_SUB	= u32.-;
	def I32_MUL	= u32.*;
	def I32_DIV_S(x: i32, y: i32) -> (i32, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		if (y == -1 && x == int.min) return (0, TrapReason.DIV_UNREPRESENTABLE);
		return (x / y, TrapReason.NONE);
	}
	def I32_DIV_U(x: u32, y: u32) -> (u32, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		return (x / y, TrapReason.NONE);
	}
	def I32_REM_S(x: i32, y: i32) -> (i32, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		return (if (y == -1, 0, x % y), TrapReason.NONE);
	}
	def I32_REM_U(x: u32, y: u32) -> (u32, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		return (if(y == 1, 0, x % y), TrapReason.NONE);
	}
	def I32_AND	= u32.&;
	def I32_OR	= u32.|;
	def I32_XOR	= u32.^;
	def I32_SHL(x: i32, y: i32) -> i32 {
		return x << u5.view(y);
	}
	def I32_SHR_S(x: i32, y: i32) -> i32 {
		return x >> u5.view(y);
	}
	def I32_SHR_U(x: i32, y: i32) -> i32 {
		return x >>> u5.view(y);
	}
	def I32_ROTL(x: u32, z: u32) -> u32 {
		var y = u5.view(z);
		if (y != 0) {
			var upper = x << y;
			var lower = x >> u6.view(32) - y;
			x = upper | lower;
		}
		return x;
	}
	def I32_ROTR(x: u32, z: u32) -> u32 {
		var y = u5.view(z);
		if (y != 0) {
			var upper = x << u6.view(32) - y;
			var lower = x >> y;
			x = upper | lower;
		}
		return x;
	}

	// ---- i64 arithmetic -------------------------------------------------
	def I64_CLZ(x: u64) -> u64 {
		var count = 0u;
		if (x == 0) return 64;
		while ((x & 0x8000000000000000ul) == 0) { count++; x <<= 1; }
		return count;
	}
	def I64_CTZ(x: u64) -> u64 {
		var count = 0u;
		if (x == 0) return 64;
		while ((x & 1u) == 0) { count++; x >>= 1; }
		return count;
	}
	def I64_POPCNT(x: u64) -> u64 {
		var count = 0u;
		for (i < 64) {
			if ((x & 1) == 1) count++;
			x >>= 1;
		}
		return count;
	}
	def I64_ADD	= u64.+;
	def I64_SUB	= u64.-;
	def I64_MUL	= u64.*;
	def I64_DIV_S(x: i64, y: i64) -> (i64, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		if (y == -1 && x == long.min) return (0, TrapReason.DIV_UNREPRESENTABLE);
		return (x / y, TrapReason.NONE);
	}
	def I64_DIV_U(x: u64, y: u64) -> (u64, TrapReason) {
		if (y == 0) return(0, TrapReason.DIV_BY_ZERO);
		return (x / y, TrapReason.NONE);
	}
	def I64_REM_S(x: i64, y: i64) -> (i64, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		return (if(y == -1, 0, x % y), TrapReason.NONE);
	}
	def I64_REM_U(x: u64, y: u64) -> (u64, TrapReason) {
		if (y == 0) return (0, TrapReason.DIV_BY_ZERO);
		return (if(y == 1, 0, x % y), TrapReason.NONE);
	}
	def I64_AND	= u64.&;
	def I64_OR	= u64.|;
	def I64_XOR	= u64.^;
	def I64_SHL(x: u64, y: u64) -> u64 {
		return x << u6.view(y);
	}
	def I64_SHR_S(x: i64, y: i64) -> i64 {
		return x >> u6.view(y);
	}
	def I64_SHR_U(x: u64, y: u64) -> u64 {
		return x >> u6.view(y);
	}
	def I64_ROTL(x: u64, z: u64) -> u64 {
		var y = u6.view(z);
		if (y != 0) {
			var upper = x << y;
			var lower = x >> byte.view(64) - y;
			x = upper | lower;
		}
		return x;
	}
	def I64_ROTR(x: u64, z: u64) -> u64 {
		var y = u6.view(z);
		if (y != 0) {
			var upper = x << byte.view(64) - y;
			var lower = x >> y;
			x = upper | lower;
		}
		return x;
	}

	// ---- f32 arithmetic -------------------------------------------------
	def F32_ABS	= float.abs;
	def F32_NEG(a: float) -> float {
		return float.view(0x80000000u ^ u32.view(a));
	}
	def F32_CEIL(a: float) -> float {
		return canonf(float.ceil(a));
	}
	def F32_FLOOR(a: float) -> float {
		return canonf(float.floor(a));
	}
	def F32_TRUNC(a: float) -> float {
		if (a < 0f) {
			if (a > -1f) return -0f;  // handle -0
			return 0f - float.floor(0f - a);
		}
		return canonf(float.floor(a));
	}
	def F32_NEAREST	= float.round;
	def F32_SQRT	= float.sqrt;
	def F32_ADD	= float.+;
	def F32_SUB	= float.-;
	def F32_MUL	= float.*;
	def F32_DIV	= float./;
	def F32_MIN(a: float, b: float) -> float {
		if (a < b) return a;
		if (a == b) return if(b.sign == 1, b, a); // handle -0
		if (b < a) return b;
		return float.nan;
	}
	def F32_MAX(a: float, b: float) -> float {
		if (a > b) return a;
		if (a == b) return if(b.sign == 0, b, a); // handle -0
		if (b > a) return b;
		return float.nan;
	}
	def F32_COPYSIGN(a: float, b: float) -> float {
		var aa = 0x7fffffffu & u32.view(a);
		var bb = 0x80000000u & u32.view(b);
		return float.view(aa | bb);
	}

	// ---- f64 arithmetic -------------------------------------------------
	def F64_ABS	= double.abs;
	def F64_NEG(a: double) -> double {
		return double.view(0x8000000000000000uL ^ u64.view(a));
	}
	def F64_CEIL(a: double) -> double {
		return canond(double.ceil(a));
	}
	def F64_FLOOR(a: double) -> double {
		return canond(double.floor(a));
	}
	def F64_TRUNC(a: double) -> double {
		if (a < 0d) {
			if (a > -1d) return -0d;  // handle -0
			return 0d - double.floor(0d - a);
		}
		return canond(double.floor(a));
	}
	def F64_NEAREST	= double.round;
	def F64_SQRT	= double.sqrt;
	def F64_ADD	= double.+;
	def F64_SUB	= double.-;
	def F64_MUL	= double.*;
	def F64_DIV	= double./;
	def F64_MIN(a: double, b: double) -> double {
		if (a < b) return a;
		if (a == b) return if(b.sign == 1, b, a); // handle -0
		if (b < a) return b;
		return double.nan;
	}
	def F64_MAX(a: double, b: double) -> double {
		if (a > b) return a;
		if (a == b) return if(b.sign == 0, b, a); // handle -0
		if (b > a) return b;
		return double.nan;
	}
	def F64_COPYSIGN(a: double, b: double) -> double {
		var aa = 0x7fffffffffffffffuL & u64.view(a);
		var bb = 0x8000000000000000uL & u64.view(b);
		return double.view(aa | bb);
	}

	// ---- v128 arithmetic ---------------------------------------------
	def V128_NOT = do_ss_s_x2(_, (u64.max, u64.max), u64.^);
	def V128_AND = do_ss_s_x2(_, _, u64.&);
	def V128_OR = do_ss_s_x2(_, _, u64.|);
	def V128_XOR = do_ss_s_x2(_, _, u64.^);
	def V128_BITSELECT(a: (u64, u64), b: (u64, u64), c: (u64, u64)) -> (u64, u64) {
		// Equivalent to v128.or(v128.and(a, c), v128.and(b, v128.not(c))).
		var not_c = V128_NOT(c);
		var and_ac = V128_AND(a, c);
		var and_b_not_c = V128_AND(b, not_c);
		return V128_OR(and_ac, and_b_not_c);
	}
	def V128_ANDNOT(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
		var not_b = V128_NOT(b);
		return V128_AND(a, not_b);
	}
	def V128_ANYTRUE(a: (u64, u64)) -> bool {
		return (a.0 | a.1) != 0;
	}
	def I8X16_ALLTRUE =		do_v128_alltrue(_, 8, i8.view<u64>, i8.==);
	def I16X8_ALLTRUE =		do_v128_alltrue(_, 16, i16.view<u64>, i16.==);
	def I32X4_ALLTRUE =		do_v128_alltrue(_, 32, i32.view<u64>, i32.==);
	def I64X2_ALLTRUE =		do_v128_alltrue(_, 64, i64.view<u64>, i64.==);
	def I8X16_BITMASK =		do_v128_bitmask(_, 8, 3);
	def I16X8_BITMASK =		do_v128_bitmask(_, 16, 4);
	def I32X4_BITMASK =		do_v128_bitmask(_, 32, 5);
	def I64X2_BITMASK =		do_v128_bitmask(_, 64, 6);
	def I64X2_ADD =			do_ss_s_x2(_, _, u64.+);
	def I64X2_SUB =			do_ss_s_x2(_, _, u64.-);
	def I64X2_MUL =			do_ss_s_x2(_, _, u64.*);
	def I64X2_NEG =			do_ss_s_x2((0, 0), _, u64.-);
	def I64X2_ABS =			do_s_s_x2(_, V128_I64_ABS);
	def I64X2_EQ =			do_ss_s_x2(_, _, V128_I64X2_EQ);
	def I64X2_NE =			do_ss_s_x2(_, _, V128_I64X2_NE);
	def I64X2_LT_S =		do_ss_s_x2(_, _, V128_I64X2_LT_S);
	def I64X2_LE_S =		do_ss_s_x2(_, _, V128_I64X2_LE_S);
	def I64X2_GT_S =		commute_binop(I64X2_LT_S);
	def I64X2_GE_S =		commute_binop(I64X2_LE_S);
	def I32X4_ADD =			do_ss_s_x4(_, _, u32.+);
	def I32X4_SUB =			do_ss_s_x4(_, _, u32.-);
	def I32X4_MUL =			do_ss_s_x4(_, _, u32.*);
	def I32X4_NEG =			do_ss_s_x4((0, 0), _, u32.-);
	def I32X4_DOT_I16X8_S =		do_v128_dotprod;
	def I32X4_EXTADDPAIRWISE_I16X8_S =	do_s_s_x8_pairwise_ext_x4(_, V128_I32X4_EXTADD_16X8_S);
	def I32X4_EXTADDPAIRWISE_I16X8_U =	do_s_s_x8_pairwise_ext_x4(_, I32X4_EXTADD_I16X8_U);
	def I32X4_MIN_S =		do_ss_s_x4(_, _, V128_I32_MIN_S);
	def I32X4_MIN_U =		do_ss_s_x4(_, _, I32_MIN_U);
	def I32X4_MAX_S =		do_ss_s_x4(_, _, V128_I32_MAX_S);
	def I32X4_MAX_U =		do_ss_s_x4(_, _, I32_MAX_U);
	def I32X4_ABS =			do_s_s_x4(_, V128_I32_ABS);
	def I32X4_EQ =			do_ss_s_x4(_, _, V128_I32X4_EQ);
	def I32X4_NE =			do_ss_s_x4(_, _, V128_I32X4_NE);
	def I32X4_LT_S =		do_ss_s_x4(_, _, V128_I32X4_LT_S);
	def I32X4_LT_U =		do_ss_s_x4(_, _, V128_I32X4_LT_U);
	def I32X4_LE_S =		do_ss_s_x4(_, _, V128_I32X4_LE_S);
	def I32X4_LE_U =		do_ss_s_x4(_, _, V128_I32X4_LE_U);
	def I32X4_GT_S =		commute_binop(I32X4_LT_S);
	def I32X4_GT_U =		commute_binop(I32X4_LT_U);
	def I32X4_GE_S =		commute_binop(I32X4_LE_S);
	def I32X4_GE_U =		commute_binop(I32X4_LE_U);
	def I32X4_SHL =			do_si_s_x4(_, _, V128_I32X4_SHL);
	def I32X4_SHR_U =		do_si_s_x4(_, _, V128_I32X4_SHR_U);
	def I32X4_SHR_S =		do_si_s_x4(_, _, V128_I32X4_SHR_S);
	def I16X8_ADD =			do_ss_s_x8(_, _, u16.+);
	def I16X8_SUB =			do_ss_s_x8(_, _, u16.-);
	def I16X8_MUL =			do_ss_s_x8(_, _, u16.*);
	def I16X8_Q15MULRSAT_S =	do_ss_s_x8(_, _, V128_I16_Q15_MUL_SAT_S);
	def I16X8_NEG =			do_ss_s_x8((0, 0), _, u16.-);
	def I16X8_EXTADDPAIRWISE_I8X16_S =	do_s_s_x16_pairwise_ext_x8(_, V128_I16X8_EXTADD_I8X16_S);
	def I16X8_EXTADDPAIRWISE_I8X16_U =	do_s_s_x16_pairwise_ext_x8(_, I16X8_EXTADD_I8X16_U);
	def I16X8_ADD_SAT_S =		do_ss_s_x8(_, _, V128_I16_ADD_SAT_S);
	def I16X8_ADD_SAT_U =		do_ss_s_x8(_, _, I16_ADD_SAT_U);
	def I16X8_SUB_SAT_S =		do_ss_s_x8(_, _, V128_I16_SUB_SAT_S);
	def I16X8_SUB_SAT_U =		do_ss_s_x8(_, _, I16_SUB_SAT_U);
	def I16X8_MIN_S =		do_ss_s_x8(_, _, V128_I16_MIN_S);
	def I16X8_MIN_U =		do_ss_s_x8(_, _, I16_MIN_U);
	def I16X8_MAX_S =		do_ss_s_x8(_, _, V128_I16_MAX_S);
	def I16X8_MAX_U =		do_ss_s_x8(_, _, I16_MAX_U);
	def I16X8_AVGR_U =		do_ss_s_x8(_, _, I16_AVGR_U);
	def I16X8_ABS =			do_s_s_x8(_, V128_I16_ABS);
	def I16X8_EQ =			do_ss_s_x8(_, _, V128_I16X8_EQ);
	def I16X8_NE =			do_ss_s_x8(_, _, V128_I16X8_NE);
	def I16X8_LT_S =		do_ss_s_x8(_, _, V128_I16X8_LT_S);
	def I16X8_LT_U =		do_ss_s_x8(_, _, V128_I16X8_LT_U);
	def I16X8_LE_S =		do_ss_s_x8(_, _, V128_I16X8_LE_S);
	def I16X8_LE_U =		do_ss_s_x8(_, _, V128_I16X8_LE_U);
	def I16X8_GT_S =		commute_binop(I16X8_LT_S);
	def I16X8_GT_U =		commute_binop(I16X8_LT_U);
	def I16X8_GE_S =		commute_binop(I16X8_LE_S);
	def I16X8_GE_U =		commute_binop(I16X8_LE_U);
	def I16X8_SHL =			do_si_s_x8(_, _, V128_I16X8_SHL);
	def I16X8_SHR_U =		do_si_s_x8(_, _, I16_SHR_U);
	def I16X8_SHR_S =		do_si_s_x8(_, _, V128_I16X8_SHR_S);
	def I8X16_ADD =			do_ss_s_x16(_, _, u8.+);
	def I8X16_SUB =			do_ss_s_x16(_, _, u8.-);
	def I8X16_NEG =			do_ss_s_x16((0, 0), _, u8.-);
	def I8X16_ADD_SAT_S =		do_ss_s_x16(_, _, V128_I8_ADD_SAT_S);
	def I8X16_ADD_SAT_U =		do_ss_s_x16(_, _, I8_ADD_SAT_U);
	def I8X16_SUB_SAT_S =		do_ss_s_x16(_, _, V128_I8_SUB_SAT_S);
	def I8X16_SUB_SAT_U =		do_ss_s_x16(_, _, I8_SUB_SAT_U);
	def I8X16_MIN_S =		do_ss_s_x16(_, _, V128_I8_MIN_S);
	def I8X16_MIN_U =		do_ss_s_x16(_, _, I8_MIN_U);
	def I8X16_MAX_S =		do_ss_s_x16(_, _, V128_I8_MAX_S);
	def I8X16_MAX_U =		do_ss_s_x16(_, _, I8_MAX_U);
	def I8X16_AVGR_U =		do_ss_s_x16(_, _, I8_AVGR_U);
	def I8X16_ABS =			do_s_s_x16(_, V128_I8_ABS);
	def I8X16_POPCNT =		do_s_s_x16(_, I8_POPCNT);
	def I8X16_EQ =			do_ss_s_x16(_, _, V128_I8X16_EQ);
	def I8X16_NE =			do_ss_s_x16(_, _, V128_I8X16_NE);
	def I8X16_LT_S =		do_ss_s_x16(_, _, V128_I8X16_LT_S);
	def I8X16_LT_U =		do_ss_s_x16(_, _, V128_I8X16_LT_U);
	def I8X16_LE_S =		do_ss_s_x16(_, _, V128_I8X16_LE_S);
	def I8X16_LE_U =		do_ss_s_x16(_, _, V128_I8X16_LE_U);
	def I8X16_GT_S =		commute_binop(I8X16_LT_S);
	def I8X16_GT_U =		commute_binop(I8X16_LT_U);
	def I8X16_GE_S =		commute_binop(I8X16_LE_S);
	def I8X16_GE_U =		commute_binop(I8X16_LE_U);
	def I8X16_SHL =			do_si_s_x16(_, _, V128_I8X16_SHL);
	def I8X16_SHR_U =		do_si_s_x16(_, _, I8_SHR_U);
	def I8X16_SHR_S =		do_si_s_x16(_, _, V128_I8X16_SHR_S);
	def F32X4_ADD =			do_ss_s_x4(_, _, F32_ADD_U);
	def F32X4_SUB =			do_ss_s_x4(_, _, F32_SUB_U);
	def F32X4_MUL =			do_ss_s_x4(_, _, F32_MUL_U);
	def F32X4_DIV =			do_ss_s_x4(_, _, F32_DIV_U);
	def F32X4_NEG =			do_s_s_x4(_, F32_NEG_U);
	def F32X4_SQRT =		do_s_s_x4(_, F32_SQRT_U);
	def F32X4_CEIL =		do_s_s_x4(_, V128_F32_CEIL);
	def F32X4_FLOOR =		do_s_s_x4(_, V128_F32_FLOOR);
	def F32X4_TRUNC =		do_s_s_x4(_, V128_F32_TRUNC);
	def F32X4_NEAREST =		do_s_s_x4(_, V128_F32_NEAREST);
	def F32X4_EQ =			do_ss_s_x4(_, _, V128_F32X4_EQ);
	def F32X4_NE =			do_ss_s_x4(_, _, V128_F32X4_NE);
	def F32X4_LT =			do_ss_s_x4(_, _, V128_F32X4_LT);
	def F32X4_LE =			do_ss_s_x4(_, _, V128_F32X4_LE);
	def F32X4_GT =			commute_binop(F32X4_LT);
	def F32X4_GE =			commute_binop(F32X4_LE);
	def F32X4_MIN =			do_ss_s_x4(_, _, V128_F32X4_MIN);
	def F32X4_MAX =			do_ss_s_x4(_, _, V128_F32X4_MAX);
	def F32X4_ABS =			do_s_s_x4(_, V128_F32X4_ABS);
	def F32X4_PMIN =		do_ss_s_x4(_, _, V128_F32X4_PMIN);
	def F32X4_PMAX =		do_ss_s_x4(_, _, V128_F32X4_PMAX);
	def F64X2_ADD =			do_ss_s_x2(_, _, F64_ADD_U);
	def F64X2_SUB =			do_ss_s_x2(_, _, F64_SUB_U);
	def F64X2_MUL =			do_ss_s_x2(_, _, F64_MUL_U);
	def F64X2_DIV =			do_ss_s_x2(_, _, F64_DIV_U);
	def F64X2_NEG =			do_s_s_x2(_, F64_NEG_U);
	def F64X2_SQRT =		do_s_s_x2(_, F64_SQRT_U);
	def F64X2_CEIL =		do_s_s_x2(_, V128_F64_CEIL);
	def F64X2_FLOOR =		do_s_s_x2(_, V128_F64_FLOOR);
	def F64X2_TRUNC =		do_s_s_x2(_, V128_F64_TRUNC);
	def F64X2_NEAREST =		do_s_s_x2(_, V128_F64_NEAREST);
	def F64X2_EQ =			do_ss_s_x2(_, _, V128_F64X2_EQ);
	def F64X2_NE =			do_ss_s_x2(_, _, V128_F64X2_NE);
	def F64X2_LT =			do_ss_s_x2(_, _, V128_F64X2_LT);
	def F64X2_LE =			do_ss_s_x2(_, _, V128_F64X2_LE);
	def F64X2_GT =			commute_binop(F64X2_LT);	
	def F64X2_GE =			commute_binop(F64X2_LE);
	def F64X2_MIN =			do_ss_s_x2(_, _, V128_F64X2_MIN);
	def F64X2_MAX =			do_ss_s_x2(_, _, V128_F64X2_MAX);
	def F64X2_ABS =			do_s_s_x2(_, V128_F64X2_ABS);
	def F64X2_PMIN =		do_ss_s_x2(_, _, V128_F64X2_PMIN);
	def F64X2_PMAX =		do_ss_s_x2(_, _, V128_F64X2_PMAX);
	def I16X8_EXTEND_HIGH_I8X16_S = do_v128_half_extend<i8, i16, u16>(_, false, 16, i8.view, i16.view, u16.view);
	def I16X8_EXTEND_LOW_I8X16_S  = do_v128_half_extend<i8, i16, u16>(_, true, 16, i8.view, i16.view, u16.view);
	def I16X8_EXTEND_HIGH_I8X16_U = do_v128_half_extend<u8, u16, u16>(_, false, 16, u8.view, u16.view, u16.view);
	def I16X8_EXTEND_LOW_I8X16_U  = do_v128_half_extend<u8, u16, u16>(_, true, 16, u8.view, u16.view, u16.view);
	def I32X4_EXTEND_HIGH_I16X8_S = do_v128_half_extend<i16, i32, u32>(_, false, 32, i16.view, i32.view, u32.view);
	def I32X4_EXTEND_LOW_I16X8_S  = do_v128_half_extend<i16, i32, u32>(_, true, 32, i16.view, i32.view, u32.view);
	def I32X4_EXTEND_HIGH_I16X8_U = do_v128_half_extend<u16, u32, u32>(_, false, 32, u16.view, u32.view, u32.view);
	def I32X4_EXTEND_LOW_I16X8_U  = do_v128_half_extend<u16, u32, u32>(_, true, 32, u16.view, u32.view, u32.view);
	def I64X2_EXTEND_HIGH_I32X4_S = do_v128_half_extend<i32, i64, u64>(_, false, 64, i32.view, i64.view, u64.view);
	def I64X2_EXTEND_LOW_I32X4_S  = do_v128_half_extend<i32, i64, u64>(_, true, 64, i32.view, i64.view, u64.view);
	def I64X2_EXTEND_HIGH_I32X4_U = do_v128_half_extend<u32, u64, u64>(_, false, 64, u32.view, u64.view, u64.view);
	def I64X2_EXTEND_LOW_I32X4_U  = do_v128_half_extend<u32, u64, u64>(_, true, 64, u32.view, u64.view, u64.view);
	def I16X8_EXTMUL_LOW_I8X16_S =	do_v128_extmul(_, _, true, 16, i8.view<u64>, i16.view<i8>, u16.view<i16>, i16.*);
	def I16X8_EXTMUL_HIGH_I8X16_S = do_v128_extmul(_, _, false, 16, i8.view<u64>, i16.view<i8>, u16.view<i16>, i16.*);
	def I16X8_EXTMUL_LOW_I8X16_U =	do_v128_extmul(_, _, true, 16, u8.view<u64>, u16.view<u8>, u16.view<u16>, u16.*);
	def I16X8_EXTMUL_HIGH_I8X16_U = do_v128_extmul(_, _, false, 16, u8.view<u64>, u16.view<u8>, u16.view<u16>, u16.*);
	def I32X4_EXTMUL_LOW_I16X8_S =	do_v128_extmul(_, _, true, 32, i16.view<u64>, i32.view<i16>, u32.view<i32>, i32.*);
	def I32X4_EXTMUL_HIGH_I16X8_S = do_v128_extmul(_, _, false, 32, i16.view<u64>, i32.view<i16>, u32.view<i32>, i32.*);
	def I32X4_EXTMUL_LOW_I16X8_U =	do_v128_extmul(_, _, true, 32, u16.view<u64>, u32.view<u16>, u32.view<u32>, u32.*);
	def I32X4_EXTMUL_HIGH_I16X8_U = do_v128_extmul(_, _, false, 32, u16.view<u64>, u32.view<u16>, u32.view<u32>, u32.*);
	def I64X2_EXTMUL_LOW_I32X4_S =	do_v128_extmul(_, _, true, 64, i32.view<u64>, i64.view<i32>, u64.view<i64>, i64.*);
	def I64X2_EXTMUL_HIGH_I32X4_S = do_v128_extmul(_, _, false, 64, i32.view<u64>, i64.view<i32>, u64.view<i64>, i64.*);
	def I64X2_EXTMUL_LOW_I32X4_U =	do_v128_extmul(_, _, true, 64, u32.view<u64>, u64.view<u32>, u64.view<u64>, u64.*);
	def I64X2_EXTMUL_HIGH_I32X4_U = do_v128_extmul(_, _, false, 64, u32.view<u64>, u64.view<u32>, u64.view<u64>, u64.*);

	def F32X4_CONVERT_I32X4_S =		do_s_s_x4(_, V128_F32_CONVERT_I32_S);
	def F32X4_CONVERT_I32X4_U =		do_s_s_x4(_, V128_F32_CONVERT_I32_U);
	def F64X2_CONVERT_LOW_I32X4_S =		do_s_s_x2_low(_, V128_F64_CONVERT_LOW_I32_S);
	def F64X2_CONVERT_LOW_I32X4_U =		do_s_s_x2_low(_, V128_F64_CONVERT_LOW_I32_U);
	def I64X2_SHL =				do_si_s_x2(_, _, V3Eval.I64_SHL);
	def I64X2_SHR_U =			do_si_s_x2(_, _, I64_SHR_U);
	def I64X2_SHR_S =			do_si_s_x2(_, _, V128_I64X2_SHR_S);
	def I8X16_NARROW_I16X8_S =		do_v128_narrow(_, _, 16, i16.view<u64>, I8_CONVERT_I16_SAT_S, u8.view<i8>);
	def I8X16_NARROW_I16X8_U =		do_v128_narrow(_, _, 16, i16.view<u64>, I8_CONVERT_I16_SAT_U, u8.view<u8>);
	def I16X8_NARROW_I32X4_S =		do_v128_narrow(_, _, 32, i32.view<u64>, I16_CONVERT_I32_SAT_S, u16.view<i16>);
	def I16X8_NARROW_I32X4_U =		do_v128_narrow(_, _, 32, i32.view<u64>, I16_CONVERT_I32_SAT_U, u16.view<u16>);
	def F64X2_PROMOTE_LOW_F32X4 =		do_s_s_x2_low(_, V128_F64_PROMOTE_LOW_F32);
	def F32X4_DEMOTE_F64X2_ZERO =		do_s_s_x4_high_zero(_, V128_F32_DEMOTE_F64);
	def I32X4_TRUNC_SAT_F32X4_S =		do_s_s_x4(_, V128_I32_TRUNC_F32_SAT_S);
	def I32X4_TRUNC_SAT_F32X4_U =		do_s_s_x4(_, V128_I32_TRUNC_F32_SAT_U);
	def I32X4_TRUNC_SAT_F64X2_S_ZERO =	do_s_s_x4_high_zero(_, V128_I32_TRUNC_F64_SAT_S);
	def I32X4_TRUNC_SAT_F64X2_U_ZERO =	do_s_s_x4_high_zero(_, V128_I32_TRUNC_F64_SAT_U);
	// ---- rounding and conversion ----------------------------------------
	def I32_WRAP_I64	= u32.view<u64>;
	def I32_TRUNC_F32_S	= truncF32(-2.1474839E9f, 2147483648f, i32.truncf, _);
	def I32_TRUNC_F32_U	= truncF32(-1f, 4294967296f, u32.truncf, _);
	def I32_TRUNC_F64_S	= truncF64(-2147483649d, 2147483648f, i32.truncd, _);
	def I32_TRUNC_F64_U	= truncF64(-1d, 4294967296d, u32.truncd, _);
	def I64_EXTEND_I32_S	= i64.view<i32>;
	def I64_EXTEND_I32_U	= u64.view<u32>;
	def I64_TRUNC_F32_S	= truncF32(-9.223373e18f, 9223372036854775808f, i64.truncf, _);
	def I64_TRUNC_F32_U	= truncF32(-1f, 18446744073709551616f, u64.truncf, _);
	def I64_TRUNC_F64_S	= truncF64(-9.223372036854778E18d, 9223372036854775808d, i64.truncd, _);
	def I64_TRUNC_F64_U	= truncF64(-1d, 18446744073709551616d, u64.truncd, _);
	def F32_CONVERT_I32_S	= float.roundi<i32>;
	def F32_CONVERT_I32_U	= float.roundi<u32>;
	def F32_CONVERT_I64_S	= float.roundi<i64>;
	def F32_CONVERT_I64_U	= float.roundi<u64>;
	def F32_DEMOTE_F64	= float.roundd;
	def F64_CONVERT_I32_S	= double.roundi<i32>;
	def F64_CONVERT_I32_U	= double.roundi<u32>;
	def F64_CONVERT_I64_S	= double.roundi<i64>;
	def F64_CONVERT_I64_U	= double.roundi<u64>;
	def F64_PROMOTE_F32	= double.!<float>;
	def I32_REINTERPRET_F32	= u32.view<float>;
	def I64_REINTERPRET_F64	= u64.view<double>;
	def F32_REINTERPRET_I32	= float.view<u32>;
	def F64_REINTERPRET_I64	= double.view<u64>;

	// ---- sign-extension and zero-extension helpers ----------------------
	def I32_EXTEND8_S(a: i32) -> i32 {
		return i8.view(a);
	}
	def I32_EXTEND16_S(a: i32) -> i32 {
		return i16.view(a);
	}
	def I64_EXTEND8_S(a: i64) -> i64 {
		return i8.view(a);
	}
	def I64_EXTEND16_S(a: i64) -> i64 {
		return i16.view(a);
	}
	def I64_EXTEND32_S(a: i64) -> i64 {
		return i32.view(a);
	}
	def signExtend(st: StorageType, val: Value) -> Value {
		match (st.pack) {
			PACKED_I8 => return Value.I32(u32.view(i8.view(Values.unbox_i(val))));
			PACKED_I16 => return Value.I32(u32.view(i16.view(Values.unbox_i(val))));
			_ => return val;
		}
	}
	def zeroExtend(st: StorageType, val: Value) -> Value {
		match (st.pack) {
			PACKED_I8 => return Value.I32(u8.view(Values.unbox_i(val)));
			PACKED_I16 => return Value.I32(u16.view(Values.unbox_i(val)));
			_ => return val;
		}
	}
}

// Private (i.e. file-scoped) utilities.
// ---- v128 arithmetic helpers ----------------------------------------
def I32_TRUNC_F32_SAT_S = truncF32_SAT(-2.1474839E9f, 2147483648f, i32.max, i32.min, i32.truncf, _);
def I32_TRUNC_F32_SAT_U = truncF32_SAT(-1f, 4294967296f, u32.max, u32.min, u32.truncf, _);
def I32_TRUNC_F64_SAT_S = truncF64_SAT(-2147483649d, 2147483648f, i32.max, i32.min, i32.truncd, _);
def I32_TRUNC_F64_SAT_U = truncF64_SAT(-1d, 4294967296d, u32.max, u32.min, u32.truncd, _);

def F32_ADD_U = do_ff_f(_, _, float.+);
def F32_SUB_U = do_ff_f(_, _, float.-);
def F32_MUL_U = do_ff_f(_, _, float.*);
def F32_DIV_U = do_ff_f(_, _, float./);
def F32_NEG_U = do_f_f(_, V3Eval.F32_NEG);
def F32_SQRT_U = do_f_f(_, float.sqrt);
def V128_F32_CEIL = do_f_f(_, V3Eval.F32_CEIL);
def V128_F32_FLOOR = do_f_f(_, V3Eval.F32_FLOOR);
def V128_F32_TRUNC = do_f_f(_, V3Eval.F32_TRUNC);
def V128_F32_NEAREST = do_f_f(_, float.round);
def F64_ADD_U = do_dd_d(_, _, double.+);
def F64_SUB_U = do_dd_d(_, _, double.-);
def F64_MUL_U = do_dd_d(_, _, double.*);
def F64_DIV_U = do_dd_d(_, _, double./);
def F64_NEG_U = do_d_d(_, V3Eval.F64_NEG);
def F64_SQRT_U = do_d_d(_, double.sqrt);
def V128_F64_CEIL = do_d_d(_, V3Eval.F64_CEIL);
def V128_F64_FLOOR = do_d_d(_, V3Eval.F64_FLOOR);
def V128_F64_TRUNC = do_d_d(_, V3Eval.F64_TRUNC);
def V128_F64_NEAREST = do_d_d(_, double.round);

def V128_I8_MIN_S = do_ii_i_8(_, _, I8_MIN_S);
def V128_I16_MIN_S = do_ii_i_16(_, _, I16_MIN_S);
def V128_I32_MIN_S = do_ii_i(_, _, I32_MIN_S);
def V128_I8_MAX_S = do_ii_i_8(_, _, I8_MAX_S);
def V128_I16_MAX_S = do_ii_i_16(_, _, I16_MAX_S);
def V128_I32_MAX_S = do_ii_i(_, _, I32_MAX_S);
def V128_I8_ABS = do_i_i_8(_, I8_ABS_S);
def V128_I16_ABS = do_i_i_16(_, I16_ABS_S);
def V128_I32_ABS = do_i_i(_, I32_ABS_S);
def V128_I64_ABS = do_l_l(_, I64_ABS_S);

def V128_I8_ADD_SAT_S = do_ii_i_8(_, _, I8_ADD_SAT_S);
def V128_I8_SUB_SAT_S = do_ii_i_8(_, _, I8_SUB_SAT_S);
def V128_I16_ADD_SAT_S = do_ii_i_16(_, _, I16_ADD_SAT_S);
def V128_I16_SUB_SAT_S = do_ii_i_16(_, _, I16_SUB_SAT_S);
def V128_I8X16_EQ = do_uu_z_8(_, _, u8.==);
def V128_I8X16_NE = do_uu_z_8(_, _, u8.!=);
def V128_I8X16_LT_S = do_ii_z_8(_, _, i8.<);
def V128_I8X16_LT_U = do_uu_z_8(_, _, u8.<);
def V128_I8X16_LE_S = do_ii_z_8(_, _, i8.<=);
def V128_I8X16_LE_U = do_uu_z_8(_, _, u8.<=);
def V128_I8X16_SHL = do_ii_i_8(_, _, I8_SHL);
def V128_I8X16_SHR_S = do_ii_i_8(_, _, I8_SHR_S);
def V128_I16_Q15_MUL_SAT_S = do_ii_i_16(_, _, I16_Q15_MUL_SAT_S);
def V128_I16X8_EQ = do_uu_z_16(_, _, u16.==);
def V128_I16X8_NE = do_uu_z_16(_, _, u16.!=);
def V128_I16X8_LT_S = do_ii_z_16(_, _, i16.<);
def V128_I16X8_LT_U = do_uu_z_16(_, _, u16.<);
def V128_I16X8_LE_S = do_ii_z_16(_, _, i16.<=);
def V128_I16X8_LE_U = do_uu_z_16(_, _, u16.<=);
def V128_I16X8_SHL = do_ii_i_16(_, _, I16_SHL);
def V128_I16X8_SHR_S = do_ii_i_16(_, _, I16_SHR_S);
def V128_I32X4_EQ = do_uu_z(_, _, u32.==);
def V128_I32X4_NE = do_uu_z(_, _, u32.!=);
def V128_I32X4_LT_S = do_ii_z(_, _, i32.<);
def V128_I32X4_LT_U = do_uu_z(_, _, u32.<);
def V128_I32X4_LE_S = do_ii_z(_, _, i32.<=);
def V128_I32X4_LE_U = do_uu_z(_, _, u32.<=);
def V128_I32X4_SHL = do_ii_i(_, _, V3Eval.I32_SHL);
def V128_I32X4_SHR_U = do_ii_i(_, _, V3Eval.I32_SHR_U);
def V128_I32X4_SHR_S = do_ii_i(_, _, V3Eval.I32_SHR_S);
def V128_I64X2_EQ = do_ww_z(_, _, u64.==);
def V128_I64X2_NE = do_ww_z(_, _, u64.!=);
def V128_I64X2_LT_S = do_ll_z(_, _, i64.<);
def V128_I64X2_LE_S = do_ll_z(_, _, i64.<=);

def V128_F32X4_EQ = do_ff_z(_, _, float.==);
def V128_F32X4_NE = do_ff_z(_, _, float.!=);
def V128_F32X4_LT = do_ff_z(_, _, float.<);
def V128_F32X4_LE = do_ff_z(_, _, float.<=);
def V128_F32X4_MIN = do_ff_f(_, _, V3Eval.F32_MIN);
def V128_F32X4_MAX = do_ff_f(_, _, V3Eval.F32_MAX);
def V128_F32X4_ABS = do_f_f(_, float.abs);
def V128_F32X4_PMIN = do_ff_f(_, _, F32X4_PMIN);
def V128_F32X4_PMAX = do_ff_f(_, _, F32X4_PMAX);
def V128_F64X2_EQ = do_dd_z(_, _, double.==);
def V128_F64X2_NE = do_dd_z(_, _, double.!=);
def V128_F64X2_LT = do_dd_z(_, _, double.<);
def V128_F64X2_LE = do_dd_z(_, _, double.<=);
def V128_F64X2_MIN = do_dd_d(_, _, V3Eval.F64_MIN);
def V128_F64X2_MAX = do_dd_d(_, _, V3Eval.F64_MAX);
def V128_F64X2_ABS = do_d_d(_, double.abs);
def V128_F64X2_PMIN = do_dd_d(_, _, F64X2_PMIN);
def V128_F64X2_PMAX = do_dd_d(_, _, F64X2_PMAX);
def V128_I64X2_SHR_S = do_ll_l(_, _, V3Eval.I64_SHR_S);

def V128_I16X8_EXTADD_I8X16_S = do_ii_i_8_16(_, _, I16X8_EXTADD_I8X16_S);
def V128_I32X4_EXTADD_16X8_S = do_ii_i_16_32(_, _, I32X4_EXTADD_I16X8_S);

def V128_F32_CONVERT_I32_S = do_i_f(_, float.roundi<i32>);
def V128_F32_CONVERT_I32_U = do_u_f(_, float.roundi<u32>);
def V128_F64_CONVERT_LOW_I32_S = do_i_d(_, double.roundi<i32>);
def V128_F64_CONVERT_LOW_I32_U = do_u_d(_, double.roundi<u32>);
def V128_F64_PROMOTE_LOW_F32 = do_f_d(_, double.!<float>);
def V128_F32_DEMOTE_F64 = do_d_f(_, float.roundd);
def V128_I32_TRUNC_F32_SAT_S = do_f_i(_, I32_TRUNC_F32_SAT_S);
def V128_I32_TRUNC_F32_SAT_U = do_f_u(_, I32_TRUNC_F32_SAT_U);
def V128_I32_TRUNC_F64_SAT_S = do_d_i(_, I32_TRUNC_F64_SAT_S);
def V128_I32_TRUNC_F64_SAT_U = do_d_u(_, I32_TRUNC_F64_SAT_U);

def I8_MIN_S(a: i8, b: i8) -> i8 {
	return if (a <= b, a, b);
}
def I16_MIN_S(a: i16, b: i16) -> i16 {
	return 	if (a <= b, a, b);
}
def I32_MIN_S(a: i32, b: i32) -> i32 {
	return 	if (a <= b, a, b);
}
def I8_MAX_S(a: i8, b: i8) -> i8 {
	return 	if (a >= b, a, b);
}
def I16_MAX_S(a: i16, b: i16) -> i16 {
	return 	if (a >= b, a, b);
}
def I32_MAX_S(a: i32, b: i32) -> i32 {
	return 	if (a >= b, a, b);
}
def I8_MIN_U(a: u8, b: u8) -> u8 {
	return if (a <= b, a, b);
}
def I16_MIN_U(a: u16, b: u16) -> u16 {
	return if (a <= b, a, b);
}
def I32_MIN_U(a: u32, b: u32) -> u32 {
	return if (a <= b, a, b);
}
def I8_MAX_U(a: u8, b: u8) -> u8 {
	return if (a >= b, a, b);
}
def I16_MAX_U(a: u16, b: u16) -> u16 {
	return if (a >= b, a, b);
}
def I32_MAX_U(a: u32, b: u32) -> u32 {
	return if (a >= b, a, b);
}
def I8_AVGR_U(a: u8, b: u8) -> u8 {
	// rounding average
	var sum = u16.view(a) + u16.view(b);
	return u8.view((sum + 1) >> 1);
}
def I16_AVGR_U(a: u16, b: u16) -> u16 {
	// rounding average
	var sum = u32.view(a) + u32.view(b);
	return u16.view((sum + 1) >> 1);
}
def I8_ABS_S(a: i8) -> i8 {
	return if (a < 0, -a, a);
}
def I16_ABS_S(a: i16) -> i16 {
	return if (a < 0, -a, a);
}
def I32_ABS_S(a: i32) -> i32 {
	return if (a < 0, -a, a);
}
def I64_ABS_S(a: i64) -> i64 {
	return if (a < 0, -a, a);
}
def I8_POPCNT(a: u8) -> u8 {
	var count:byte = 0;
	for (i < 8) {
		if ((a & 1) == 1) count++;
		a >>= 1;
	}
	return count;
}
def I8_ADD_SAT_S(a: i8, b: i8) -> i8 {
	var sum = i16.view(a) + i16.view(b);
	if (sum > 127) return 127;
	else if (sum < -128) return -128;
	else return i8.view(sum);
}
def I8_ADD_SAT_U(a: u8, b: u8) -> u8 {
	var sum = u16.view(a) + u16.view(b);
	return if (sum > 255, 255, u8.view(sum));
}
def I8_SUB_SAT_S(a: i8, b: i8) -> i8 {
	var dif = i16.view(a) - i16.view(b);
	if (dif < i8.min) return i8.min;
	else if (dif > i8.max) return i8.max;
	else return i8.view(dif);
}
def I8_SUB_SAT_U(a: u8, b: u8) -> u8 {
	var dif = i16.view(a) - i16.view(b);
	return if (dif < 0, u8.view(0), u8.view(dif));
}
def I16_ADD_SAT_S(a: i16, b: i16) -> i16 {
	var sum = i32.view(a) + i32.view(b);
	if (sum > i16.max) return i16.max;
	else if (sum < i16.min) return i16.min;
	else return i16.view(sum);
}
def I16_ADD_SAT_U(a: u16, b: u16) -> u16 {
	var sum = u32.view(a) + u32.view(b);
	return if (sum > u16.max, u16.max, u16.view(sum));
}
def I16_SUB_SAT_S(a: i16, b: i16) -> i16 {
	var dif = i32.view(a) - i32.view(b);
	if (dif < i16.min) return i16.min;
	else if (dif > i16.max) return i16.max;
	else return i16.view(dif);
}
def I16_SUB_SAT_U(a: u16, b: u16) -> u16 {
	var dif = i32.view(a) - i32.view(b);
	return if (dif < 0, u16.view(0), u16.view(dif));
}
def I8_CONVERT_I16_SAT_S(a: i16) -> i8 {
	return if (a > i8.max, i8.max, if (a < i8.min, i8.min, i8.view(a)));
}
def I8_CONVERT_I16_SAT_U(a: i16) -> u8 {
	return if (a > u8.max, u8.max, if (a < u8.min, u8.min, u8.view(a)));
}
def I16_CONVERT_I32_SAT_S(a: i32) -> i16 {
	return if (a > i16.max, i16.max, if (a < i16.min, i16.min, i16.view(a)));
}
def I16_CONVERT_I32_SAT_U(a: i32) -> u16 {
	return if (a > u16.max, u16.max, if (a < u16.min, u16.min, u16.view(a)));
}
def I16_Q15_MUL_SAT_S(a: i16, b: i16) -> i16 {
	var prod = (i32.view(a) * i32.view(b) + 0x4000) >> 15;
	return if(prod > i16.max, i16.max, i16.view(prod));	
}
def I16X8_EXTADD_I8X16_S(a: i8, b: i8) -> i16 {
	return i16.+(i16.view(a), i16.view(b));
}
def I16X8_EXTADD_I8X16_U(a: u8, b: u8) -> u16 {
	return u16.+(u16.view(a), u16.view(b));
}
def I32X4_EXTADD_I16X8_S(a: i16, b: i16) -> i32 {
	return i32.+(i32.view(a), i32.view(b));
}
def I32X4_EXTADD_I16X8_U(a: u16, b: u16) -> u32 {
	return u32.+(u32.view(a), u32.view(b));
}
def F32X4_PMIN(a: float, b: float) -> float {
	return if (a > b,  b, a);
}
def F32X4_PMAX(a: float, b: float) -> float {
	return if (a < b, b, a);
}
def F64X2_PMIN(a: double, b: double) -> double {
	return if (a > b, b, a);
}
def F64X2_PMAX(a: double, b: double) -> double {
	return if (a < b, b, a);
}
def I8_SHL(a: i8, b: i8) -> i8 {
	return a << u3.view(b);
}
def I16_SHL(a: i16, b: i16) -> i16 {
	return a << u4.view(b);
}
def I8_SHR_U(a: u8, b: u8) -> u8 {
	return a >> u3.view(b);
}
def I16_SHR_U(a: u16, b: u16) -> u16 {
	return a >> u4.view(b);
}
def I8_SHR_S(a: i8, b: i8) -> i8 {
	return a >> u3.view(b);
}
def I16_SHR_S(a: i16, b: i16) -> i16 {
	return a >> u4.view(b);
}
// Adapters
def do_uu_z_8(a: u8, b: u8, f: (u8, u8) -> bool) -> u8 {  // Adapts a unsigned bool binop to a u8 binop
	return if (f(a, b), u8.max, u8.view(0));
}
def do_uu_z_16(a: u16, b: u16, f: (u16, u16) -> bool) -> u16 {  // Adapts a unsigned bool binop to a u16 binop
	return if (f(a, b), u16.max, u16.view(0));
}
def do_uu_z(a: u32, b: u32, f: (u32, u32) -> bool) -> u32 {  // Adapts a unsigned bool binop to a u32 binop
	return if (f(a, b), u32.max, u32.view(0));
}
def do_ww_z(a: u64, b: u64, f: (u64, u64) -> bool) -> u64 {  // Adapts a unsigned bool binop to a u64 binop
	return if (f(a, b), u64.max, u64.view(0));
}
def do_ii_z_8(a: u8, b: u8, f: (i8, i8) -> bool) -> u8 {  // Adapts a signed bool binop to a u8 binop
	return if (f(i8.view(a), i8.view(b)), u8.max, u8.view(0));
}
def do_ii_z_16(a: u16, b: u16, f: (i16, i16) -> bool) -> u16 {  // Adapts a signed bool binop to a u16 binop
	return if (f(i16.view(a), i16.view(b)), u16.max, u16.view(0));
}
def do_ii_z(a: u32, b: u32, f: (i32, i32) -> bool) -> u32 {  // Adapts a signed bool binop to a u32 binop
	return if (f(i32.view(a), i32.view(b)), u32.max, u32.view(0));
}
def do_ll_z(a: u64, b: u64, f: (i64, i64) -> bool) -> u64 {  // Adapts a signed bool binop to a u64 binop
	return if (f(i64.view(a), i64.view(b)), u64.max, u64.view(0));
}
def do_ff_z(a: u32, b: u32, f: (float, float) -> bool) -> u32 {  // Adapts a floating point bool binop to a u32 binop
	return if (f(float.view(a), float.view(b)), u32.max, u32.view(0));
}
def do_dd_z(a: u64, b: u64, f: (double, double) -> bool) -> u64 {  // Adapts a floating point bool binop to a u64 binop
	return if (f(double.view(a), double.view(b)), u64.max, u64.view(0));
}
def do_ff_f(a: u32, b: u32, f: (float, float) -> float) -> u32 {  // Adapts a floating point binop to a u32 binop
	return u32.view(f(float.view(a), float.view(b)));
}
def do_f_f(a: u32, f: float -> float) -> u32 {  // Adapts a floating point unop to a u32 unop
	return u32.view(f(float.view(a)));
}
def do_dd_d(a: u64, b: u64, f: (double, double) -> double) -> u64 {  // Adapts a floating point binop to a u64 binop
	return u64.view(f(double.view(a), double.view(b)));
}
def do_d_d(a: u64, f: double -> double) -> u64 {  // Adapts a floating point unop to a u64 unop
	return u64.view(f(double.view(a)));
}
def do_ii_i_8(a: u8, b: u8, f: (i8, i8) -> i8) -> u8 {  // Adapts a signed i8 binop to a u8 binop
	return u8.view(f(i8.view(a), i8.view(b)));
}
def do_ii_i_8_16(a: u8, b: u8, f: (i8, i8) -> i16) -> u16 {
	return u16.view(f(i8.view(a), i8.view(b)));
}
def do_ii_i_16_32(a: u16, b: u16, f: (i16, i16) -> i32) -> u32 {
	return u32.view(f(i16.view(a), i16.view(b)));
}
def do_i_i_8(a: u8, f: i8 -> i8) -> u8 {  // Adapts a signed i8 unop to a u8 unop
	return u8.view(f(i8.view(a)));
}
def do_ii_i_16(a: u16, b: u16, f: (i16, i16) -> i16) -> u16 {  // Adapts a signed i16 binop to a u16 binop
	return u16.view(f(i16.view(a), i16.view(b)));
}
def do_i_i_16(a: u16, f: i16 -> i16) -> u16 {  // Adapts a signed i16 unop to a u16 unop
	return u16.view(f(i16.view(a)));
}
def do_ii_i(a: u32, b: u32, f: (i32, i32) -> i32) -> u32 {  // Adapts a signed i32 binop to a u32 binop
	return u32.view(f(i32.view(a), i32.view(b)));
}
def do_i_i(a: u32, f: i32 -> i32) -> u32 {  // Adapts a signed i32 unop to a u32 unop
	return u32.view(f(i32.view(a)));
}
def do_ll_l(a: u64, b: u64, f: (i64, i64) -> i64) -> u64 {  // Adapts a signed i64 binop to a u64 binop
	return u64.view(f(i64.view(a), i64.view(b)));
}
def do_l_l(a: u64, f: i64 -> i64) -> u64 {  // Adapts a signed i64 unop to a u64 unop
	return u64.view(f(i64.view(a)));
}
def do_i_f(a: u32, f: i32 -> float) -> u32 {  // Adapts a i32 -> float unop to a u32 unop
	return u32.view(f(i32.view(a)));
}
def do_f_i(a: u32, f: float -> i32) -> u32 {  // Adapts a float -> i32 unop to a u32 unop
	return u32.view(f(float.view(a)));
}
def do_u_f(a: u32, f: u32 -> float) -> u32 {  // Adapts a u32 -> float unop to a u32 unop
	return u32.view(f(a));
}
def do_f_u(a: u32, f: float -> u32) -> u32 {  // Adapts a float -> u32 unop to a u32 unop
	return f(float.view(a));
}
def do_i_d(a:u32, f: i32 -> double) -> u64 {  // Adapts a i32 -> double unop to a u64 unop
	return u64.view(f(i32.view(a)));
}
def do_u_d(a:u32, f: u32 -> double) -> u64 {  // Adapts a u32 -> double unop to a u64 unop
	return u64.view(f(a));
}
def do_f_d(a: u32, f: float -> double) -> u64 {  // Adapts a float -> double unop to a u64 unop
	return u64.view(f(float.view(a)));
}
def do_d_f(a: u64, f: double -> float) -> u32 {  // Adapts a double -> float unop to a u32 unop
	return u32.view(f(double.view(a)));
}
def do_d_i(a: u64, f: double -> i32) -> u32 {  // Adapts a double -> i32 unop to a u32 unop
	return u32.view(f(double.view(a)));
}
def do_d_u(a: u64, f: double -> u32) -> u32 {  // Adapts a double -> u32 unop to a u32 unop
	return u32.view(f(double.view(a)));
}
// Unary v128 ops
def do_s_s_x2(a: (u64, u64), f: (u64) -> u64) -> (u64, u64) { // Performs a 2-lane unop
	var r0 = f(a.0);
	var r1 = f(a.1);
	return (r0, r1);
}
def do_s_s_x4(a: (u64, u64), f: (u32) -> u32) -> (u64, u64) { // Performs a 4-lane unop
	var r0 = f(u32.view(a.0));
	var r1 = f(u32.view(a.0 >> 32));
	var r2 = f(u32.view(a.1));
	var r3 = f(u32.view(a.1 >> 32));
	return ((u64.view(r1) << 32) | r0, (u64.view(r3) << 32) | r2);
}
def do_s_s_x8(a: (u64, u64), f: (u16) -> u16) -> (u64, u64) { // Performs an 8-lane unop
	var low: u64 = 0;
	var high: u64 = 0;
	for (i < 4) {
		var shift = u6.view(i << 4);
		var r_a = u16.view(a.0 >> shift);
		var res = f(r_a);
		low |= (u64.view(res) << shift);
		
		r_a = u16.view(a.1 >> shift);
		res = f(r_a);
		high |= (u64.view(res) << shift);
	}
	return (low, high);
}
def do_s_s_x16(a: (u64, u64), f: (u8) -> u8) -> (u64, u64) { // Performs a 16-lane unop
	var low: u64 = 0;
	var high: u64 = 0;
	for (i < 8) {
		var shift = u6.view(i << 3);
		var r_a = u8.view(a.0 >> shift);
		var res = f(r_a);
		low |= (u64.view(res) << shift);
		
		r_a = u8.view(a.1 >> shift);
		res = f(r_a);
		high |= (u64.view(res) << shift);
	}
	return (low, high);
}
def do_s_s_x2_low(a: (u64, u64), f: (u32) -> u64) -> (u64, u64) { // Performs a 2-lane unop
	var r0 = f(u32.view(a.0));
	var r1 = f(u32.view(a.0 >> 32));
	return (r0, r1);
}
def do_s_s_x4_high_zero(a: (u64, u64), f: (u64) -> u32) -> (u64, u64) { // Performs a 4-lane unop
	var r0 = f(a.0);
	var r1 = f(a.1);
	return ((u64.view(r1) << 32) | r0, 0);
}
// Binary v128 ops, in the form of (v, i32) -> v
def do_si_s_x16(a: (u64, u64), b: i32, f: (u8, u8) -> u8) -> (u64, u64) {
	var low: u64 = 0;
	var high: u64 = 0;
	for (i < 8) {
		var shift = u6.view(i << 3);
		var r_a = u8.view(a.0 >> shift);
		var res = f(r_a, u8.view(b));
		low |= (u64.view(res) << shift);
		
		r_a = u8.view(a.1 >> shift);
		res = f(r_a, u8.view(b));
		high |= (u64.view(res) << shift);
	}
	return (low, high);
}
def do_si_s_x8(a: (u64, u64), b: i32, f: (u16, u16) -> u16) -> (u64, u64) {
	var low: u64 = 0;
	var high: u64 = 0;
	for (i < 4) {
		var shift = u6.view(i << 4);
		var r_a = u16.view(a.0 >> shift);
		var res = f(r_a, u16.view(b));
		low |= (u64.view(res) << shift);
		
		r_a = u16.view(a.1 >> shift);
		res = f(r_a, u16.view(b));
		high |= (u64.view(res) << shift);
	}
	return (low, high);
}
def do_si_s_x4(a: (u64, u64), b: i32, f: (u32, u32) -> u32) -> (u64, u64) {
	var r0 = f(u32.view(a.0), u32.view(b));
	var r1 = f(u32.view(a.0 >> 32), u32.view(b));
	var r2 = f(u32.view(a.1), u32.view(b));
	var r3 = f(u32.view(a.1 >> 32), u32.view(b));
	return ((u64.view(r1) << 32) | r0, (u64.view(r3) << 32) | r2);
}
def do_si_s_x2(a: (u64, u64), b: i32, f: (u64, u64) -> u64) -> (u64, u64) {
	var r0 = f(a.0, u64.view(b));
	var r1 = f(a.1, u64.view(b));
	return (r0, r1);
}
// Binary v128 ops
def do_ss_s_x2(a: (u64, u64), b: (u64, u64), f: (u64, u64) -> u64) -> (u64, u64) { // Performs a 2-lane binop
	var r0 = f(a.0, b.0);
	var r1 = f(a.1, b.1);
	return (r0, r1);
}
def do_ss_s_x4(a: (u64, u64), b: (u64, u64), f: (u32, u32) -> u32) -> (u64, u64) { // Performs a 4-lane binop
	var r0 = f(u32.view(a.0), u32.view(b.0));
	var r1 = f(u32.view(a.0 >> 32), u32.view(b.0 >> 32));
	var r2 = f(u32.view(a.1), u32.view(b.1));
	var r3 = f(u32.view(a.1 >> 32), u32.view(b.1 >> 32));
	return ((u64.view(r1) << 32) | r0, (u64.view(r3) << 32) | r2);
}
def do_ss_s_x8(a: (u64, u64), b: (u64, u64), f: (u16, u16) -> u16) -> (u64, u64) { // Performs an 8-lane binop
	var low: u64 = 0;
	var high: u64 = 0;
	for (i < 4) {
		var shift = u6.view(i << 4);
		var r_a = u16.view(a.0 >> shift);
		var r_b = u16.view(b.0 >> shift);
		var res = f(r_a, r_b);
		low |= (u64.view(res) << shift);
		
		r_a = u16.view(a.1 >> shift);
		r_b = u16.view(b.1 >> shift);
		res = f(r_a, r_b);
		high |= (u64.view(res) << shift);
	}
	return (low, high);
}
def do_ss_s_x16(a: (u64, u64), b: (u64, u64), f: (u8, u8) -> u8) -> (u64, u64) { // Performs a 16-lane binop
	var low: u64 = 0;
	var high: u64 = 0;
	for (i < 8) {
		var shift = u6.view(i << 3);
		var r_a = u8.view(a.0 >> shift);
		var r_b = u8.view(b.0 >> shift);
		var res = f(r_a, r_b);
		low |= (u64.view(res) << shift);
		
		r_a = u8.view(a.1 >> shift);
		r_b = u8.view(b.1 >> shift);
		res = f(r_a, r_b);
		high |= (u64.view(res) << shift);
	}
	return (low, high);
}
def do_v128_alltrue<T>(a: (u64, u64), bits: byte, view: u64 -> T, equal: (T, T) -> bool) -> bool {
	for (shift = 0; shift < 64; shift += bits) {
		var r1 = view(a.0 >> u6.view(shift));
		var r2 = view(a.1 >> u6.view(shift));
		if (equal(r1, T.!(0)) || equal(r2, T.!(0))) return false;
	}
	return true;
}
def do_v128_bitmask<T>(a: (u64, u64), bits: byte, bits_log2:byte) -> i32 {
	var low: i32 = 0, high: i32 = 0;
	var src_low = a.0, src_high = a.1;
	def dst_shift = u6.view(64 >> bits_log2);
	def src_shift = u6.view(bits - 1);
	var i:byte = 0;
	for (iter = 0; iter < 64; iter += bits) {
		src_low >>= src_shift;
		src_high >>= src_shift;
		// Extract msb of each lane
		if (u1.>(u1.view(src_low), 0)) low |= (1 << i);
		if (u1.>(u1.view(src_high), 0)) high |= (1 << i);
		// Shift over the msb to the next lane
		src_low >>= 1;
		src_high >>= 1;
		i++;
	}
	return low | (high << dst_shift);
}
// Helper to perform lane-wise extensions.
// T: type to be converted to, having the same width as source type
// Tw: wider type of T, i.e., the extended type
// Uw: corresponding unsignced type of Tw
def do_v128_half_extend<T, Tw, Uw>(a: (u64, u64), low_half: bool, dest_bits: byte, 
	convert: u64 -> T, extend: (T) -> Tw, convert_to_u: (Tw) -> Uw) -> (u64, u64) {
	var src = if(low_half, a.0, a.1);	// Choose low or high half of source lanes.
	var src_high: u32 = u32.view(src >> 32);
	var src_low: u32 = u32.view(src);
	var dst_low = do_32_ext_64<T, Tw, Uw>(src_low, dest_bits, convert, extend, convert_to_u);
	var dst_high = do_32_ext_64<T, Tw, Uw>(src_high, dest_bits, convert, extend, convert_to_u);
	return (dst_low, dst_high);
}
def do_32_ext_64<T, Tw, Uw>(src: u32, dest_bits: byte, 
	convert: u64 -> T, extend: (T) -> Tw, convert_to_u: (Tw) -> Uw) -> u64 {
	var res: u64 = 0u;
	var shift = u6.view(dest_bits >> 1);	// Compute source shift amount, i.e., the width of the original lane.
	var dst_shift = dest_bits;
	for (i = 0; i < 64; i += dst_shift) {
		var val = extend(convert(u64.view(src)));
		src >>= shift;
		var res_shift = u6.view(i);	// Compute shift amount for result.
		res |= u64.!(convert_to_u(val)) << res_shift;
	}
	return res;
}
def do_v128_extmul<T, Tw, Uw>(a: (u64, u64), b: (u64, u64), low_half: bool, dest_bits: byte,
	convert: u64 -> T, extend: (T) -> Tw, convert_to_u: (Tw) -> Uw, f: (Tw, Tw) -> Tw) -> (u64, u64) {
    var src_b = if(low_half, b.0, b.1);
	var src_a = if(low_half, a.0, a.1);	// Choose low or high half of source lanes.
	var src_high_a = u32.view(src_a >> 32);
	var src_high_b = u32.view(src_b >> 32);
	var src_low_a = u32.view(src_a);
	var src_low_b = u32.view(src_b);
	// Extend low and high lanes separately.
	var dst_low = do_64_extmul_32(src_low_a, src_low_b, dest_bits, convert, extend, convert_to_u, f);
	var dst_high = do_64_extmul_32(src_high_a, src_high_b, dest_bits, convert, extend, convert_to_u, f);
	return (dst_low, dst_high);
}
def do_64_extmul_32<T, Tw, Uw>(src_a: u32, src_b: u32, dest_bits: byte,
	convert: u64 -> T, extend: (T) -> Tw, convert_to_u: (Tw) -> Uw, f: (Tw, Tw) -> Tw) -> u64 {
	var res = 0ul;
	var shift = u6.view(dest_bits >> 1);	// Compute source shift amount, i.e., the width of the original lane.
	var res_shift: u6 = 0;
	var res_shift_inc = u6.view(shift << 1);
	for (i = 0; i < 64; i += dest_bits) {
		var a = extend(convert(u64.view(src_a)));
		src_a >>= shift;
		var b = extend(convert(u64.view(src_b)));
		src_b >>= shift;
		var val = f(a, b);
		res |= u64.!(convert_to_u(val)) << res_shift;
		res_shift += res_shift_inc;
	}
	return res;
}
def do_v128_narrow<Tw, Tn, Un>(a: (u64, u64), b:(u64, u64), src_bits: byte,
	convert: u64 -> Tw, narrow: (Tw) -> Tn, convert_to_u: (Tn) -> Un) -> (u64, u64) {
	var low = do_v128_narrow0(a, src_bits, convert, narrow, convert_to_u);
	var high = do_v128_narrow0(b, src_bits, convert, narrow, convert_to_u);
	return (low, high);
}
def do_v128_narrow0<Tw, Tn, Un>(src: (u64, u64), src_bits: byte,
	convert: u64 -> Tw, narrow: (Tw) -> Tn, convert_to_u: (Tn) -> Un) -> u64 {
	var src_low = src.0, src_high = src.1;
	var low = 0ul, high = 0ul;
	var shift = u6.view(src_bits);
	var res_shift: u6 = 0;
	var res_shift_inc = u6.>>(shift, 1);
	for (i = 0; i < 64; i += src_bits) {
		var val = narrow(convert(src_low));
		src_low >>= shift;
		low |= u64.!(convert_to_u(val)) << res_shift;
		res_shift += res_shift_inc;
	}
	res_shift = 0;
	for (i = 0; i < 64; i += src_bits) {
		var val = narrow(convert(src_high));
		src_high >>= shift;
		high |= u64.!(convert_to_u(val)) << res_shift;
		res_shift += res_shift_inc;
	}
	return low | (high << 32);
}
// Apply a binary operation to adjacent lanes and put the result into a wider lane
def do_s_s_x16_pairwise_ext_x8(a: (u64, u64), f: (u8, u8) -> u16) -> (u64, u64) {
    var low: u64 = 0;
    var high: u64 = 0;
	for (i < 4) {
		var shift = u6.view(i << 4);
		// Process the low part
		var r_a1 = u8.view(a.0 >> shift);
		var r_a2 = u8.view(a.0 >> byte.view(shift + 8));
		var res = f(r_a1, r_a2);
		low |= (u64.view(res) << shift);
		// Process the high part
		r_a1 = u8.view(a.1 >> shift);
		r_a2 = u8.view(a.1 >> byte.view(shift + 8));
		res = f(r_a1, r_a2);
		high |= (u64.view(res) << shift);
	}
    return (low, high);
}
def do_s_s_x8_pairwise_ext_x4(a: (u64, u64), f: (u16, u16) -> u32) -> (u64, u64) {
    var low: u64 = 0;
    var high: u64 = 0;
	for (i < 2) {
		var shift = u6.view(i << 5);
		// Process the low part
		var r_a1 = u16.view(a.0 >> shift);
		var r_a2 = u16.view(a.0 >> byte.view(shift + 16));
		var res = f(r_a1, r_a2);
		low |= (u64.view(res) << shift);
		// Process the high part
		r_a1 = u16.view(a.1 >> shift);
		r_a2 = u16.view(a.1 >> byte.view(shift + 16));
		res = f(r_a1, r_a2);
		high |= (u64.view(res) << shift);
	}
    return (low, high);
}
def do_v128_dotprod(a: (u64, u64), b: (u64, u64)) -> (u64, u64) {
	var low = do_v128_dotprod0(a.0, b.0);
	var high = do_v128_dotprod0(a.1, b.1);
	return (low, high);
}
def do_v128_dotprod0(a: u64, b: u64) -> u64 {
	var res: u64 = 0;
	for (shift = 0; shift < 64; shift += 32) {
		var src_shift = u6.view(shift >> 1);
		var src_shift2 = u6.+(src_shift, u6.view(32 >> 1));
		var r_a1 = i16.view(a >> src_shift);
		var r_a2 = i16.view(a >> src_shift2);
		var r_b1 = i16.view(b >> src_shift);
		var r_b2 = i16.view(b >> src_shift2);
		var tmp1 = i32.*(i32.view(r_a1), i32.view(r_b1));
		var tmp2 = i32.*(i32.view(r_a2), i32.view(r_b2));
		res |= (u64.!(u32.view(i32.+(tmp1, tmp2))) << u6.view(shift));
	}
	return res;
}
// Helper to commute a binary operation
def commute_binop<T, R>(f: (T, T) -> R) -> (T, T) -> R {
	return commute_binop0(f, _, _);
}
def commute_binop0<T, R>(f: (T, T) -> R, a: T, b: T) -> R {
	return f(b, a);
}
def canonf(a: float) -> float {
	return if(a == a, a, float.nan);
}
def canond(a: double) -> double {
	return if(a == a, a, double.nan);
}
def truncF32<T>(min: float, max: float, trunc: float -> T, a: float) -> (T, TrapReason) {
	var d: T;
	if (a >= max) return (d, TrapReason.FLOAT_UNREPRESENTABLE);
	if (a <= min) return (d, TrapReason.FLOAT_UNREPRESENTABLE);
	if (!(a == a)) return (d, TrapReason.FLOAT_UNREPRESENTABLE);
	return (trunc(a), TrapReason.NONE);
}
def truncF32_SAT<T>(min: float, max: float, tmin: T, tmax: T, trunc: float -> T, a: float) -> T {
	var d: T;
	if (a >= max) return tmin;
	if (a <= min) return tmax;
	if (!(a == a)) return T.!(0);	// If any input lane is a NaN, the resulting lane is 0
	return trunc(a);
}
def truncF64<T>(min: double, max: double, trunc: double -> T, a: double) -> (T, TrapReason) {
	var d: T;
	if (a >= max) return (d, TrapReason.FLOAT_UNREPRESENTABLE);
	if (a <= min) return (d, TrapReason.FLOAT_UNREPRESENTABLE);
	if (!(a == a)) return (d, TrapReason.FLOAT_UNREPRESENTABLE);
	return (trunc(a), TrapReason.NONE);
}
def truncF64_SAT<T>(min: double, max: double, tmin: T, tmax: T, trunc: double -> T, a: double) -> T {
	var d: T;
	if (a >= max) return tmin;
	if (a <= min) return tmax;
	if (!(a == a)) return T.!(0);	// If any input lane is a NaN, the resulting lane is 0
	return trunc(a);
}
