Dynamic metrics to gather and analyze:

# Loop-centric

* **Loop Kernel Density (LKD).**
  For loop (L): (K(L)=\frac{\text{iters}(L)}{\text{unique_insts_in_L}}).
  Program LKD: (\sum_L w_L K(L)) with (w_L=\frac{\text{time}(L)}{\text{total time}}). Higher → tighter/smaller kernels run many trips.
* **Loop Trip Spectrum.**
  Histogram or percentiles of trip counts per distinct loop head (P50/P90/P99). Detects “a few gigantic loops” vs “many tiny loops.”
* **Loop Body Footprint.**
  Distribution of unique static instructions per loop body; pair with LKD to classify “tiny hot loops” vs “messy big loops.”
* **Inner-hotness Ratio.**
  Time in innermost loops ÷ time in all loops. High → time mainly at deepest nest.
* **Loop Burstiness.**
  Fano factor of iterations per fixed time window: (\frac{\mathrm{Var}}{\mathrm{Mean}}). High → loops run in bursts/phases.

# Control-flow concentration & predictability

* **PC Entropy (temporal).**
  (H=-\sum_{b} p_b\log_2 p_b), where (p_b) is fraction of time at basic block (b). Low (H) → concentrated hotspots.
* **Lorenz/Gini for Time.**
  Gini over time-per-block/function; complements your “distribution of time” with a single scalar.
* **Hot-path Coverage.**
  % of time explained by top-(k) acyclic paths (e.g., top 10 paths).
* **Branch Perplexity & Bias.**
  For each dynamic branch, entropy of outcomes; aggregate mean/median. Pair with MPKI (mispredicts per kilo-inst) if you can sample it.

# Calls, frames & structure

* **Call Intensity.**
  Calls per K instructions (CPKI); split direct vs indirect.
* **Dynamic Stack Depth Profile.**
  Distribution (P50/P95/P99) of call depth; also **Depth Oscillation**: standard deviation/TV distance across windows to capture recursive thrashing.
* **Return Site Locality.**
  For each call site, #distinct return PCs (with inlining/tail-calls this still shows polymorphism).

# Locality & memory working sets

* **Code Working-Set (CWS).**
  Unique basic blocks executed within a sliding time window (W). Report median/peak and the half-life until 50% of the set turns over.
* **Data Working-Set (DWS).**
  Unique cache-line addresses touched in window (W); same stats as CWS.
* **Stride Regularity.**
  For each load/store stream (by PC), entropy of stride distribution; mean unique strides per stream.
* **Reuse Distance (Stack Distance) Stats.**
  P50/P90/P99 of reuse distance for loads; great predictor of cache pressure without sim.
* **Spatial Clustering.**
  Mean run length of same-page (or same 64 KB region) accesses.

# Cache & memory system

* **Miss Ratios by Level.**
  L1/L2/LLC MPKI (or misses/second) plus **Miss Cost Mix** (avg cycles per miss) to separate bandwidth vs latency pain.
* **Prefetch Utility.**
  Fraction of LLC hits satisfied by hardware/software prefetch (if counters exist).
* **TLB Pressure.**
  iTLB/dTLB MPKI; **Superpage Benefit** estimate by recomputing with 2 MB/1 GB page model on traces.

# Objects, allocation, GC (if managed)

* **Allocation Rate & Churn.**
  Bytes/sec and objs/sec; **Lifetime Spectrum** (log-binned); **Survival Curve** (gen-to-gen).
* **Pointer Locality.**
  Fraction of loads that target *recently* allocated objects (e.g., within last (X) ms).
* **GC Time Share & Tail.**
  % time in GC and P95/P99 pause lengths; **Mutator Utilization** over windows.

# Concurrency & synchronization

* **Runnable Parallelism.**
  Average runnable threads; **CPU Utilization Efficiency** = (user time ÷ #HW threads) / wall time.
* **Lock Contention Rate.**
  Contended acquisitions per second; **Hold-time** P90/P99; **Critical Section Density** = time in critical sections ÷ total.
* **Queueing/Burstiness.**
  Arrival vs service variability (COV or Fano factor) for task queues.

# I/O & system interaction

* **Syscall Mix & Density.**
  Syscalls per K instructions; HHI (Herfindahl index) across syscall types → whether one dominates.
* **I/O Stall Share.**
  Fraction of wall time blocked on file/net; **Outstanding I/O Depth** distribution.

# Phase & regularity

* **Phase Change Rate.**

  # of statistically significant distribution shifts per second (change-point detection on PC/time or CWS).
* **Trace Compressibility.**
  LZ77 compression ratio of the instruction-address stream (lower = more regular); can compute per window to spot phases.
* **Allan Variance of IPC.**
  Stability of throughput across timescales (used in signal processing; great for “smooth vs jittery” executors).

# Two compact “loopiness” scalars

* **Kernelity Index (KI).**
  ( \text{KI}=\frac{\text{time in top-}N\text{ loops}}{\text{total time}} \times \frac{\text{iters in those loops}}{\text{unique insts in those loops}} ).
  Tunable (N) (e.g., 5 or 10). High KI ⇒ “small, tight kernels dominate.”
* **Call-Loop Mix (CLM).**
  ( \text{CLM}=\frac{\text{call sites per K inst}}{\text{loop heads per K inst}} ).

  > 1 ⇒ call-heavy; <1 ⇒ loop-heavy.

# Visuals that pair well

* **Lorenz curve** of time by static site (func/BB/loop) with Gini/HHI.
* **Trip vs Body-size scatter** (each loop: x=unique insts in body, y=trip count, bubble=size by time).
* **CWS/DWS over time** (lines) with phase change markers.

# Practical notes

* Normalize wherever possible (per K inst, per second, or weighted by time share) so binaries are comparable across machines and builds.
* For PCs/loops, use stable IDs (symbol+offset or hashed CFG node) to survive address randomization/inlining.
* Sliding windows (e.g., 10–100 ms) make “phase” and “burstiness” metrics meaningful without overwhelming you with data.

If you want, I can sketch a tiny instrumentation plan (e.g., what to log at each BB/loop head/call/alloc) and give you 10–12 metrics you can compute in a single pass over the trace.
